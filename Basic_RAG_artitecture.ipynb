{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
      "metadata": {
        "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe"
      },
      "source": [
        "# Rag From Scratch: Overview\n",
        "\n",
        "These notebooks walk through the process of building RAG app(s) from scratch.\n",
        "\n",
        "They will build towards a broader understanding of the RAG langscape, as shown here:\n",
        "\n",
        "The topic for the RAG is\n",
        "##Research Papers in Deep Learning and Chemical Structures (Image Data)\n",
        "\n",
        "## Enviornment\n",
        "\n",
        "`(1) Packages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "306e0202",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "306e0202",
        "outputId": "51322ccc-962d-4af7-c681-efb80efb243f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/161.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/161.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
        "outputId": "e117e53f-fe27-4c80-caea-d980ab12f04a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.13-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.66)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (24.2)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Downloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.5-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading chromadb-1.0.13-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=04309d4bfae1b2875b558f0090c17696cd42648bca4fc42ff2020249980ebae7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, filetype, durationpy, uvloop, types-requests, python-dotenv, PyMuPDF, pybase64, overrides, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, bcrypt, backoff, watchfiles, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, langchainhub, coloredlogs, pydantic-settings, opentelemetry-semantic-conventions, onnxruntime, kubernetes, dataclasses-json, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, google-ai-generativelanguage, langchain-google-genai, chromadb, langchain_community\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDF-1.26.1 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.13 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 filetype-1.2.0 google-ai-generativelanguage-0.6.18 httptools-0.6.4 httpx-sse-0.4.1 humanfriendly-10.0 kubernetes-33.1.0 langchain-google-genai-2.1.5 langchain_community-0.3.26 langchainhub-0.1.21 marshmallow-3.26.1 mmh3-5.1.0 mypy-extensions-1.1.0 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.1 pydantic-settings-2.10.1 pypika-0.48.9 python-dotenv-1.1.1 types-requests-2.32.4.20250611 typing-inspect-0.9.0 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "71ea0f00993643428221dc760d6b434b",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "! pip install langchain_community tiktoken langchain-google-genai langchainhub chromadb langchain PyMuPDF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a8ab66-8477-429f-bbbe-ba439322d085",
      "metadata": {
        "id": "75a8ab66-8477-429f-bbbe-ba439322d085"
      },
      "source": [
        "`(2) LangSmith`\n",
        "\n",
        "https://docs.smith.langchain.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b76f68a8-4745-4377-8057-6090b87377d1",
      "metadata": {
        "id": "b76f68a8-4745-4377-8057-6090b87377d1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8eb312d-8a07-4df3-8462-72ac526715f7",
      "metadata": {
        "id": "f8eb312d-8a07-4df3-8462-72ac526715f7"
      },
      "source": [
        "`(3) API Keys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e",
      "metadata": {
        "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W4wixE7iaJs9",
      "metadata": {
        "id": "W4wixE7iaJs9"
      },
      "source": [
        "# Basic Rag using Chroma DB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c",
      "metadata": {
        "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c"
      },
      "source": [
        "## Part 1: Overview\n",
        "\n",
        "[RAG quickstart](https://python.langchain.com/docs/use_cases/question_answering/quickstart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "kmXRagov4wfU",
      "metadata": {
        "id": "kmXRagov4wfU"
      },
      "outputs": [],
      "source": [
        "vectorstore.delete_collection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "qzwHhIFujOfA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzwHhIFujOfA",
        "outputId": "04b4b5ee-da24-4777-a6c3-d58f3157ab1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing: https://portlandpress.com/biochemj/article/477/23/4559/227194/Deep-learning-and-generative-methods-in\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\n",
            "Error loading https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236: 403 Client Error: Forbidden for url: https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\n",
            "Failed to load document\n",
            "\n",
            "Processing: https://www.osti.gov/servlets/purl/1427646\n",
            "Error loading https://www.osti.gov/servlets/purl/1427646: 502 Server Error: Proxy Error for url: https://www.osti.gov/servlets/purl/1427646\n",
            "Failed to load document\n",
            "\n",
            "Processing: https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf\n",
            "Successfully loaded 16 documents\n",
            "\n",
            "Processing: http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf\n",
            "Successfully loaded 13 documents\n",
            "\n",
            "Processing: https://www.nature.com/articles/s41467-022-28494-3\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://link.springer.com/article/10.1007/s00521-021-05961-4\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.mdpi.com/journal/molecules/special_issues/deep_learning_structure\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\n",
            "Error loading https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\n",
            "Failed to load document\n",
            "\n",
            "Processing: https://www.mdpi.com/1420-3049/25/12/2764\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.nature.com/articles/s41598-025-95720-5\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://link.springer.com/article/10.1557/s43578-022-00628-9\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Summary:\n",
            "- Successfully loaded: 39 documents\n",
            "- Failed URLs: 3\n",
            "Failed URLs: ['https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236', 'https://www.osti.gov/servlets/purl/1427646', 'https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050']\n",
            "- Total chunks after splitting: 177\n",
            "\n",
            "Vector store created with 177 chunks\n",
            "Vector store and retriever created successfully!\n"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "import fitz  # PyMuPDF\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader, PyMuPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import requests\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "from langchain.schema import Document\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "\n",
        "class DocumentProcessor:\n",
        "    def __init__(self):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=5000,\n",
        "            chunk_overlap=250\n",
        "        )\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        self.ua = UserAgent()\n",
        "        self.headers = {\n",
        "            'User-Agent': self.ua.random,\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "        }\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(self.headers)\n",
        "\n",
        "    def load_html(self, url):\n",
        "        \"\"\"Enhanced HTML loader with better error handling\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Check if content-type is PDF\n",
        "            content_type = response.headers.get('Content-Type', '')\n",
        "            if 'application/pdf' in content_type:\n",
        "                return self.load_pdf_from_url(url)\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Try to find main content areas\n",
        "            article = (soup.find('article') or\n",
        "                      soup.find('main') or\n",
        "                      soup.find(class_=re.compile('content|main|body|post')) or\n",
        "                      soup.find('div', role='main') or\n",
        "                      soup)\n",
        "\n",
        "            # Extract all text with structure\n",
        "            content = self._extract_structured_content(article)\n",
        "            if not content:\n",
        "                raise ValueError(\"No content extracted from HTML\")\n",
        "\n",
        "            return [Document(page_content=content, metadata={'source': url, 'type': 'html'})]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {url}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _extract_structured_content(self, element):\n",
        "        \"\"\"Extract content while preserving document structure\"\"\"\n",
        "        content = []\n",
        "\n",
        "        def process_element(elem):\n",
        "            if isinstance(elem, bs4.NavigableString):\n",
        "                text = elem.strip()\n",
        "                if text and len(text) > 10:\n",
        "                    content.append(text)\n",
        "                return\n",
        "\n",
        "            tag = elem.name\n",
        "            if not tag:\n",
        "                return\n",
        "\n",
        "            text = elem.get_text(' ', strip=True)\n",
        "            if not text or len(text) <= 10:\n",
        "                return\n",
        "\n",
        "            # Handle headings\n",
        "            if tag.startswith('h') and tag[1:].isdigit():\n",
        "                level = int(tag[1:])\n",
        "                content.append(f\"\\n{'#'*level} {text}\\n\")\n",
        "            # Handle list items\n",
        "            elif tag == 'li':\n",
        "                content.append(f\"- {text}\")\n",
        "            # Handle table cells\n",
        "            elif tag in ['td', 'th']:\n",
        "                content.append(f\"[TABLE CELL] {text}\")\n",
        "            # Handle regular paragraphs\n",
        "            elif tag == 'p':\n",
        "                content.append(text)\n",
        "            # Recursively process containers\n",
        "            else:\n",
        "                for child in elem.children:\n",
        "                    process_element(child)\n",
        "\n",
        "        process_element(element)\n",
        "        full_text = '\\n'.join(content)\n",
        "        full_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n",
        "        full_text = re.sub(r'[ \\t]{2,}', ' ', full_text)\n",
        "        return full_text.strip()\n",
        "\n",
        "    def load_pdf_from_url(self, url):\n",
        "        \"\"\"Improved PDF loader with retries and better cleaning\"\"\"\n",
        "        max_retries = 3\n",
        "        retry_delay = 2\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = self.session.get(url, timeout=30)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "                    tmp_file.write(response.content)\n",
        "                    tmp_path = tmp_file.name\n",
        "\n",
        "                loader = PyMuPDFLoader(tmp_path)\n",
        "                docs = loader.load()\n",
        "\n",
        "                # Clean up the extracted text\n",
        "                for doc in docs:\n",
        "                    doc.page_content = self._clean_pdf_text(doc.page_content)\n",
        "                    doc.metadata.update({\n",
        "                        'source': url,\n",
        "                        'type': 'pdf',\n",
        "                        'pages': doc.metadata.get('page', '')\n",
        "                    })\n",
        "\n",
        "                os.unlink(tmp_path)\n",
        "                return docs\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed for {url}: {str(e)}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    if 'tmp_path' in locals() and os.path.exists(tmp_path):\n",
        "                        os.unlink(tmp_path)\n",
        "                    return []\n",
        "\n",
        "    def _clean_pdf_text(self, text):\n",
        "        \"\"\"Clean and normalize PDF text\"\"\"\n",
        "        # Remove page numbers and footers\n",
        "        text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        # Remove lonely characters\n",
        "        text = re.sub(r'(?<!\\w)\\w(?!\\w)', '', text)\n",
        "        # Fix hyphenated words\n",
        "        text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
        "        return text\n",
        "\n",
        "    def process_documents(self, urls):\n",
        "        \"\"\"Process documents with better error handling\"\"\"\n",
        "        all_docs = []\n",
        "        failed_urls = []\n",
        "\n",
        "        for url in urls:\n",
        "            print(f\"\\nProcessing: {url}\")\n",
        "            try:\n",
        "                if url.lower().endswith('.pdf'):\n",
        "                    docs = self.load_pdf_from_url(url)\n",
        "                else:\n",
        "                    docs = self.load_html(url)\n",
        "\n",
        "                if docs:\n",
        "                    all_docs.extend(docs)\n",
        "                    print(f\"Successfully loaded {len(docs)} documents\")\n",
        "                else:\n",
        "                    failed_urls.append(url)\n",
        "                    print(\"Failed to load document\")\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_urls.append(url)\n",
        "                print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "        if not all_docs:\n",
        "            raise ValueError(\"No documents were successfully loaded\")\n",
        "\n",
        "        print(f\"\\nSummary:\")\n",
        "        print(f\"- Successfully loaded: {len(all_docs)} documents\")\n",
        "        print(f\"- Failed URLs: {len(failed_urls)}\")\n",
        "        if failed_urls:\n",
        "            print(\"Failed URLs:\", failed_urls)\n",
        "\n",
        "        splits = self.text_splitter.split_documents(all_docs)\n",
        "        print(f\"- Total chunks after splitting: {len(splits)}\")\n",
        "        return splits, failed_urls # Return splits and failed_urls\n",
        "\n",
        "    def create_vector_store(self, splits, persist_dir=\"chroma_db\"):\n",
        "        \"\"\"Create and persist Chroma vector store\"\"\"\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=splits,\n",
        "            embedding=self.embeddings,\n",
        "            persist_directory=persist_dir\n",
        "        )\n",
        "        print(f\"\\nVector store created with {vectorstore._collection.count()} chunks\")\n",
        "        return vectorstore\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Your list of documents\n",
        "    documents = [\n",
        "        \"https://portlandpress.com/biochemj/article/477/23/4559/227194/Deep-learning-and-generative-methods-in\",\n",
        "        \"https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\",\n",
        "        \"https://www.osti.gov/servlets/purl/1427646\",\n",
        "        \"https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/\",\n",
        "        \"https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf\",\n",
        "        \"http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf\",\n",
        "        \"https://www.nature.com/articles/s41467-022-28494-3\",\n",
        "        \"https://link.springer.com/article/10.1007/s00521-021-05961-4\",\n",
        "        \"https://www.mdpi.com/journal/molecules/special_issues/deep_learning_structure\",\n",
        "        \"https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\",\n",
        "        \"https://www.mdpi.com/1420-3049/25/12/2764\",\n",
        "        \"https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6\",\n",
        "        \"https://www.nature.com/articles/s41598-025-95720-5\",\n",
        "        \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/\",\n",
        "        \"https://link.springer.com/article/10.1557/s43578-022-00628-9\"\n",
        "    ]\n",
        "\n",
        "    # Initialize and process\n",
        "    processor = DocumentProcessor()\n",
        "    splits, failed_urls = processor.process_documents(documents) # Unpack the tuple here\n",
        "    vectorstore = processor.create_vector_store(splits) # Pass only splits\n",
        "\n",
        "    # Get retriever\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    print(\"Vector store and retriever created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r5vjKPHMUP1Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5vjKPHMUP1Y",
        "outputId": "7349361c-2966-4c18-8d5a-a62c186e8eeb"
      },
      "outputs": [],
      "source": [
        "collection_data = vectorstore._collection.get(include=[\"embeddings\",\"documents\",'metadatas'])\n",
        "print(collection_data.get('ids', []),)\n",
        "print(collection_data.get('documents', []))\n",
        "print(collection_data.get('metadatas', []))\n",
        "print(collection_data.get('embeddings', []))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df",
        "outputId": "5b89a295-31e3-4a0a-c4cd-33e6e1ea3a7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context from research papes to answer the question in detail minimum 500 words. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\"), additional_kwargs={})]\n",
            "SMILES, which stands for Simplified Molecular-Input Line-Entry System, is a line notation used to represent chemical structures. It encodes the connection table and stereochemistry of a molecule as a line of text using short ASCII strings. SMILES utilizes a grammar structure where alphabets denote atoms, special characters indicate bond types, encapsulated numbers represent rings, and parentheses represent side chains.\n",
            "\n",
            "SMILES is a popular specification for describing chemical structures in a linear format. It is used as a sequence input in deep learning models like CheMixNet for predicting chemical properties. CheMixNet leverages SMILES notations along with molecular fingerprints to improve upon existing approaches for learning from vector or text representations of molecules. The model generates SMILES character by character from left to right without using an external dictionary for chemical abbreviations, learning these abbreviations as part of the model.\n",
            "\n",
            "In SMILES, organic molecules attach to form long continuous chains known as branches. The complete encoder-decoder framework used for SMILES generation is fully differentiable and trained end-to-end using backpropagation. During decoding, SMILES are generated one character at a time, and an attention mechanism helps the decoder access information produced earlier in the encoder, minimizing the loss of important details. The attention output can also be used for repositioning a predicted structure into an orientation that better matches the original input image.\n"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
        "from langsmith import Client\n",
        "client = Client(api_key=LANGCHAIN_API_KEY)\n",
        "# prompt_object = client.pull_prompt(\"chatbot\", include_model=True)\n",
        "\n",
        "# Define the prompt template using input variables\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context from research papes to answer the question in detail minimum 500 words. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\"\"\")\n",
        "\n",
        "print(prompt)\n",
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question\n",
        "print(rag_chain.invoke(\"What is SMILES tell everything about it in detail\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e8e856-bafd-469e-b99a-11596b18aad4",
      "metadata": {
        "id": "18e8e856-bafd-469e-b99a-11596b18aad4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "edd7beeb-21fa-4f4b-b8fa-5a4f70489a16",
      "metadata": {
        "id": "edd7beeb-21fa-4f4b-b8fa-5a4f70489a16"
      },
      "outputs": [],
      "source": [
        "# Documents\n",
        "question = \"What kinds of pets do I like?\"\n",
        "document = \"My favorite pet is a cat.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0552ea4-935d-4dfa-bd2b-56d148e96304",
      "metadata": {
        "id": "e0552ea4-935d-4dfa-bd2b-56d148e96304"
      },
      "source": [
        "[Count tokens](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) considering [~4 char / token](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "df119cca-1676-4caa-bad4-11805d69e616",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df119cca-1676-4caa-bad4-11805d69e616",
        "outputId": "08f39b94-7539-4d31-9527-e250c83d7fbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(question, \"cl100k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f04fd74-829f-472c-a1bc-ec6521a0529f",
      "metadata": {
        "id": "4f04fd74-829f-472c-a1bc-ec6521a0529f"
      },
      "source": [
        "[Text embedding models](https://python.langchain.com/docs/integrations/text_embedding/openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "6bd98786-755d-4d49-ba97-30c5a623b74e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bd98786-755d-4d49-ba97-30c5a623b74e",
        "outputId": "d7130914-6424-4854-9c7b-c90545cc1c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.006222310476005077, -0.006061272229999304, -0.026451561599969864, -0.020352467894554138, 0.0056349425576627254, -0.008037385530769825, 0.028191709890961647, -0.010441510006785393, 0.027259130030870438, 0.013420348055660725, 0.06445972621440887, -0.016194358468055725, 0.025595178827643394, 0.016282696276903152, 0.0048562223091721535, -0.031569819897413254, 0.005982452072203159, -0.00033394136698916554, 0.0012122254120185971, -0.04223364591598511, 0.009387334808707237, -0.002535228617489338, -0.019733130931854248, -0.005538017023354769, 0.04108477756381035, -0.06169067695736885, 0.04840322211384773, -0.029915019869804382, 0.0035526345018297434, 0.04842689633369446, -0.06559431552886963, 0.05941709503531456, -0.07555248588323593, -0.0007875484297983348, -0.023788658902049065, -0.04315219447016716, -0.032032158225774765, -0.019618835300207138, -0.014419357292354107, 0.05566913262009621, 0.01879522204399109, 0.002243547234684229, -0.05868314951658249, -0.051063422113657, 0.022266339510679245, -0.012864183634519577, -0.022421887144446373, 0.06182055175304413, -0.02373102679848671, -0.06208764761686325, -0.016852153465151787, 0.003738647559657693, 0.050278812646865845, -0.04256988316774368, -0.023102061823010445, -0.022187812253832817, 0.034776706248521805, 0.024805039167404175, -0.05842166021466255, -0.008013466373085976, 0.04033150523900986, -0.001955340150743723, -0.03222067654132843, 0.049259766936302185, 0.015838002786040306, 0.010767270810902119, -0.08347824215888977, 0.032192423939704895, 0.046924952417612076, -0.03559058904647827, 0.05548948049545288, -0.02429872378706932, 0.01198603305965662, -0.016376353800296783, 0.0013767048949375749, -0.012627207674086094, -0.039111118763685226, 0.07820290327072144, 0.008970683440566063, -0.03766179457306862, 0.01895843632519245, -0.05883047729730606, -0.02358003333210945, -0.009172654710710049, -0.054935213178396225, 0.053952157497406006, -0.05240539461374283, 0.0026102664414793253, 0.0063984934240579605, 0.059731047600507736, -0.04408351704478264, 0.029073666781187057, -0.016399770975112915, -0.07623711228370667, 0.009799780324101448, 0.007496033329516649, -0.009890802204608917, -0.006045255344361067, -0.02850692719221115, -0.0428740456700325, -0.004080966580659151, -0.036088354885578156, -0.0527612529695034, 0.021450763568282127, 0.030037250369787216, 0.01786760985851288, 0.015905577689409256, 0.034350376576185226, -0.01812109723687172, 0.02363850548863411, -0.05234648287296295, -0.014596424996852875, 0.01265321858227253, -0.059035394340753555, 0.018571363762021065, -0.023240476846694946, -0.036214083433151245, 0.025008603930473328, 0.0030875015072524548, 0.004920251667499542, -0.022183984518051147, -0.013271444477140903, 0.09297160059213638, -0.03406680002808571, -0.016684599220752716, 0.008161990903317928, -0.011368989944458008, -0.003504280699416995, 0.014322439208626747, 0.0538010448217392, -0.024336237460374832, -0.027956442907452583, -0.01984374038875103, 0.014147549867630005, 0.056704774498939514, 0.06660541892051697, 0.035353709012269974, 0.038736287504434586, 0.04888185113668442, 0.00838612113147974, -0.004395216703414917, 0.03520061448216438, -0.009493265300989151, -0.011935634538531303, -0.006906851194798946, 0.04367882385849953, -0.03165670111775398, -0.07639797776937485, 0.029765918850898743, -0.020529726520180702, -0.009268160909414291, -0.014753740280866623, -0.041690628975629807, 0.02674628421664238, 0.08705016225576401, 0.048058804124593735, 0.007582631893455982, 0.01816888153553009, 0.031955286860466, 0.00041392940329387784, 0.06307274848222733, 0.04522327706217766, 0.017325609922409058, 0.008565298281610012, -0.008582072332501411, -0.013929125852882862, -0.02154286950826645, 0.0069761332124471664, -0.009223918430507183, -0.013340090401470661, -0.04090652987360954, 0.007913832552731037, -0.026527568697929382, -0.033383894711732864, -0.024662230163812637, -0.0788801833987236, 0.012774978764355183, -0.0016996216727420688, -0.013259803876280785, -0.022261817008256912, -0.002170777413994074, -0.055035147815942764, -0.01793244108557701, 0.03790370747447014, 0.005097468849271536, -0.01058557815849781, 0.1172015517950058, 0.012220829725265503, -0.0017323552165180445, 0.021787727251648903, 0.021449929103255272, -0.024996211752295494, -0.0062807295471429825, -0.003195086494088173, -0.028913464397192, 0.010833497159183025, -0.009153824299573898, 0.006836880464106798, 0.0506306029856205, -0.028558949008584023, -0.02062748372554779, 0.09718089550733566, 0.020679784938693047, -0.0018793073249980807, 0.0024828596506267786, 0.010115155950188637, 0.037040505558252335, -0.012137283571064472, -0.006627485156059265, 0.07121410220861435, -0.022658921778202057, -0.014101369306445122, -0.0264345183968544, 0.016777360811829567, 0.00962794292718172, 0.04009684547781944, 0.06611631065607071, 0.03699716925621033, 0.028152503073215485, -0.009452222846448421, 0.01136341318488121, -0.0094816405326128, -0.007854917086660862, 0.026372084394097328, -0.007117667235434055, -0.004156750626862049, -0.002880386309698224, -0.00557001493871212, -0.014998636208474636, -0.06312571465969086, -0.0017526198644191027, 0.08024539053440094, 0.0019628778100013733, -0.03730218857526779, 0.04536611586809158, 0.0168409813195467, -0.017978807911276817, 0.0017182460287585855, 0.03215448930859566, 0.04417799785733223, -0.061807651072740555, 0.007454653736203909, 0.0949333906173706, 0.049554355442523956, -0.025820989161729813, -0.03422542288899422, 0.009838230907917023, 0.035022951662540436, -0.015508243814110756, 0.05620777979493141, 0.011158211156725883, -0.0617312528192997, -0.0049789054319262505, 0.017650291323661804, -0.09215918928384781, 0.016243530437350273, -0.036797039210796356, 0.025979379191994667, -0.02499113418161869, 0.04356689751148224, 0.060674820095300674, -0.01125197671353817, 0.03261711820960045, 0.005400656722486019, -0.020767496898770332, 0.009640897624194622, -0.004393969662487507, -0.060756441205739975, 0.033224258571863174, 0.020760992541909218, 0.021893706172704697, -0.04034193605184555, 0.0453353188931942, 0.0032870396971702576, 0.014589757658541203, 0.02079039253294468, -0.009038198739290237, 0.008882427588105202, 0.02293095365166664, -0.05524638295173645, -0.004743624944239855, 0.022031128406524658, 0.04962436109781265, -0.07838759571313858, 0.005264940205961466, -0.0038582179695367813, -0.06632698327302933, -0.005239606834948063, 0.01688847877085209, -0.017003891989588737, 0.007343181874603033, -0.01591329090297222, -0.03413620591163635, -0.02517874352633953, -0.045086994767189026, -0.023525595664978027, -0.014312485232949257, 0.03576505184173584, 0.026165293529629707, -0.008626342751085758, 0.0034218106884509325, 0.013346115127205849, -0.017145894467830658, -0.0832761749625206, -0.02690863236784935, 0.007166056893765926, -0.01800401322543621, -0.07352308183908463, -0.030146488919854164, 0.05765384063124657, -0.043825991451740265, -0.013815945014357567, -0.019679615274071693, -0.006803792901337147, 0.03382819518446922, 0.02548799477517605, -0.02237756736576557, -0.0012384685687720776, -0.04824494197964668, 0.03150542452931404, -0.012546392157673836, 0.05707462504506111, -0.031844623386859894, -0.031232377514243126, -0.029145359992980957, -0.022220730781555176, 0.010013540275394917, 0.0880306288599968, 0.03828343749046326, -0.03909068927168846, -0.029900480061769485, 0.008164994418621063, -0.07986867427825928, 0.01667923294007778, -0.007998897694051266, 0.028702260926365852, -0.07239975780248642, 0.06913494318723679, -0.06683997809886932, 0.054620448499917984, 0.08031804114580154, -0.006420033052563667, 0.03177199512720108, -0.03995697572827339, 0.010199376381933689, -0.01985473372042179, -0.008054823614656925, 0.018221329897642136, 0.06913547217845917, -0.01836201921105385, -0.011506939306855202, 0.0799444168806076, 0.0071341204456985, -0.0027047060430049896, -0.025045007467269897, -0.07816136628389359, -0.011572420597076416, 0.01011291891336441, 0.06198987364768982, -0.02619713358581066, -0.03872048482298851, 0.05082957446575165, -0.030150186270475388, 0.02136915735900402, -0.08276428282260895, 0.02205660752952099, 0.01609732210636139, -0.02525123581290245, 0.010621946305036545, 0.024748075753450394, 0.007750580087304115, -0.03866662085056305, 0.025272179394960403, 0.00799619685858488, -0.022710222750902176, -0.06430424749851227, -0.05956410616636276, -0.003157079918310046, 0.008517741225659847, 0.004720916971564293, 0.0013632802292704582, -0.03453361243009567, 0.06115865707397461, 0.05254590883851051, 0.021484987810254097, -0.021190987899899483, 0.054971449077129364, 0.0023969297762960196, -0.014507941901683807, 0.04610981047153473, 0.06678164750337601, 0.018558833748102188, 0.10207806527614594, 0.06751010566949844, -0.02613859437406063, -0.00978553295135498, 0.0218732301145792, -0.05150146409869194, -0.003227489534765482, -0.008518537506461143, 0.032509639859199524, -0.03365805745124817, -0.023541413247585297, -0.012840299867093563, 0.0031310180202126503, -0.018941566348075867, -0.019284185022115707, -0.04839293286204338, 0.02901245653629303, 0.02481534704566002, 0.04845448583364487, -0.005530698224902153, 0.0257271695882082, -0.040302712470293045, -0.038247473537921906, -0.008933342061936855, 0.03836940973997116, 0.0021311885211616755, 0.001238861819729209, 0.03263643756508827, 0.01910492032766342, 0.011987969279289246, -0.0072154696099460125, -0.011135516688227654, -0.013482240960001945, -0.09619176387786865, -0.00284109846688807, -0.031617358326911926, -0.007321139331907034, 0.04293518513441086, 0.03888135030865669, 0.0074499561451375484, 0.0453876331448555, -0.03224983438849449, -0.0036641890183091164, -0.0532708466053009, 0.038342952728271484, 0.010178589262068272, -0.030120987445116043, 0.0007784761837683618, -0.02136644721031189, -0.03135555237531662, 0.08762139081954956, -0.006176386959850788, -0.06497150659561157, -0.055412355810403824, -0.015292159281671047, -0.01894320361316204, 0.060275353491306305, -0.12686148285865784, 0.03842536360025406, -0.08040332049131393, 0.032158687710762024, -0.0853026732802391, -0.03741225600242615, -0.0045944154262542725, 0.026269273832440376, 0.07520689070224762, -0.034906283020973206, 0.012010338716208935, -0.012123403139412403, -0.02453664131462574, -0.017574802041053772, -0.10276437550783157, 0.019098637625575066, -0.0028959326446056366, 0.050150930881500244, 0.03936423361301422, 0.04118887335062027, 0.0312623456120491, 0.016740063205361366, -0.033514127135276794, 0.00039950141217559576, -0.00017502308764960617, -0.013173354789614677, 0.01108616590499878, -0.026806635782122612, 0.05038892477750778, 0.0043753948993980885, -0.007672145497053862, -0.01787707954645157, 0.010911660268902779, 0.04259378835558891, 0.013805950060486794, -0.000926888023968786, 0.03156478703022003, -0.010552387684583664, -0.01848777011036873, -0.037695787847042084, 0.04864494875073433, -0.009853307157754898, -0.01900371164083481, 0.026866190135478973, -0.08510293811559677, -0.020982330664992332, -0.0010560571681708097, 0.03521820530295372, 0.02425103262066841, 0.04154380038380623, 0.015209326520562172, -0.023058230057358742, -0.02073162980377674, -0.00881422683596611, -0.03437693789601326, 0.04885832592844963, -0.03684130311012268, 0.041908830404281616, 0.03726545348763466, -0.031555142253637314, -0.025085479021072388, -0.025665991008281708, 0.02009359560906887, 0.005692091770470142, 0.0015471898950636387, 0.023352358490228653, -0.010443704202771187, -0.0007931442814879119, 0.0389215461909771, 0.013341039419174194, 0.00792199932038784, 0.08238022774457932, -0.010153889656066895, -0.08826595544815063, 0.003708837553858757, -0.00790178868919611, -0.04541262984275818, 0.034236326813697815, 0.007414258550852537, -0.04952846094965935, 0.03958142548799515, 0.0012699613580480218, 0.07592562586069107, -0.03631739318370819, -0.007693612482398748, 0.022386768832802773, -0.031089749187231064, 0.006291577126830816, 0.03324173390865326, 0.013601651415228844, -0.043673060834407806, 0.014343158341944218, -0.031789641827344894, -0.016721952706575394, -0.001181011670269072, 0.0014951539924368262, 0.02646714821457863, 0.03900761902332306, -0.13507145643234253, -2.1695657778764144e-05, 0.016054658219218254, -0.04502986744046211, -0.021668104454874992, 0.028080036863684654, -0.03338281065225601, 0.025173330679535866, -0.03887387365102768, -0.013341051526367664, 0.045678529888391495, -0.015623877756297588, -0.00323879555799067, -0.006672833114862442, -0.05083855241537094, 0.040168989449739456, 0.02950318716466427, 0.020274553447961807, 0.06246742978692055, -0.01947663724422455, -0.006250296253710985, 0.033749498426914215, -0.019073044881224632, -0.0012707780115306377, 0.05592648312449455, 0.017843112349510193, 0.03027539886534214, -0.010540924035012722, -0.005811755545437336, -0.020448779687285423, -0.018625767901539803, 0.01837206445634365, 0.002297384897246957, -0.008037511259317398, 0.038389649242162704, -0.023721015080809593, 0.03076195903122425, 0.03552326560020447, 0.023314451798796654, -0.02375273033976555, 0.02098756842315197, 0.027866147458553314, 0.022313034161925316, -0.0298688355833292, -0.015213570557534695, 0.0008680954342707992, -0.013819228857755661, -0.0043741436675190926, 0.03083951584994793, -0.0033141898456960917, 0.04076112434267998, -0.04712656885385513, -0.013860278762876987, 0.03981253132224083, -0.013642427511513233, 0.05852948874235153, -0.03348103165626526, -9.035384573508054e-05, 0.0424821637570858, 0.0040221065282821655, -0.011862930841743946, -0.0358462892472744, -0.00833798572421074, 0.014579486101865768, -0.02034306339919567, 0.04573488608002663, -0.048322319984436035, -0.039896924048662186, 0.02610732987523079, 0.053058914840221405, -0.005257606040686369, -0.01872539520263672, 0.0030635499861091375, -0.016886241734027863, -0.04179335758090019, -0.006699382793158293, 0.03158465400338173, 0.0653468519449234, -0.008893278427422047, -0.05161214992403984, -0.04014836251735687, 0.03632934018969536, 0.021560199558734894, -0.0030419768299907446, 0.04321299493312836, 0.02225159853696823, 0.002754639368504286, -0.04505162686109543, 0.06500685960054398, 0.01167864166200161, -0.034678924828767776, 0.04138224944472313, -0.01842549629509449, -0.0708552896976471, 0.008461566641926765, 0.05401470139622688, -0.019911985844373703, -0.010294688865542412, 0.09427517652511597, 0.02156023308634758, -0.07226941734552383, -0.055048197507858276, 0.0051818047650158405, -0.04928750917315483, 0.011914144270122051, 0.04594628885388374, 0.0030446224845945835, -0.03713378682732582, 0.030263403430581093, 0.009819657541811466, -0.04331134259700775, 0.00227575795724988, -0.018270086497068405, -0.06504759192466736, 0.021116551011800766, 0.00029753701528534293, 0.01646040566265583, -0.011993364430963993, -0.033073969185352325, -0.03747263923287392, -0.0426548533141613, 0.0016005506040528417, 0.0736650824546814, -0.04177403450012207, 0.04702145978808403, 0.03837857022881508, -0.05159685015678406, 0.010306221432983875, -0.0007469553966075182, 0.018175018951296806, 0.04068892449140549, -0.019249115139245987, 0.012687825597822666, -0.05245411768555641, -0.009167675860226154, -0.023831289261579514, 0.01964111253619194, 0.007377552799880505, 0.014204662293195724, 0.04547350853681564, 0.005664472468197346, 0.006113715935498476, -0.06038382649421692, 0.028820138424634933, -0.03317975625395775, -0.04722796380519867, 0.0030220444314181805, 0.019605185836553574, 0.047453586012125015, -0.03490660339593887, -0.008620934560894966, 0.0012332773767411709, 0.022557426244020462, -0.009802593849599361, -0.014318830333650112, -0.0054045068100094795, 0.020086942240595818, 0.03582363948225975, 0.011382166296243668, -0.009027134627103806, -0.01102243922650814, 0.023939859122037888, -0.006164007354527712, 0.0033958274871110916, -0.006284547038376331, -0.05482916533946991, 0.033326078206300735, 0.008532715030014515, -0.07069732248783112, 0.02062596008181572, 0.009252901189029217, -0.05240609496831894, 0.03851873800158501, 0.07828879356384277, 0.013988192193210125, 0.040955785661935806, -0.09619496762752533, -0.09545956552028656, 0.007432819809764624, -0.06277358531951904, -0.006722668185830116, -0.004455231595784426, -0.029110366478562355, 0.052251722663640976, 0.006692181807011366, 0.029053829610347748, 0.05157654359936714, -0.053826648741960526, 0.05922183021903038, 0.004383416380733252, 0.0029034793842583895, -0.009292484261095524, -0.0667487308382988, -0.01939607597887516, 0.02696274034678936, 0.013988045044243336, 0.059236761182546616, 0.002888416638597846, 0.004973513539880514, -0.01084493100643158, -0.03393896669149399, 0.003472422482445836, 0.0035342243500053883, -0.0486472025513649, 0.007264544256031513, -0.007920731790363789, 0.015597708523273468, 0.01313565019518137, -0.004419966135174036, -0.019788257777690887, 0.011834892444312572, 0.004210616461932659, 0.004688818473368883, -0.04259055480360985, 0.007920368574559689, 0.030654333531856537, -0.003336508758366108, 0.0006730898167006671, -0.004574465099722147, 0.06419017910957336, 0.03328429162502289]\n",
            "[-0.006332166027277708, -0.00822352897375822, -0.014464504085481167, -0.05680663511157036, 0.009324422106146812, 0.0034724415745586157, 0.01438699010759592, -0.04281593859195709, 0.026579787954688072, 0.010314357466995716, 0.05600443482398987, -0.0077405814081430435, 0.03188120201230049, 0.019051309674978256, -0.012084736488759518, 0.0006474329857155681, 0.025722267106175423, 0.019833169877529144, -0.012804540805518627, -0.051161814481019974, 0.01274709403514862, -0.018077002838253975, 0.0006599840126000345, -0.0018343885894864798, 0.07481805980205536, -0.03220312297344208, 0.03170042857527733, -0.0496043860912323, -0.01436623465269804, 0.05247478187084198, -0.07438939809799194, 0.05319599434733391, -0.08905528485774994, -0.004539891146123409, -0.007467073854058981, -0.02387746423482895, -0.005717173218727112, -0.011360274627804756, -0.001178929815068841, 0.08863552659749985, 0.0044535440392792225, -0.003807578468695283, -0.032894279807806015, -0.04083385691046715, 0.030320018529891968, -0.018680255860090256, 0.006341264583170414, 0.020881125703454018, 0.0035658932756632566, -0.05230938643217087, 0.0009052691166289151, 0.017750155180692673, 0.031087886542081833, -0.03909201920032501, -0.03184995427727699, -0.03367836773395538, 0.009556821547448635, 0.03411555662751198, -0.04403672739863396, -0.025905828922986984, 0.02140767127275467, 0.014590607024729252, -0.01976892724633217, 0.04207693785429001, 0.0133728152140975, 0.007383553311228752, -0.07653940469026566, 0.057538047432899475, 0.026084309443831444, -0.056671466678380966, 0.04390762746334076, -0.026888687163591385, 0.01908860169351101, -0.049889691174030304, -0.04149121791124344, -0.008219531737267971, -0.03214683756232262, 0.07472586631774902, 0.009675280191004276, -0.02018657512962818, 0.05356384813785553, -0.03166160359978676, 0.0031261269468814135, 0.00035878107883036137, -0.05731692537665367, 0.06256542354822159, -0.04949212074279785, -0.012230362743139267, -0.018528858199715614, 0.06648014485836029, -0.028633499518036842, 0.040603503584861755, -0.01527594868093729, -0.07021017372608185, 0.04457944259047508, 0.0031444295309484005, 0.012913639657199383, -0.0029484869446605444, -0.016442833468317986, -0.039709191769361496, -0.005683897528797388, -0.023486675694584846, -0.04422970116138458, 0.018331078812479973, 0.027376793324947357, 0.008549010381102562, 0.013967692852020264, 0.056536972522735596, -0.006944487802684307, -0.0037593375891447067, -0.036546602845191956, 0.013240653090178967, -0.005799820646643639, -0.043644633144140244, 0.04145650193095207, -0.01681729592382908, -0.05503241345286369, 0.054537806659936905, 0.023914525285363197, 0.004378036130219698, -0.01223444752395153, 0.008723508566617966, 0.06534495949745178, -0.013200300745666027, -0.002521870192140341, 0.012102282606065273, -0.007893688045442104, 0.019675863906741142, 0.005670183803886175, 0.0235742200165987, -0.04653119295835495, -0.06479053944349289, -0.017249969765543938, 0.008806219324469566, 0.04770113527774811, 0.06243157386779785, 0.06225805729627609, 0.012229103595018387, 0.056920815259218216, 0.04495227709412575, 0.0050054206512868404, 0.01994835026562214, -0.022710168734192848, -0.009610173292458057, 0.00883969385176897, 0.050657037645578384, -0.025265468284487724, -0.06010043993592262, -0.011994162574410439, -0.006970300804823637, -0.0027360415551811457, -0.016772858798503876, -0.05334392189979553, 0.03094140999019146, 0.0749802365899086, 0.03730805218219757, -0.0004868867981713265, -0.01346841175109148, 0.016997165977954865, 0.014580465853214264, 0.05846763029694557, 0.03106069378554821, 0.02590048313140869, 0.008903543464839458, 0.04764170944690704, -0.05475321412086487, 0.00526118790730834, -0.001267503364942968, -0.012367586605250835, -0.016237078234553337, -0.011895635165274143, 0.018267054110765457, -0.04517221450805664, -0.05446302145719528, 0.0028297100216150284, -0.07901404052972794, -0.002109702443704009, -0.04214769974350929, -0.019700301811099052, -0.03537498787045479, -0.01990681327879429, -0.04373051971197128, -0.031143777072429657, 0.0280934888869524, 0.0036479937843978405, -0.02598438784480095, 0.052178144454956055, -0.0026143882423639297, -0.018596434965729713, -0.0024850349873304367, -0.011735893785953522, 0.012306573800742626, -0.02950161322951317, 0.024958811700344086, 0.009245157241821289, 0.01619976945221424, 0.0011012167669832706, -0.004583443980664015, 0.005106269847601652, -0.04255131632089615, -0.011067737825214863, 0.08707249909639359, -0.015222282148897648, 0.01964983157813549, 0.017697390168905258, 0.023367833346128464, 0.06822340935468674, -0.023420805111527443, 0.00851165410131216, 0.04357055947184563, 0.0033629355020821095, 0.013211466372013092, -0.037161022424697876, -0.018958065658807755, 0.009195245802402496, 0.03374553471803665, 0.0503048375248909, 0.03234122693538666, 0.027880877256393433, -0.019160529598593712, -0.015961211174726486, -0.032207272946834564, -0.013776656240224838, 0.06494968384504318, -0.008976002223789692, 0.009217838756740093, -0.017863819375634193, -0.000179340218892321, -0.014248539693653584, -0.06959766894578934, 0.0005034460336901248, 0.05081868916749954, -0.029033692553639412, -0.054429005831480026, 0.060356028378009796, 0.02009434811770916, -0.007180685177445412, 0.0024295635521411896, 0.012292956933379173, 0.025607775896787643, -0.033042557537555695, 0.03182971104979515, 0.09817341715097427, 0.03828128054738045, -0.02921072207391262, -0.041004203259944916, 0.010082505643367767, 0.02144678868353367, -0.01952504739165306, 0.04563678056001663, -0.0005767563707195222, -0.0734441801905632, 0.018948154523968697, 0.003481189953163266, -0.06789784133434296, 0.016766902059316635, -0.0465465746819973, 0.04862577095627785, 0.0013537199702113867, 0.01806773990392685, 0.059684157371520996, -0.031093960627913475, 0.003590796608477831, -0.009707399643957615, -0.05589175224304199, 0.016218632459640503, 0.0045119537971913815, -0.06887847930192947, 0.01570233330130577, -0.00435404060408473, 0.02753852866590023, -0.06483042985200882, 0.054110296070575714, 0.030400734394788742, 0.012666002847254276, 0.009101622737944126, -0.03019559383392334, 0.01685759425163269, 0.01127224788069725, -0.017107682302594185, 0.04425226151943207, 0.040421538054943085, 0.0692676231265068, -0.045873869210481644, 0.001799353281967342, -0.042142849415540695, -0.08645506948232651, 0.00037267725565470755, -0.008230806328356266, -0.010932687669992447, -0.03542396426200867, -0.04157625883817673, -0.03963689133524895, -0.038282930850982666, -0.045541223138570786, -0.007115588523447514, -0.00986792054027319, 0.04499901831150055, 0.045585740357637405, -0.001969846896827221, -0.008020787499845028, 0.029372410848736763, -0.007574901916086674, -0.08391363173723221, 0.007323253434151411, -0.016556745395064354, -0.009944021701812744, -0.06365818530321121, -0.01592523790895939, 0.032241564244031906, -0.0349540188908577, -0.02536497637629509, -0.0236594770103693, -0.015519935637712479, 0.04556244611740112, 0.03729462996125221, -0.01044116634875536, 0.014047646895051003, -0.044568102806806564, -0.006697603967040777, -0.010848555713891983, 0.06686494499444962, -0.020082199946045876, -0.02836650237441063, -0.01621049828827381, -0.010868759825825691, -0.009586572647094727, 0.0723666101694107, 0.0493353009223938, -0.03850741684436798, -0.03824865445494652, 0.035955626517534256, -0.05630674585700035, 0.04673558846116066, 0.006996144540607929, 0.01332033146172762, -0.03728889301419258, 0.05801180750131607, -0.07026010751724243, 0.016898375004529953, 0.07401490956544876, 0.013965525664389133, -0.0212226714938879, -0.03751461207866669, -0.02838832512497902, 0.00282649090513587, -0.029189126566052437, 0.03619273379445076, 0.08719077706336975, -0.010461793281137943, -0.002568766474723816, 0.06550285220146179, -0.002613422693684697, 0.005313934292644262, -0.011251251213252544, -0.07948709279298782, 0.010855510830879211, -0.03457155451178551, 0.016783662140369415, -0.04165758565068245, -0.020406430587172508, 0.0715617761015892, -0.015655532479286194, 0.026090404018759727, -0.06659998744726181, 0.05224449560046196, 0.056511007249355316, 0.017046598717570305, -0.018007047474384308, 0.01662961207330227, -0.002810560166835785, -0.03735863044857979, -0.026178767904639244, -0.01765221729874611, -0.03127446398139, -0.04251177981495857, -0.055521100759506226, -0.029061391949653625, 0.009475814178586006, 0.02411748841404915, 0.024302106350660324, -0.01916712336242199, 0.06904362142086029, 0.04810529202222824, 0.006879673805087805, -0.020226402208209038, 0.0362129844725132, -0.007707476150244474, -0.007303836289793253, 0.038551248610019684, 0.04516085982322693, 0.016278481110930443, 0.07324723899364471, 0.06400588154792786, -0.042708270251750946, 0.006876244675368071, 0.008690990507602692, -0.06383899599313736, 0.005963462870568037, -0.024305257946252823, 0.02000952884554863, 0.005981534253805876, -0.03734573349356651, -0.017394481226801872, 0.009905540384352207, -0.005490302573889494, 0.019328374415636063, -0.010236674919724464, 0.009785202331840992, 0.018778719007968903, 0.02250603586435318, -0.010842468589544296, 0.06498701870441437, -0.022937873378396034, -0.07191147655248642, -0.03074895776808262, 0.05830919370055199, 0.011134573258459568, -0.013015764765441418, 0.030235735699534416, 0.0004525792901404202, -0.01071858312934637, -0.013246256858110428, -0.026432542130351067, -0.0075041851960122585, -0.045131176710128784, 0.014123723842203617, -0.023337209597229958, 0.008039399050176144, 0.03732844069600105, 0.03261367231607437, -0.016295919194817543, 0.011899242177605629, -0.04125257581472397, -0.0360620841383934, -0.04083339869976044, 0.027883945032954216, 0.012370949611067772, -0.0330536775290966, 0.012007993645966053, -0.023094192147254944, -0.031121373176574707, 0.06505684554576874, -0.014722189866006374, -0.0654212087392807, -0.028665632009506226, -0.02615348994731903, -0.04427174851298332, 0.07924190908670425, -0.1002398431301117, 0.011774376034736633, -0.056673940271139145, 0.01424897275865078, -0.07334012538194656, -0.026393042877316475, -0.026391349732875824, 0.02103222720324993, 0.04178436100482941, -0.015826821327209473, 0.02259708009660244, -0.022911436855793, -0.0338742621243, -0.003314427100121975, -0.07270954549312592, 0.02440410852432251, -0.027278529480099678, 0.057143885642290115, 0.012586717493832111, 0.04048451781272888, 0.05638353154063225, 0.017322992905974388, 0.002955250209197402, 0.012201902456581593, 0.027013855054974556, -0.03360707312822342, 0.03853802755475044, -0.05042532831430435, 0.04623866081237793, 0.04317362606525421, -0.03579794615507126, -0.00826738215982914, 0.007685997989028692, 0.04648011550307274, -0.016383161768317223, -0.027333442121744156, 0.023596294224262238, 0.005037402268499136, -0.015474437735974789, -0.028902791440486908, 0.05736904591321945, -0.00972397904843092, -0.016784504055976868, 0.020581595599651337, -0.0358554981648922, -0.041382405906915665, -0.015182760544121265, 0.034463297575712204, 0.049216561019420624, 0.03603394329547882, 0.005361166782677174, -0.001956503139808774, -0.03463797643780708, 0.010307135060429573, -0.023112192749977112, 0.06212340667843819, -0.051610104739665985, 0.02416435070335865, 0.01722479984164238, -0.021909696981310844, -0.03314421325922012, -0.019558336585760117, 0.023371903225779533, 0.0006045065820217133, -0.017339732497930527, -0.011083601973950863, 0.029917342588305473, 0.010519241914153099, 0.030903294682502747, -0.014101543463766575, 0.00886147003620863, 0.07726714760065079, -0.014075946994125843, -0.1010916605591774, -0.006181885953992605, -0.00011818493658211082, -0.04697272554039955, 0.032761719077825546, 0.02796330489218235, -0.022617796435952187, 0.022733259946107864, -0.010036539286375046, 0.08836553245782852, -0.036275431513786316, -0.012090381234884262, 0.00838710367679596, -0.025823574513196945, -0.0037644929252564907, 0.03884034976363182, -0.0002816852065734565, -0.026303429156541824, 0.04891647398471832, -0.048927754163742065, -0.015803853049874306, 0.007156419567763805, -0.0037000898737460375, 0.020839646458625793, 0.054395198822021484, -0.1271219700574875, -0.0007126582786440849, -0.026926366612315178, -0.055495429784059525, -0.0308977123349905, -0.002688049804419279, -0.03751259297132492, 0.008813347667455673, -0.006400954443961382, -0.033489104360342026, 0.016286266967654228, -0.0373542420566082, -0.007113006431609392, -0.03364536166191101, -0.03855767846107483, 0.07387395948171616, 0.022095952183008194, 0.04845088720321655, 0.04654344543814659, -0.007086847443133593, 0.02162167802453041, 0.05996650084853172, 0.01523300539702177, -0.004160447046160698, 0.06727038323879242, 0.04506966471672058, -0.02884279377758503, 0.016462424769997597, 0.0023751724511384964, -0.03576979041099548, 0.0074481964111328125, 0.018824676051735878, -0.02269722707569599, 0.010151311755180359, 0.04736117273569107, -0.004762188531458378, 0.022513914853334427, 0.016081098467111588, 0.07208140939474106, -0.006282265763729811, -0.035358328372240067, 0.04348227009177208, -0.010724597610533237, -0.024346180260181427, -0.031239351257681847, -0.030752314254641533, -0.004488928243517876, -0.008404526859521866, 0.0364457368850708, -0.00010694823140511289, 0.048931803554296494, -0.027220575138926506, -0.031127355992794037, 0.06168631464242935, -0.02230219729244709, 0.0673442855477333, -0.07004055380821228, 0.03433092311024666, 0.03939211741089821, 0.0063563683070242405, -0.02993098832666874, -0.03053787350654602, -0.017320914193987846, 0.029356107115745544, -0.007400559727102518, -0.001063718693330884, -0.08094216138124466, -0.04971543699502945, 0.010527956299483776, 0.05319151654839516, -0.020878326147794724, -0.04340608790516853, -0.002123191487044096, -0.004895600490272045, -0.024794306606054306, -0.016330623999238014, 0.014798174612224102, 0.04500267654657364, 0.005800032056868076, -0.05775583162903786, -0.044906847178936005, 0.05679899826645851, -0.0026284148916602135, -0.004540069028735161, 0.05726579204201698, 0.015373311936855316, 0.0037175938487052917, -0.04462716728448868, 0.04449509456753731, 0.019130278378725052, -0.03844049572944641, 0.06321712583303452, -0.031549688428640366, -0.10324614495038986, -0.009517990984022617, 0.045298296958208084, -0.0179174542427063, -0.0006701464299112558, 0.08859806507825851, 0.01353198941797018, -0.06984159350395203, -0.028150038793683052, 0.017993653193116188, -0.0452108159661293, 0.029224826022982597, 0.007014616392552853, 0.003665035590529442, -0.013532786630094051, 0.03266507759690285, -0.034742265939712524, -0.020327800884842873, 0.04707027226686478, 0.018165981397032738, -0.07866979390382767, 0.02604988031089306, -0.0016427671071141958, -0.0014642373425886035, -0.0007573147304356098, -0.03333986923098564, -0.03981851413846016, -0.04702983796596527, -0.006005658768117428, 0.07156381011009216, -0.02430306188762188, 0.05201772227883339, 0.03854680061340332, -0.0416838563978672, 0.00575868459418416, 0.020131174474954605, 0.015297907404601574, 0.038923610001802444, -0.054991573095321655, 0.025102682411670685, -0.03900117054581642, -0.022632533684372902, -0.0344005785882473, 0.04712942615151405, 0.018979281187057495, 0.03914406895637512, 0.019407618790864944, 0.008313983678817749, 0.007092860992997885, -0.07480093091726303, 0.016998428851366043, 0.008450815454125404, -0.03562851622700691, 0.01646801270544529, 0.038919564336538315, 0.03906508535146713, -0.023946324363350868, 0.0005150760407559574, -0.009831213392317295, 0.04780401661992073, -0.022037258371710777, 0.0035951053723692894, -0.0029385071247816086, 0.0015799847897142172, -0.004059208557009697, 0.044138070195913315, 0.019071077927947044, -0.01370670460164547, 0.022095631808042526, 0.009637444280087948, -0.006692444905638695, -0.028850538656115532, -0.01834847778081894, 0.04889790713787079, 0.021382395178079605, -0.08816438168287277, 0.05290139466524124, -0.005105198360979557, -0.04378552734851837, 0.03775044530630112, 0.0970325618982315, 0.031236477196216583, 0.02365771308541298, -0.07075947523117065, -0.05468496307730675, 0.03649242967367172, -0.05565623566508293, 0.024845078587532043, 0.0013713219668716192, -0.00873659085482359, 0.026381943374872208, 0.006252927239984274, 0.01170879416167736, 0.07334553450345993, -0.05454261973500252, 0.058864761143922806, 0.004063171800225973, 0.03257272019982338, 0.012366145849227905, -0.05245770514011383, 0.005463084205985069, 0.029185254126787186, 0.019454581663012505, 0.06921549141407013, 0.005755291786044836, -0.015896691009402275, 0.00431612366810441, 0.004982156213372946, 0.0003810603229794651, 0.009668832644820213, -0.07874227315187454, 0.0008262699120678008, -0.007617757190018892, 0.024712571874260902, 0.017329618334770203, -0.021482758224010468, -0.012106366455554962, -0.0024927551858127117, 0.005462105851620436, -0.010723200626671314, -0.02634696662425995, 0.057716820389032364, 0.0035632692743092775, 0.0015746088465675712, 0.007447585929185152, -0.012017943896353245, 0.03510730341076851, 0.05536537244915962]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "query_result = embd.embed_query(question)\n",
        "document_result = embd.embed_query(document)\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e0e35f-6861-4c5e-9301-04fd5408f8f8",
      "metadata": {
        "id": "f5e0e35f-6861-4c5e-9301-04fd5408f8f8"
      },
      "source": [
        "[Cosine similarity](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions) is reccomended (1 indicates identical) for OpenAI embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "b8001998-b08c-4560-b124-bfa1fced8958",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8001998-b08c-4560-b124-bfa1fced8958",
        "outputId": "aaceec97-6bb0-4644-ad98-5048a39ef010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.8535652119095083\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "print(\"Cosine Similarity:\", similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aea73bc-98e3-4fdc-ba72-d190736bed20",
      "metadata": {
        "id": "8aea73bc-98e3-4fdc-ba72-d190736bed20"
      },
      "source": [
        "[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5778c31a-6138-4130-8865-31a08e82b9fb",
      "metadata": {
        "id": "5778c31a-6138-4130-8865-31a08e82b9fb"
      },
      "outputs": [],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798e731e-c6ff-46e3-a8bc-386832362af2",
      "metadata": {
        "id": "798e731e-c6ff-46e3-a8bc-386832362af2"
      },
      "source": [
        "[Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
        "\n",
        "> This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e668d339-3951-4662-8387-c3d296646906",
      "metadata": {
        "collapsed": true,
        "id": "e668d339-3951-4662-8387-c3d296646906"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427303a1-3ed4-430c-bfc7-cb3e48022f1d",
      "metadata": {
        "id": "427303a1-3ed4-430c-bfc7-cb3e48022f1d"
      },
      "source": [
        "[Vectorstores](https://python.langchain.com/docs/integrations/vectorstores/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa90aaf-cc1b-46a1-9fba-cf20804dcb41",
      "metadata": {
        "id": "baa90aaf-cc1b-46a1-9fba-cf20804dcb41"
      },
      "outputs": [],
      "source": [
        "# Index\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba890329-1411-4922-bd27-fe0490dd1208",
      "metadata": {
        "id": "ba890329-1411-4922-bd27-fe0490dd1208"
      },
      "source": [
        "## Part 3: Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930",
      "metadata": {
        "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930"
      },
      "outputs": [],
      "source": [
        "# # Index\n",
        "# from langchain_google_genai import google_vector_store\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "# vectorstore = Chroma.from_documents(documents=splits,\n",
        "#                                     embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
        "\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "57c2de7a-93e6-4072-bc5b-db6516f96dda",
      "metadata": {
        "id": "57c2de7a-93e6-4072-bc5b-db6516f96dda"
      },
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\"what is NER?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "db96f877-60d3-4741-9846-e2903831583d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db96f877-60d3-4741-9846-e2903831583d",
        "outputId": "3c399d7a-d949-45dd-8b57-56bc8f598af1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "omcvmTTyYSWF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omcvmTTyYSWF",
        "outputId": "0909935a-54a5-43c4-8d0e-e0b566c28029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, page_content='### 5.1 SMILES reconstruction\\n\\nForty-six tests (9x5+1 for more details see 4.1 ) were conducted to assess the accuracy of SMILES reconstruction on different portion of training data. The results obtained are detailed in Table 6 .\\nTable 6 SMILES reconstruction on different portion of training data\\nFull size table\\nChanges in accuracy and editing distance for different size of training data sets are presented in Figs. 13 and 14 . As it can be seen from Fig. 13 , the reconstruction accuracy increases from 0.247 \\\\(\\\\pm \\\\) 0.027 for 10% of randomly selected samples to 0.877 \\\\(\\\\pm \\\\) 0.009 for 50% of samples accordingly. From 60% onwards accuracy stays around 0.8 on average with slight fluctuations. This is an expected result. However slight variations in accuracy starting from 60% of samples needs to be addressed. It is a difficult task to identify the exact reason of what is influencing these changes in accuracy. One of the possible reasons for such behaviour is early signs of overfitting. Obviously there is no evidence of this phenomenon spotted during training. However, it is possible that with increasing of samples number a model starts to memorize the input. This can be addressed by a better sampling algorithm. For example, Butina clustering [ 13 ] can be a useful technique to design a sampling algorithm.\\nIn addition to accuracy, model performance was measured using Hamming and Levenshtein distances. Both belong to a family of editing distances and give a different perspective on the results obtained. They show a similar trend to accuracy. The Hamming distance decreased from 4.374 \\\\(\\\\pm \\\\) 0.817 to 0.663 \\\\(\\\\pm \\\\) 0.071 for 10% and 50% of sample cases respectively. The Levenshtein distance decreased from 4.299 \\\\(\\\\pm \\\\) 0.127 to 0.648 \\\\(\\\\pm \\\\) 0.031 for the same percentage of samples. Both show slight fluctuation in editing distance from 60% onwards.\\nSMILES reconstruction accuracy for different percentage of training sample\\nFull size image\\nSMILES reconstruction editing distances for different percentages of training samples\\nFull size image\\nAccording to the carried out experiment the best result (accuracy 0.877 \\\\(\\\\pm \\\\) 0.009 ) was obtained for 50% of randomly selected samples. This configuration was selected for building a production variational autoencoder , which can be later used for training classification and regression models with SMILES input. Training was conducted using 40 epochs, and produced a model with accuracy 0.872 . The editing distances for the production model were 0.682 and 0.677 for Hamming and Levenshtein respectively.\\n\\n### 5.2 LogD prediction'), Document(metadata={'subject': '', 'title': '', 'author': '', 'moddate': '', 'pages': 5, 'total_pages': 16, 'producer': 'Skia/PDF m66', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'modDate': '', 'keywords': '', 'creationDate': '', 'page': 5, 'type': 'pdf', 'creationdate': '', 'format': 'PDF 1.5', 'creator': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'trapped': ''}, page_content='Figure . Heatmaps are here depicted representing the computed attention during character prediction in the order left to right, top to bottom: [, @, , /, =, . The complete encoder-decoder framework is fully differentiable and was trained end-to-end using  suitable form of backpropagation, enabling SMILES to be fully generated using only raw images as input. During decoding SMILES were generated  character at  time, from left to right. Additionally, no external dictionary was used for chemical abbreviations (superatoms) rather these were learned as part of the model, thus images may contain superatoms and the SMILES are still generated  character at  time. This model operates on raw images and directly generates chemically valid SMILES with no explicit subcomponent recognition required. Datasets Segmentation Dataset To our knowledge, no dataset addressing molecular structure segmentation has been published. To provide sufficient data to train  neural network while minimizing manual effort required to curate such  dataset, we developed  pipeline for automatically generating segmentation data. To programmatically generate data, in summary, the following steps were performed: ) remove structures from journal and patent pages, ii) overlay structures onto the pages, iii) produce  ground truth mask identifying the overlaid structures, and iv) randomly crop images from the pages containing structures and the corresponding mask. In detail, OSRA\\u200b was utilized to identify bounding boxes of candidate molecules within the pages of  large number of publications, both published journal articles and patents. The regions expected to contain molecules were whited-out, thus leaving pages without molecules. OSRA was not always correct in finding structures and occasionally non-structures (.., charts) were removed suggesting that cleaner input may further improve model performance. Next, images of molecules made publically available by the United States Patent and Trademark Office (USPTO)\\u200b28 were randomly overlaid onto the pages while ensuring that no structures overlapped with any non-white pixels. Structure images were occasionally perturbed using affine transformations, changes in background shade, and/or lines added around the structure (to simulate table grid lines). We also generated the true mask for each overlaid page; these masks were zero-valued except where pixels were part of  molecule (pixels assigned  value of ). During training, samples of 128x128 pixels were randomly cropped from the'), Document(metadata={'title': '', 'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'trapped': '', 'modDate': 'D:20181203013712Z', 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'creationDate': 'D:20181203013712Z', 'moddate': '2018-12-03T01:37:12+00:00', 'creationdate': '2018-12-03T01:37:12+00:00', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'author': '', 'subject': '', 'format': 'PDF 1.5', 'keywords': '', 'page': 1, 'total_pages': 13, 'pages': 1, 'type': 'pdf'}, page_content='and to construct features similar to engineered chemical features, with minimal assistance from an expert chemist. This approach that leverages representation learning of deep neural networks is  signiﬁcant departure from the traditional research paradigm in chemistry. In this work, we develop CheMixNetset of neural networks for predicting chemical properties by leveraging multiple molecular representations as inputs. We used simpliﬁed molecular-input line-entry system (SMILES) [10] notations as sequence inputs and molecular ﬁngerprints as vector inputs. SMILES is  line notation of chemical structures which encodes the connection table and the stereochemistry of  molecule as  line of text. Our work improves upon the existing state-ofthe-art approach of directly learning from vector representations such as molecular ﬁngerprints or chemical text representations such as SMILES by harnessing the network structure of both forms of representations. CheMixNet is  variation of multi-input-single-output (MISO) [11] architectures that learn the chemical properties from  mix of intermediate features learned from two different input representations -  vector input in the form of molecular ﬁngerprints and  sequence input in the form of SMILES strings. In our experiments, we used MACCS ﬁngerprints -  ﬁrst 2D representation of chemical structure using 167 features. Although MACCS usually perform worse than other molecular ﬁngerprints, we chose MACCS because of its simplicity and ease of interpretation. We perform signiﬁcant experimentation to determine the best neural network structure for the CheMixNet architectures. We evaluated the effectiveness of our mixed approach for building DNN architectures by training CheMixNet on six different datasetslarge dataset composed of . million samples from the Harvard Clean Energy Project (CEP) database and ﬁve other relatively smaller datasets from the MoleculeNet [12, 13] benchmark. Compared to other DL models, CheMixNet architecture outperforms fully connected MLP models trained on molecular ﬁngerprints, recurrent neural networks (RNN) and -dimensional convolutional neural network (CNN) models trained on SMILES, as well as other models - convolutional molecular graphs (ConvGraph) [14] and Chemception [15]. For instance, we achieved  mean absolute percentage error (MAPE) of .24 % on the CEP dataset; this is signiﬁcantly better than the MAPE of .43 % using CNN-RNN model. The CheMixNet architectures, as well as the benchmark models, are made accessible for the research community at https://github.com/paularindam/CheMixNet [16].  Background and Related Works In this section, we present  description of the two molecular representations we use in this work - SMILES and molecular ﬁngerprints, and discuss existing deep neural architectures for predictive modeling of chemical properties in the Quantitative structure-activity relationship (QSAR)/Quantitative structure-property relationship (QSPR) [17] modeling. . SMILES & Fingerprints Line notations are linear representations of chemical structures which encode the connection table and the stereochemistry of  molecule as  line of text [18]. SMILES [10] is the most popular speciﬁcation in the form of  line notation to describe the structure of chemical species using short ASCII strings encoding molecular structures and speciﬁc instances. One or more organic molecules attach to form long continuous chains known as branches. SMILES has  grammar structure in which alphabets denote atoms, special characters such as = and ≡bond denote the type of bonds, encapsulated numbers indicate rings, and parentheses represent side chains. In this work, we limit ourselves to character level representation and do not explicitly encode the grammar. Molecular ﬁngerprints are representations of chemical structures, successfully used in similarity search [19], clustering [20], classiﬁcations [21], drug discovery [22], and virtual screening [23],  standard and computationally efﬁcient abstract representation where structural features are represented by either bits in  bit string or counts in  count vector. Fingerprints were motivated by the need to ﬁnd materials that match target material properties. They follow the assumptions that the properties of the material is  direct function of its structure and that materials with similar structure are likely to have similar physical-chemical character. Different ﬁngerprints represent different aspects of  molecule, and thus each type of ﬁngerprint can have different suitability for mapping to particular physical property. Various machine learning (ML) algorithms have been used to predict the activity or property of chemicals using molecular descriptors and/or ﬁngerprints as input features. In our'), Document(metadata={'producer': 'Skia/PDF m66', 'creationDate': '', 'author': '', 'type': 'pdf', 'total_pages': 16, 'creationdate': '', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'keywords': '', 'modDate': '', 'pages': 4, 'moddate': '', 'trapped': '', 'title': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'format': 'PDF 1.5', 'page': 4, 'creator': '', 'subject': ''}, page_content=\"Figure . An example showing the output of the segmentation model when processing  journal article page from Salunke et al.\\u200b14 All of the text and other extraneous items are completely removed with the exception of  few faint lines that are amenable to automated post processing. state vector from the encoder was used to initialize the GridLSTM cell states and the SMILES sequence was then generated  character at  time, similar to the decoding method described in Sutskever et al.\\u200b26 (wherein sentences were generated  word at  time while translating English to French). Decoding is started by projecting  special start token into the GridLSTM (initialized by the encoder and conditioned on an initial context vector as computed by the attention mechanism), processing this input in the cell, and predicting the first character of the output sequence. Subsequent characters are produced similarly, with each prediction conditioned on the previous cell state, the current attention, and the previous output projected back into the network. The logits vector for each character produced by the network is of length , where  is the number of available characters (65 characters in this case).  softmax activation is applied to the logits to compute  probability distribution over characters, and the highest scoring character is selected for  particular step in the sequence. Sequences were generated until  special end-of-sequence token was predicted, at which point the completed SMILES string was returned. During inference we found accuracy improved when predicting images at several different (low) resolutions and returning sequences of the highest confidence, which was determined by multiplying together the softmax output of each predicted character in the sequence. The addition of an attention mechanism in the decoder helped solve several challenges. Most importantly, attention enabled the decoder to access information produced earlier in the encoder and minimized the loss of important details that may otherwise be overly compressed when encoding the state vector. Additionally, attention enabled the decoder to reference information closer to the raw input during the prediction of each character and was important considering the significance of pixelwise features in low resolution structure images. See Figure  for an example of the computed attention and how the output corresponds to various characters recognized during the decoding process. Apart from using attention for improved performance, the attention output is useful for repositioning  predicted structure into an orientation that better matches the original input image. This is done by converting the SMILES into  connection table using the open source Indigo toolkit,\\u200b27 and repositioning each atom in 2D space according to the coordinates of each character' computed attention. The repositioned structure then more closely matches the original positioning and orientation in the input image, enabling users to more easily identify and correct mistakes when comparing the output with the original source.\")]\n"
          ]
        }
      ],
      "source": [
        "print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2",
      "metadata": {
        "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2"
      },
      "source": [
        "## Part 4: Generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
        "outputId": "5aa89152-6331-45dd-9174-a0a14ba27881"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context from research papes to answer the question in detail minimum 500 words. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\n\"), additional_kwargs={})])"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context from research papes to answer the question in detail minimum 500 words. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "e4461264-5cac-479a-917c-9bf589826da4",
      "metadata": {
        "id": "e4461264-5cac-479a-917c-9bf589826da4"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d",
      "metadata": {
        "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d"
      },
      "outputs": [],
      "source": [
        "# Chain\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
        "outputId": "c3f8dd91-b2cd-4ce9-9c49-3a6e97527373"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Based on the context provided, NER refers to Named Entity Recognition, specifically in the context of the BC5CDR task. The experiment results for ChemProt relation extraction and BC5CDR NER indicate that pre-trained language models are generally the best solutions for these natural language processing tasks. Models like BioBERT (+PubMed) and RoBERTa achieve comparable results with Sci-BERT in these tasks.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--a5769e56-f382-4730-aec3-023040ddb2d5-0', usage_metadata={'input_tokens': 3546, 'output_tokens': 79, 'total_tokens': 3625, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run\n",
        "chain.invoke({\"context\":docs,\"question\":\"What is NER\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65770e2d-3d5e-4371-abc9-0aeca9646885",
      "metadata": {
        "id": "65770e2d-3d5e-4371-abc9-0aeca9646885"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53e5840-0a0f-4428-a4a4-6922800aff89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f53e5840-0a0f-4428-a4a4-6922800aff89",
        "outputId": "b4412e02-a986-4009-ae52-e3947dde645f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_hub_rag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffe29a1-5527-419e-9f12-8a3061d12885",
      "metadata": {
        "id": "8ffe29a1-5527-419e-9f12-8a3061d12885"
      },
      "source": [
        "[RAG chains](https://python.langchain.com/docs/expression_language/get_started#rag-search-example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "8208a8bc-c75f-4e8e-8601-680746cd6276",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8208a8bc-c75f-4e8e-8601-680746cd6276",
        "outputId": "47d79faa-bba7-4c4a-f329-c9b17ab2beba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I am sorry, but the provided context does not contain information about Task Decomposition and Self Reflection. Therefore, I cannot answer your question.'"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition and Self Reflection?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1CTl-nqddyg8",
      "metadata": {
        "id": "1CTl-nqddyg8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "TDS1RcnPdpNL",
      "metadata": {
        "id": "TDS1RcnPdpNL"
      },
      "source": [
        "# Basic RAG using PINECONE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CvBmv7vVdy_W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvBmv7vVdy_W",
        "outputId": "c04f7ec4-00e3-42e5-ef9e-e7d2a8dfee55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.2.8-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pinecone\n",
            "  Downloading pinecone-7.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain_pinecone) (0.3.66)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain_pinecone) (2.0.2)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain_pinecone)\n",
            "  Downloading langchain_tests-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langchain-openai>=0.3.11 (from langchain_pinecone)\n",
            "  Downloading langchain_openai-0.3.25-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.6.15)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.14.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.4.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.4.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (24.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (2.11.7)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain_pinecone) (1.91.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain_pinecone) (0.9.0)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (8.3.5)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.28.1)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting pytest-benchmark (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_benchmark-5.1.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pytest-codspeed (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_codspeed-3.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pytest-recording (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting vcrpy>=7.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading vcrpy-7.0.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.3)\n",
            "Requirement already satisfied: aiohttp>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (3.11.15)\n",
            "Collecting aiohttp-retry<3.0.0,>=2.9.1 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.23.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.4.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai>=0.3.11->langchain_pinecone) (2024.11.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from vcrpy>=7.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.17.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (9.0.0)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.17.1)\n",
            "Requirement already satisfied: rich>=13.8.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (13.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.1.2)\n",
            "Downloading langchain_pinecone-0.2.8-py3-none-any.whl (22 kB)\n",
            "Downloading pinecone-7.2.0-py3-none-any.whl (524 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.3/524.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.25-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_tests-0.3.20-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vcrpy-7.0.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_benchmark-5.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_codspeed-3.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (25 kB)\n",
            "Downloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, vcrpy, syrupy, pytest-socket, pytest-benchmark, pytest-asyncio, pinecone-plugin-assistant, pytest-recording, pytest-codspeed, pinecone, aiohttp-retry, langchain-tests, langchain-openai, langchain_pinecone\n",
            "Successfully installed aiohttp-retry-2.9.1 langchain-openai-0.3.25 langchain-tests-0.3.20 langchain_pinecone-0.2.8 pinecone-7.2.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7 pytest-asyncio-0.26.0 pytest-benchmark-5.1.0 pytest-codspeed-3.2.0 pytest-recording-0.13.4 pytest-socket-0.7.0 syrupy-4.9.1 vcrpy-7.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_pinecone pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb474b9-1e38-4a20-91cc-24cdce6d8631",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fb474b9-1e38-4a20-91cc-24cdce6d8631",
        "outputId": "96fd86b5-c977-4b69-eff1-6f4ea1714017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone vector store 'langchain-demo' created and retriever initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "import pinecone\n",
        "import os\n",
        "\n",
        "# Initialize Pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "\n",
        "# Create Pinecone index (if it doesn't exist)\n",
        "index_name = \"langchain-demo\"  # Choose a unique index name\n",
        "dimension = 768  # Dimension of Google's embedding-001 model\n",
        "\n",
        "# Check if index exists, if not create it\n",
        "if index_name not in [index.name for index in pc.list_indexes()]:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dimension,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1') # Specify cloud and region\n",
        "    )\n",
        "\n",
        "# Load blog (assuming this part is still needed for Pinecone indexing)\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)\n",
        "\n",
        "# Create vector store\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "# Get retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "print(f\"Pinecone vector store '{index_name}' created and retriever initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I_b7Fwb_gRgX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I_b7Fwb_gRgX",
        "outputId": "504def1e-0187-4738-8124-19f5d785d8b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing: https://portlandpress.com/biochemj/article/477/23/4559/227194/Deep-learning-and-generative-methods-in\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\n",
            "Error loading https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236: 403 Client Error: Forbidden for url: https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\n",
            "Failed to load document\n",
            "\n",
            "Processing: https://www.osti.gov/servlets/purl/1427646\n",
            "Successfully loaded 22 documents\n",
            "\n",
            "Processing: https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf\n",
            "Successfully loaded 16 documents\n",
            "\n",
            "Processing: http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf\n",
            "Successfully loaded 13 documents\n",
            "\n",
            "Processing: https://www.nature.com/articles/s41467-022-28494-3\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://link.springer.com/article/10.1007/s00521-021-05961-4\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.mdpi.com/journal/molecules/special_issues/deep_learning_structure\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\n",
            "Error loading https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\n",
            "Failed to load document\n",
            "\n",
            "Processing: https://www.mdpi.com/1420-3049/25/12/2764\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.nature.com/articles/s41598-025-95720-5\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://link.springer.com/article/10.1557/s43578-022-00628-9\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Summary:\n",
            "- Successfully loaded: 61 documents\n",
            "- Failed URLs: 2\n",
            "Failed URLs: ['https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236', 'https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050']\n",
            "- Total chunks after splitting: 199\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'PineconeVectorStore' object has no attribute '_collection'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-130-3038183343.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Unpack the tuple here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mvectorstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_vector_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pass only splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Get retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-130-3038183343.py\u001b[0m in \u001b[0;36mcreate_vector_store\u001b[0;34m(self, splits)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mindex_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m )\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nVector store created with {vectorstore._collection.count()} chunks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PineconeVectorStore' object has no attribute '_collection'"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "import fitz  # PyMuPDF\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader, PyMuPDFLoader\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import requests\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "from langchain.schema import Document\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "import pinecone\n",
        "import os\n",
        "\n",
        "# Initialize Pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "\n",
        "# Create Pinecone index (if it doesn't exist)\n",
        "index_name = \"langchain-demo\"  # Choose a unique index name\n",
        "dimension = 768  # Dimension of Google's embedding-001 model\n",
        "if index_name not in [index.name for index in pc.list_indexes()]:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dimension,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1') # Specify cloud and region\n",
        "    )\n",
        "class DocumentProcessor:\n",
        "    def __init__(self):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=5000,\n",
        "            chunk_overlap=250\n",
        "        )\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        self.ua = UserAgent()\n",
        "        self.headers = {\n",
        "            'User-Agent': self.ua.random,\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "        }\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(self.headers)\n",
        "\n",
        "    def load_html(self, url):\n",
        "        \"\"\"Enhanced HTML loader with better error handling\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Check if content-type is PDF\n",
        "            content_type = response.headers.get('Content-Type', '')\n",
        "            if 'application/pdf' in content_type:\n",
        "                return self.load_pdf_from_url(url)\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Try to find main content areas\n",
        "            article = (soup.find('article') or\n",
        "                      soup.find('main') or\n",
        "                      soup.find(class_=re.compile('content|main|body|post')) or\n",
        "                      soup.find('div', role='main') or\n",
        "                      soup)\n",
        "\n",
        "            # Extract all text with structure\n",
        "            content = self._extract_structured_content(article)\n",
        "            if not content:\n",
        "                raise ValueError(\"No content extracted from HTML\")\n",
        "\n",
        "            return [Document(page_content=content, metadata={'source': url, 'type': 'html'})]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {url}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _extract_structured_content(self, element):\n",
        "        \"\"\"Extract content while preserving document structure\"\"\"\n",
        "        content = []\n",
        "\n",
        "        def process_element(elem):\n",
        "            if isinstance(elem, bs4.NavigableString):\n",
        "                text = elem.strip()\n",
        "                if text and len(text) > 10:\n",
        "                    content.append(text)\n",
        "                return\n",
        "\n",
        "            tag = elem.name\n",
        "            if not tag:\n",
        "                return\n",
        "\n",
        "            text = elem.get_text(' ', strip=True)\n",
        "            if not text or len(text) <= 10:\n",
        "                return\n",
        "\n",
        "            # Handle headings\n",
        "            if tag.startswith('h') and tag[1:].isdigit():\n",
        "                level = int(tag[1:])\n",
        "                content.append(f\"\\n{'#'*level} {text}\\n\")\n",
        "            # Handle list items\n",
        "            elif tag == 'li':\n",
        "                content.append(f\"- {text}\")\n",
        "            # Handle table cells\n",
        "            elif tag in ['td', 'th']:\n",
        "                content.append(f\"[TABLE CELL] {text}\")\n",
        "            # Handle regular paragraphs\n",
        "            elif tag == 'p':\n",
        "                content.append(text)\n",
        "            # Recursively process containers\n",
        "            else:\n",
        "                for child in elem.children:\n",
        "                    process_element(child)\n",
        "\n",
        "        process_element(element)\n",
        "        full_text = '\\n'.join(content)\n",
        "        full_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n",
        "        full_text = re.sub(r'[ \\t]{2,}', ' ', full_text)\n",
        "        return full_text.strip()\n",
        "\n",
        "    def load_pdf_from_url(self, url):\n",
        "        \"\"\"Improved PDF loader with retries and better cleaning\"\"\"\n",
        "        max_retries = 3\n",
        "        retry_delay = 2\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = self.session.get(url, timeout=30)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "                    tmp_file.write(response.content)\n",
        "                    tmp_path = tmp_file.name\n",
        "\n",
        "                loader = PyMuPDFLoader(tmp_path)\n",
        "                docs = loader.load()\n",
        "\n",
        "                # Clean up the extracted text\n",
        "                for doc in docs:\n",
        "                    doc.page_content = self._clean_pdf_text(doc.page_content)\n",
        "                    doc.metadata.update({\n",
        "                        'source': url,\n",
        "                        'type': 'pdf',\n",
        "                        'pages': doc.metadata.get('page', '')\n",
        "                    })\n",
        "\n",
        "                os.unlink(tmp_path)\n",
        "                return docs\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed for {url}: {str(e)}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    if 'tmp_path' in locals() and os.path.exists(tmp_path):\n",
        "                        os.unlink(tmp_path)\n",
        "                    return []\n",
        "\n",
        "    def _clean_pdf_text(self, text):\n",
        "        \"\"\"Clean and normalize PDF text\"\"\"\n",
        "        # Remove page numbers and footers\n",
        "        text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        # Remove lonely characters\n",
        "        text = re.sub(r'(?<!\\w)\\w(?!\\w)', '', text)\n",
        "        # Fix hyphenated words\n",
        "        text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
        "        return text\n",
        "\n",
        "    def process_documents(self, urls):\n",
        "        \"\"\"Process documents with better error handling\"\"\"\n",
        "        all_docs = []\n",
        "        failed_urls = []\n",
        "\n",
        "        for url in urls:\n",
        "            print(f\"\\nProcessing: {url}\")\n",
        "            try:\n",
        "                if url.lower().endswith('.pdf'):\n",
        "                    docs = self.load_pdf_from_url(url)\n",
        "                else:\n",
        "                    docs = self.load_html(url)\n",
        "\n",
        "                if docs:\n",
        "                    all_docs.extend(docs)\n",
        "                    print(f\"Successfully loaded {len(docs)} documents\")\n",
        "                else:\n",
        "                    failed_urls.append(url)\n",
        "                    print(\"Failed to load document\")\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_urls.append(url)\n",
        "                print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "        if not all_docs:\n",
        "            raise ValueError(\"No documents were successfully loaded\")\n",
        "\n",
        "        print(f\"\\nSummary:\")\n",
        "        print(f\"- Successfully loaded: {len(all_docs)} documents\")\n",
        "        print(f\"- Failed URLs: {len(failed_urls)}\")\n",
        "        if failed_urls:\n",
        "            print(\"Failed URLs:\", failed_urls)\n",
        "\n",
        "        splits = self.text_splitter.split_documents(all_docs)\n",
        "        print(f\"- Total chunks after splitting: {len(splits)}\")\n",
        "        return splits, failed_urls # Return splits and failed_urls\n",
        "\n",
        "    def create_vector_store(self, splits):\n",
        "        \"\"\"Create and persist Chroma vector store\"\"\"\n",
        "        vectorstore = PineconeVectorStore.from_documents(\n",
        "        documents=splits,\n",
        "        embedding=self.embeddings,\n",
        "        index_name=index_name\n",
        ")\n",
        "        print(f\"Pinecone vector store '{index_name}' created successfully!\")\n",
        "        return vectorstore\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Your list of documents\n",
        "    documents = [\n",
        "        \"https://portlandpress.com/biochemj/article/477/23/4559/227194/Deep-learning-and-generative-methods-in\",\n",
        "        \"https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\",\n",
        "        \"https://www.osti.gov/servlets/purl/1427646\",\n",
        "        \"https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/\",\n",
        "        \"https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf\",\n",
        "        \"http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf\",\n",
        "        \"https://www.nature.com/articles/s41467-022-28494-3\",\n",
        "        \"https://link.springer.com/article/10.1007/s00521-021-05961-4\",\n",
        "        \"https://www.mdpi.com/journal/molecules/special_issues/deep_learning_structure\",\n",
        "        \"https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\",\n",
        "        \"https://www.mdpi.com/1420-3049/25/12/2764\",\n",
        "        \"https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6\",\n",
        "        \"https://www.nature.com/articles/s41598-025-95720-5\",\n",
        "        \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/\",\n",
        "        \"https://link.springer.com/article/10.1557/s43578-022-00628-9\"\n",
        "    ]\n",
        "\n",
        "    # Initialize and process\n",
        "    processor = DocumentProcessor()\n",
        "    splits, failed_urls = processor.process_documents(documents) # Unpack the tuple here\n",
        "    vectorstore = processor.create_vector_store(splits) # Pass only splits\n",
        "\n",
        "    # Get retriever\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    print(\"Vector store and retriever created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rkXtIp2HfEDg",
      "metadata": {
        "id": "rkXtIp2HfEDg"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "docs=retriever.get_relevant_documents(\"what is SMILE?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sCXvz2J9fYN4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCXvz2J9fYN4",
        "outputId": "4d9109b0-f7cd-44d2-99de-10698308588d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"SMILES, or Simplified Molecular Input Line Entry System, is a prevalent method for representing molecules in deep learning. It uses ASCII character strings to represent a molecule's chemical structure, encoding the connection table and stereochemistry as a line of text. Each element in the periodic table is assigned a corresponding token using its atomic symbol, with bond types inferred or explicitly indicated using non-alphanumeric tokens and brackets for branches or cycles.\\n\\nSMILES can be considered a chemical language with chemical tokens as words and molecules as sentences, but it can have syntactic and grammar errors, especially with branches and cycles. While SMILES is a non-unique molecular representation, it can be transformed into a unique one through canonicalization algorithms. DeepSMILES and SELFIES are SMILES-like notations developed to address some of the limitations of SMILES, such as grammatical errors and valency constraints.\\n\\nSMILES notations are used as sequence inputs for neural networks in predicting chemical properties. SMILES can be generated from raw images using deep learning models, which learn chemical abbreviations and generate chemically valid SMILES without explicit subcomponent recognition. SMILES reconstruction accuracy can be improved by increasing the size of the training data and using techniques like Butina clustering to address potential overfitting.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--3876f0a4-e400-4550-bde3-a5883a91fa80-0', usage_metadata={'input_tokens': 4447, 'output_tokens': 251, 'total_tokens': 4698, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"context\":docs,\"question\":\"What is SMILE tell me about it in detail\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0VgrLH3fgnXy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VgrLH3fgnXy",
        "outputId": "63093c19-fa17-46e8-edcb-09459c39342c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
