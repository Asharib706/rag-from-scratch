{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
      "metadata": {
        "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe"
      },
      "source": [
        "# Rag From Scratch: Overview\n",
        "\n",
        "These notebooks walk through the process of building RAG app(s) from scratch.\n",
        "\n",
        "They will build towards a broader understanding of the RAG langscape, as shown here:\n",
        "\n",
        "The topic for the RAG is\n",
        "##Research Papers in Deep Learning and Chemical Structures (Image Data)\n",
        "\n",
        "## Enviornment\n",
        "\n",
        "`(1) Packages`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "306e0202",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "306e0202",
        "outputId": "51322ccc-962d-4af7-c681-efb80efb243f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/161.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/161.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-2.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install fake_useragent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
        "outputId": "e117e53f-fe27-4c80-caea-d980ab12f04a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-1.0.13-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.66)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (8.5.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.1)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (24.2)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Collecting pybase64>=1.4.1 (from chromadb)\n",
            "  Downloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-5.4.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-33.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n",
            "  Downloading durationpy-0.10-py3-none-any.whl.metadata (340 bytes)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.34.1 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.34.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop>=0.15.1 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.5)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Downloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_google_genai-2.1.5-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading chromadb-1.0.13-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m92.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.1-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n",
            "Downloading kubernetes-33.1.0-py2.py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m89.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.34.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.34.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-5.4.0-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pybase64-1.4.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.10-py3-none-any.whl (3.9 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (453 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53803 sha256=04309d4bfae1b2875b558f0090c17696cd42648bca4fc42ff2020249980ebae7\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, filetype, durationpy, uvloop, types-requests, python-dotenv, PyMuPDF, pybase64, overrides, opentelemetry-proto, mypy-extensions, mmh3, marshmallow, humanfriendly, httpx-sse, httptools, bcrypt, backoff, watchfiles, typing-inspect, posthog, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, langchainhub, coloredlogs, pydantic-settings, opentelemetry-semantic-conventions, onnxruntime, kubernetes, dataclasses-json, opentelemetry-sdk, opentelemetry-exporter-otlp-proto-grpc, google-ai-generativelanguage, langchain-google-genai, chromadb, langchain_community\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDF-1.26.1 backoff-2.2.1 bcrypt-4.3.0 chromadb-1.0.13 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.10 filetype-1.2.0 google-ai-generativelanguage-0.6.18 httptools-0.6.4 httpx-sse-0.4.1 humanfriendly-10.0 kubernetes-33.1.0 langchain-google-genai-2.1.5 langchain_community-0.3.26 langchainhub-0.1.21 marshmallow-3.26.1 mmh3-5.1.0 mypy-extensions-1.1.0 onnxruntime-1.22.0 opentelemetry-api-1.34.1 opentelemetry-exporter-otlp-proto-common-1.34.1 opentelemetry-exporter-otlp-proto-grpc-1.34.1 opentelemetry-proto-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 overrides-7.7.0 posthog-5.4.0 pybase64-1.4.1 pydantic-settings-2.10.1 pypika-0.48.9 python-dotenv-1.1.1 types-requests-2.32.4.20250611 typing-inspect-0.9.0 uvloop-0.21.0 watchfiles-1.1.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "71ea0f00993643428221dc760d6b434b",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "! pip install langchain_community tiktoken langchain-google-genai langchainhub chromadb langchain PyMuPDF"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75a8ab66-8477-429f-bbbe-ba439322d085",
      "metadata": {
        "id": "75a8ab66-8477-429f-bbbe-ba439322d085"
      },
      "source": [
        "`(2) LangSmith`\n",
        "\n",
        "https://docs.smith.langchain.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b76f68a8-4745-4377-8057-6090b87377d1",
      "metadata": {
        "id": "b76f68a8-4745-4377-8057-6090b87377d1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "LANGCHAIN_API_KEY = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8eb312d-8a07-4df3-8462-72ac526715f7",
      "metadata": {
        "id": "f8eb312d-8a07-4df3-8462-72ac526715f7"
      },
      "source": [
        "`(3) API Keys`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e",
      "metadata": {
        "id": "df28175e-24b6-4939-8a3c-5a1f9511f51e"
      },
      "outputs": [],
      "source": [
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ['GOOGLE_API_KEY'] = GOOGLE_API_KEY\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W4wixE7iaJs9",
      "metadata": {
        "id": "W4wixE7iaJs9"
      },
      "source": [
        "# Basic Rag using Chroma DB"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c",
      "metadata": {
        "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c"
      },
      "source": [
        "## Part 1: Overview\n",
        "\n",
        "[RAG quickstart](https://python.langchain.com/docs/use_cases/question_answering/quickstart)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "kmXRagov4wfU",
      "metadata": {
        "id": "kmXRagov4wfU"
      },
      "outputs": [],
      "source": [
        "vectorstore.delete_collection()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "id": "qzwHhIFujOfA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qzwHhIFujOfA",
        "outputId": "04b4b5ee-da24-4777-a6c3-d58f3157ab1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing: https://portlandpress.com/biochemj/article/477/23/4559/227194/Deep-learning-and-generative-methods-in\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\n",
            "Error loading https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236: 403 Client Error: Forbidden for url: https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\n",
            "Failed to load document\n",
            "\n",
            "Processing: https://www.osti.gov/servlets/purl/1427646\n",
            "Error loading https://www.osti.gov/servlets/purl/1427646: 502 Server Error: Proxy Error for url: https://www.osti.gov/servlets/purl/1427646\n",
            "Failed to load document\n",
            "\n",
            "Processing: https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf\n",
            "Successfully loaded 16 documents\n",
            "\n",
            "Processing: http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf\n",
            "Successfully loaded 13 documents\n",
            "\n",
            "Processing: https://www.nature.com/articles/s41467-022-28494-3\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://link.springer.com/article/10.1007/s00521-021-05961-4\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.mdpi.com/journal/molecules/special_issues/deep_learning_structure\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\n",
            "Error loading https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\n",
            "Failed to load document\n",
            "\n",
            "Processing: https://www.mdpi.com/1420-3049/25/12/2764\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.nature.com/articles/s41598-025-95720-5\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://link.springer.com/article/10.1557/s43578-022-00628-9\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Summary:\n",
            "- Successfully loaded: 39 documents\n",
            "- Failed URLs: 3\n",
            "Failed URLs: ['https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236', 'https://www.osti.gov/servlets/purl/1427646', 'https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050']\n",
            "- Total chunks after splitting: 177\n",
            "\n",
            "Vector store created with 177 chunks\n",
            "Vector store and retriever created successfully!\n"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "import fitz  # PyMuPDF\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader, PyMuPDFLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import requests\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "from langchain.schema import Document\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "\n",
        "class DocumentProcessor:\n",
        "    def __init__(self):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=5000,\n",
        "            chunk_overlap=250\n",
        "        )\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        self.ua = UserAgent()\n",
        "        self.headers = {\n",
        "            'User-Agent': self.ua.random,\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "        }\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(self.headers)\n",
        "\n",
        "    def load_html(self, url):\n",
        "        \"\"\"Enhanced HTML loader with better error handling\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Check if content-type is PDF\n",
        "            content_type = response.headers.get('Content-Type', '')\n",
        "            if 'application/pdf' in content_type:\n",
        "                return self.load_pdf_from_url(url)\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Try to find main content areas\n",
        "            article = (soup.find('article') or\n",
        "                      soup.find('main') or\n",
        "                      soup.find(class_=re.compile('content|main|body|post')) or\n",
        "                      soup.find('div', role='main') or\n",
        "                      soup)\n",
        "\n",
        "            # Extract all text with structure\n",
        "            content = self._extract_structured_content(article)\n",
        "            if not content:\n",
        "                raise ValueError(\"No content extracted from HTML\")\n",
        "\n",
        "            return [Document(page_content=content, metadata={'source': url, 'type': 'html'})]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {url}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _extract_structured_content(self, element):\n",
        "        \"\"\"Extract content while preserving document structure\"\"\"\n",
        "        content = []\n",
        "\n",
        "        def process_element(elem):\n",
        "            if isinstance(elem, bs4.NavigableString):\n",
        "                text = elem.strip()\n",
        "                if text and len(text) > 10:\n",
        "                    content.append(text)\n",
        "                return\n",
        "\n",
        "            tag = elem.name\n",
        "            if not tag:\n",
        "                return\n",
        "\n",
        "            text = elem.get_text(' ', strip=True)\n",
        "            if not text or len(text) <= 10:\n",
        "                return\n",
        "\n",
        "            # Handle headings\n",
        "            if tag.startswith('h') and tag[1:].isdigit():\n",
        "                level = int(tag[1:])\n",
        "                content.append(f\"\\n{'#'*level} {text}\\n\")\n",
        "            # Handle list items\n",
        "            elif tag == 'li':\n",
        "                content.append(f\"- {text}\")\n",
        "            # Handle table cells\n",
        "            elif tag in ['td', 'th']:\n",
        "                content.append(f\"[TABLE CELL] {text}\")\n",
        "            # Handle regular paragraphs\n",
        "            elif tag == 'p':\n",
        "                content.append(text)\n",
        "            # Recursively process containers\n",
        "            else:\n",
        "                for child in elem.children:\n",
        "                    process_element(child)\n",
        "\n",
        "        process_element(element)\n",
        "        full_text = '\\n'.join(content)\n",
        "        full_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n",
        "        full_text = re.sub(r'[ \\t]{2,}', ' ', full_text)\n",
        "        return full_text.strip()\n",
        "\n",
        "    def load_pdf_from_url(self, url):\n",
        "        \"\"\"Improved PDF loader with retries and better cleaning\"\"\"\n",
        "        max_retries = 3\n",
        "        retry_delay = 2\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = self.session.get(url, timeout=30)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "                    tmp_file.write(response.content)\n",
        "                    tmp_path = tmp_file.name\n",
        "\n",
        "                loader = PyMuPDFLoader(tmp_path)\n",
        "                docs = loader.load()\n",
        "\n",
        "                # Clean up the extracted text\n",
        "                for doc in docs:\n",
        "                    doc.page_content = self._clean_pdf_text(doc.page_content)\n",
        "                    doc.metadata.update({\n",
        "                        'source': url,\n",
        "                        'type': 'pdf',\n",
        "                        'pages': doc.metadata.get('page', '')\n",
        "                    })\n",
        "\n",
        "                os.unlink(tmp_path)\n",
        "                return docs\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed for {url}: {str(e)}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    if 'tmp_path' in locals() and os.path.exists(tmp_path):\n",
        "                        os.unlink(tmp_path)\n",
        "                    return []\n",
        "\n",
        "    def _clean_pdf_text(self, text):\n",
        "        \"\"\"Clean and normalize PDF text\"\"\"\n",
        "        # Remove page numbers and footers\n",
        "        text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        # Remove lonely characters\n",
        "        text = re.sub(r'(?<!\\w)\\w(?!\\w)', '', text)\n",
        "        # Fix hyphenated words\n",
        "        text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
        "        return text\n",
        "\n",
        "    def process_documents(self, urls):\n",
        "        \"\"\"Process documents with better error handling\"\"\"\n",
        "        all_docs = []\n",
        "        failed_urls = []\n",
        "\n",
        "        for url in urls:\n",
        "            print(f\"\\nProcessing: {url}\")\n",
        "            try:\n",
        "                if url.lower().endswith('.pdf'):\n",
        "                    docs = self.load_pdf_from_url(url)\n",
        "                else:\n",
        "                    docs = self.load_html(url)\n",
        "\n",
        "                if docs:\n",
        "                    all_docs.extend(docs)\n",
        "                    print(f\"Successfully loaded {len(docs)} documents\")\n",
        "                else:\n",
        "                    failed_urls.append(url)\n",
        "                    print(\"Failed to load document\")\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_urls.append(url)\n",
        "                print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "        if not all_docs:\n",
        "            raise ValueError(\"No documents were successfully loaded\")\n",
        "\n",
        "        print(f\"\\nSummary:\")\n",
        "        print(f\"- Successfully loaded: {len(all_docs)} documents\")\n",
        "        print(f\"- Failed URLs: {len(failed_urls)}\")\n",
        "        if failed_urls:\n",
        "            print(\"Failed URLs:\", failed_urls)\n",
        "\n",
        "        splits = self.text_splitter.split_documents(all_docs)\n",
        "        print(f\"- Total chunks after splitting: {len(splits)}\")\n",
        "        return splits, failed_urls # Return splits and failed_urls\n",
        "\n",
        "    def create_vector_store(self, splits, persist_dir=\"chroma_db\"):\n",
        "        \"\"\"Create and persist Chroma vector store\"\"\"\n",
        "        vectorstore = Chroma.from_documents(\n",
        "            documents=splits,\n",
        "            embedding=self.embeddings,\n",
        "            persist_directory=persist_dir\n",
        "        )\n",
        "        print(f\"\\nVector store created with {vectorstore._collection.count()} chunks\")\n",
        "        return vectorstore\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Your list of documents\n",
        "    documents = [\n",
        "        \"https://portlandpress.com/biochemj/article/477/23/4559/227194/Deep-learning-and-generative-methods-in\",\n",
        "        \"https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\",\n",
        "        \"https://www.osti.gov/servlets/purl/1427646\",\n",
        "        \"https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/\",\n",
        "        \"https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf\",\n",
        "        \"http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf\",\n",
        "        \"https://www.nature.com/articles/s41467-022-28494-3\",\n",
        "        \"https://link.springer.com/article/10.1007/s00521-021-05961-4\",\n",
        "        \"https://www.mdpi.com/journal/molecules/special_issues/deep_learning_structure\",\n",
        "        \"https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\",\n",
        "        \"https://www.mdpi.com/1420-3049/25/12/2764\",\n",
        "        \"https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6\",\n",
        "        \"https://www.nature.com/articles/s41598-025-95720-5\",\n",
        "        \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/\",\n",
        "        \"https://link.springer.com/article/10.1557/s43578-022-00628-9\"\n",
        "    ]\n",
        "\n",
        "    # Initialize and process\n",
        "    processor = DocumentProcessor()\n",
        "    splits, failed_urls = processor.process_documents(documents) # Unpack the tuple here\n",
        "    vectorstore = processor.create_vector_store(splits) # Pass only splits\n",
        "\n",
        "    # Get retriever\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    print(\"Vector store and retriever created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "id": "r5vjKPHMUP1Y",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5vjKPHMUP1Y",
        "outputId": "7349361c-2966-4c18-8d5a-a62c186e8eeb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['c090b012-46b1-4bf6-96d3-f82908bc376b', 'b3acdb1e-6656-43e3-93c4-6834f8cf6cde', '1ef3f7b3-244c-4e78-8521-28a3ecb76193', 'bfb14399-e5ad-402d-a8d5-bb7b1f46efc2', 'cc693e82-78ca-4768-8540-e20ba45e901e', 'a651cf43-a168-45f9-ade6-a9fa5d1f69f9', '8f4b23de-f2fd-4bbd-89ce-3f0911c8e91b', '2609aca1-eb86-4ef7-808b-ce72cc157aa2', '5cf78115-f655-469a-a701-6e34a8f498cb', 'ebeba1ee-4b06-4f6f-9c6c-a427b7fba549', 'a8d08d8f-ee23-4485-b534-df4e7d1a69a7', 'f4f7dcc0-94d8-41d6-b8c3-d76b1766c503', '080f8b47-d1c0-46f0-bdcd-e8eff2a66178', '965dafef-c070-4cba-88ef-84520558f1f8', '9b29abaa-0f23-47df-892c-7bf10231bd3c', '31286184-c547-41f8-8fdc-a61996b3caea', '84983b0d-00b0-4979-9d34-da243a6ddc77', '08ae02df-fb3e-40f0-b5ad-4a020da56f3e', 'd9ac33d3-5bfa-4261-b645-f28b68da97e7', 'b90510b2-097b-48f2-9b69-fb206b95def4', '0fd7c7bf-0ec2-4386-a19d-a2401a7e6df9', '0fc9bb41-107c-4ddd-9a50-849702664a70', '594ef9c2-80c1-4eb8-8664-79df440194b7', 'dc8bc722-1101-449c-998e-e756ce2e7547', '5538dd8d-41ac-440c-8412-951094aa1d3c', 'f4412512-3df6-4833-a0fb-ffbb639509fe', '0d104bf8-d970-47e3-96c8-6cfeae5cbd8e', '9ccb7908-0446-44ff-b6a5-167488a32300', 'd6488bc8-e172-4a5f-8109-48fd438cc4db', '30cf1ba4-ef09-40c8-accd-89cb8b599ead', 'e6b9185b-abec-428e-9b56-c64b94818494', 'b34745e3-1ce5-489f-a4d7-1d865e147984', '4e98f00f-fe63-48f3-b28b-cafeb2a3b019', '915cf44a-4421-4897-b49b-5fad9157152c', 'b66f7bdc-b810-4daa-bfd5-b115ba9aeb04', '8277ddd4-4ef1-40f4-b441-f6a0c41121cb', '28f3666b-7b7a-4af0-8a85-17dc4bc31368', '3b65082d-27ad-4108-839c-21136a304265', '4f9fd578-7986-4f30-9b91-abadc495e57f', '09b915ce-a0af-4342-af0c-050e6f314a28', '1bd6aedf-e270-4bd2-a006-f54b0e9a6a4f', 'bfcfe195-8041-43c9-8db9-b8c6578089b1', 'cdac8c94-fee2-4499-aa02-0eadb8df86d0', '07cb3ec6-82f4-4d0f-aeff-e5285b655e3e', '956f06ea-fbe0-4922-a3fe-0db3e390d4a2', '0c67af91-991a-4f99-8b11-45506698f53c', 'ff2e649a-3a29-4a05-8d80-90d272eca6c6', '2f51f840-0d7e-4837-83c9-995689b50125', '73e0e364-061a-4ece-afc3-f3af17d1b1e8', 'a6defb0a-1648-4831-a516-66bc8b5e326a', 'dba3da17-6964-4ca4-8320-2668504ed753', '85d6b363-b2e3-4325-b757-4cb0477ab82a', 'c0ae917e-2a01-4097-a2ed-8463f90a5fb4', 'b6d28a94-12da-418b-b0de-e2befdb1d705', '8ba64995-240f-44f8-8c42-c7d7d77a99b2', '5c4cee44-bece-4bcd-8515-aa06fd8f1a83', '68545c8c-0d36-46f4-b284-2a6e2599ea6a', '0cf81de0-5194-4aba-aff8-41104551dd03', '146a7a77-4c53-4fae-a00b-a6e782010e24', 'eb3266ef-f006-4898-97e4-92cd49be17cb', '56406531-ca40-46ff-9910-53e81b368f64', 'a7925856-838e-4594-8b22-c8d98642afeb', '261ad64a-3e17-467c-a01e-d4897cab2861', 'a0204c8b-d7a8-4a77-9307-6cd03904728f', '074b6fa6-e37f-4f84-8ff4-dfbe71719912', 'b5be3232-013f-43d8-b426-da4ba62e0a90', '7d296f77-2b2b-455d-941e-6387eae8e0e9', '9f406838-5596-4399-a033-bde4b2b41920', 'b03f83aa-67b2-443e-8474-b1ffd1b0355a', '34065d78-8230-49ba-823d-c05fa8e5a4cc', '02a86120-9270-4d29-9ef5-b2daa0441956', '0bfd25e4-05d7-4be4-be39-f1f161cf3374', '463eeb90-f295-4b03-88d8-4be0de675a3b', '984a9eee-9f4c-4fbb-a578-4fb8c7b04176', 'c7ed56fb-7b99-4a5e-a14f-622696a90903', '59c24e34-f75f-4b45-83b7-a3a21ad98e7c', '89af06de-399b-4695-a67f-97e993005758', 'a06ac1ec-f3c6-4df3-92de-cd752870477e', '414fc507-e9ab-4110-ade2-13f9ce223c5b', 'c1be3eef-fc8a-46df-b98c-0d83528a9095', '242aff5e-dd43-46cf-8280-3fe4c3cfa27a', '7fab0216-5881-4095-9412-3e0e62e683d7', '90ec3f74-4880-48c9-a303-e295782d6e6b', 'ebe15f53-bdf1-4afc-be91-c1cfd56d845b', '1c29afba-9238-4bfb-b900-d14b8b66093c', '7aca5ae5-d380-4dbe-b1b4-e61d820eab8c', '159fc221-1b9f-4bc3-bb52-d570a1f26c82', '5c2013e4-45e4-4884-8055-9fe03145c1de', 'd99c96e6-0aeb-4ffe-a6c9-9ea112f64f06', '271dcf63-3a90-4303-98da-aba25852659d', '6d909f6a-62fb-41b9-9137-ed0998cc12cb', 'd115056f-96e3-4d0e-b494-1fdd0e8a1920', '2093cbae-739a-452f-959c-1358eba78fd3', '03f1fee9-e77f-46f6-8df1-ed3442d64a6d', 'd1ae57f4-e734-4115-8816-d7c06832b2cf', 'cb07cd5a-b963-426a-bfb2-f4dab248bc07', 'da055c88-cdd2-4d37-a611-b09fcc241574', '4f225a3d-1aa0-4baf-8679-29c7f52f09b3', '92ef45b0-0c54-47ff-b56a-79fc1558dfde', '6cd34200-ba57-4122-902f-86d357acb88f', 'd4700822-083b-4173-9287-455d733e7ab3', 'd9a8ac40-f8cc-4433-a799-0c11b31fbec6', '38bc272d-3aec-475f-93ee-57a8332f59e0', '692c613e-34ec-4f6a-b27c-116fd65b5697', '5b777ae8-42df-4a77-9ea3-074d0ad645f9', '7cdf34ac-5bad-458d-b660-20b0fe9c7dc3', '7ac61516-d643-47de-8bcb-5b42bd9a26e3', 'cd4a0530-9bfc-4c40-bebb-bf3dc9d04c57', 'bbed67fa-f69e-4b86-ac1f-f19f8b12a635', '97d1538b-abd6-445f-a391-447ffe0c75cd', 'adae9231-7836-4cc5-8104-006924a0b396', '7b739064-15f4-42a1-973b-4b05aba42d98', '99ef6fd4-4478-4781-871e-c0d3fc9d0f28', '7c0c80f2-737e-40b9-b19b-64ade4a96780', 'ce757587-c95a-4baf-bdca-b482038a36a4', '19fea6dd-6661-4895-a0e0-1c88d115d0f1', 'e6d1f486-9ac7-427f-8828-475f8a4fdcac', 'af39b74a-baa2-4a69-a686-de6512337042', 'f9825817-0303-4317-815e-3d8d58da3998', '38a5a81c-530e-4d45-9c9b-84472000680f', '6f7dfdae-b9a1-4030-830f-14c04e4899dd', 'f71b89d1-09d5-4135-a91b-ac3df30e3d15', '4416efc8-ec55-42f2-9fc9-5db2f50fed55', '91a301f5-60cc-4c99-9154-5dbd93161840', 'd7dafbaa-7443-427e-96a7-b73fa70fb6cd', 'ae0bb786-c082-4d7a-8ae2-b53efc84bbe3', 'a4090c39-5787-407f-8d9d-1ab3197178d2', '24a2e820-57b9-4a8e-bdb9-061af707c785', '72639933-91a2-4ba3-9306-6474710a7802', '5e88b50b-f9b2-4e52-98e1-8bb913a796f5', '19f193b5-89bf-4864-a621-12cb73d679cd', 'd6e41d74-3fbc-49e3-8e54-41352ec5d53b', '731ca099-e7f3-41be-915a-3b399e89143c', '358a9ff2-c1ab-4dfb-80e1-c17b8631ec96', 'f60484ca-b7d9-495c-ba82-5b8917686043', '57f916b5-10b2-441c-973f-0cda4a4a9658', '922b75b4-fdf7-439b-8cd0-9f60284ff379', 'f02952bd-e6d5-4d30-86c5-4372644ff56e', 'a0686fbc-8022-4ed0-8503-48745466a468', '3c32ff09-dfe4-4d78-a1af-d759ae29a02a', '9265c2fe-058a-45ee-b616-6cf6fcf0f238', '9be96a1b-d4ad-4e35-bdf7-d24e7efe338c', '555999ba-cf7f-41dc-91bf-d2f1279da197', '4f449dba-e316-4fe1-a67e-d6cf574f97b1', '32a11d15-e31f-4edf-94ac-c20a21dc5208', 'e944c885-0d70-4d82-93cd-22626fb317ce', '3b18ef17-8c4d-4d7b-ac8e-dd8264211a19', 'ae21bda4-1679-4e20-9f25-d6f723e9de3b', '1907bbd3-0464-42a5-a292-628e562ccb97', '101105ef-879c-4636-a9ea-e26e33818089', '7178a72d-89ce-4414-9a22-eef41a3ff0b6', 'bc195ba2-cc9a-4d1c-ae17-dafe54ffaa00', 'ec60d9e2-3044-4a86-be2f-94aec768b046', '1033c6f0-ea8d-4e3e-b7ca-6aa496d16a5d', '66e55d1d-392e-4aa4-ac7f-77ca6225a35d', 'e09a43fc-8e8e-4049-8559-4cc8169df008', 'ff3cc2c6-b16d-4602-9edf-902da30dd173', 'a25b04b9-e424-4cdd-8fe8-9da40bb5feaf', '34497240-3853-4d5c-b9d8-1594e648c586', 'c3fc8225-4ba9-400a-bfdf-5a7fa248d6f2', 'd12d3459-b485-42d6-b291-1d4ed77ff062', '5b9babe5-7662-4fad-ad79-491ed10ff050', '2e57ec40-56b8-4c9b-95f0-7763d83aabb8', '0fd25e5c-f91d-41f2-a39c-33f3abd0a5f8', '0323b38f-cfc1-4457-a03b-fb222ba3d991', '30daa78d-3399-4a2c-b536-68a2e79cb4c2', 'f6dccfb8-d6f7-4fb3-b858-74c962a4639d', 'c73c2830-b451-4813-b501-0562d9601bfe', 'e2d58b91-457f-495c-8bfd-24a6823fbee1', 'c2f935ab-3759-4dbe-b8c8-54767957a54c', 'c392bcde-8c4f-4921-8184-4730d0c9494a', '66e596d8-f958-4324-b5a9-04a27a0e6495', 'bc5c0043-711e-4d56-b796-c23b4be23b51', '38229351-4eab-4018-8aa5-cbefabd5c658', 'a5117bd3-fa4e-43c0-a932-c943d82ff74f', '31b064d0-a7f9-4bfb-b7ce-0feef995a009', '93b2008e-5b03-477b-b8e5-bde6b7fed5fb']\n",
            "['Search Dropdown Menu\\nheader search\\nsearch input\\nSearch input auto suggest\\nfilter your search\\nAll Content\\nAll Journals\\nBiochemical Journal\\n/#MicrositeSearch\\n/.navbar-search\\nAdvanced Search\\n/.navbar-search-container\\nMOBILE SHOPPING CART ICON\\nUser Tools Dropdown\\nDESKTOP SHOPPING CART ICON\\nDESKTOP REGISTRATION\\nDESKTOP INSTITUTIONS\\nDESKTOP SIGN IN\\n/.dropdown-panel', '# Chemception: Deep Learning from 2D Chemical Structure Images\\n\\nBy Richard L. Apodaca\\n2019-02-04T19:30:00.000Z\\nRecent advances in machine learning techniques have yielded systems that meet or even exceed human pattern-recognition capability. These powerful techniques are now starting to be used for chemical structure-property prediction. This article highlights a new approach that, in a break with past systems, works in a way that will be immediately (perhaps uncannily) recognizable to many research chemists.\\n\\n## Chemception\\n\\nChemception is a deep convolutional neural network (CNN) that accepts 80x80-pixel 2D chemical structure images as input. It can be configured to produce binary classifications (\"active\" vs \"inactive\" predictions) or regressions (numerical predictions) as output. The original Chemception paper describes training and use on datasets drawn from three areas: a panel of vitro toxicity assays (Tox21); an in vitro anti-HIV assay ; and hydration free energy (FreeSolv).\\nChemception\\'s accuracy compared favorably with, and sometimes exceeded, existing machine learning and computational methods. However, those other methods required extensive involvement from trained subject-matter experts. Chemception, on the other hand, required only the transformation of chemical line notations (SMILES strings) into appropriately-encoded grayscale images.\\nChemception demonstrates that a deep neural network trained on 2D structure images can make predictions that rival or even exceed those from systems driven by human-driven feature identification.\\n\\n## Image Encoding\\n\\nGrayscale 80x80-pixel images were created directly from SMILES strings using a pipeline of open source tools. Atoms were encoded as dots with a shade of gray proportional to atomic number. Bonds were were encoded as lines between atoms using a shade of grey proportional to the number 2. The remainder of the image was colored black (a value of zero).\\nThe authors don\\'t discuss whether or not bond order was encoded in any way. Indeed, the description seems to state that bond order was not considered at all. If true, this lack of chemical information makes the relatively high accuracy of Chemception\\'s predictions all the more impressive.\\nImages were barely processed further before entering the Chemception pipeline. Noting the sparsity of information density (~90% of pixels were set to zero in any given image), various helpful transformations used in other image classification studies were not performed. Images were randomly rotated 180 degrees, but were otherwise used as-is. A subsequent study (see below) encoded additional chemical information using a four-channel model.\\n\\n## Machine Learning vs. Machine Intelligence\\n\\nThe creators of Chemception identify two fundamentally different modes for machine-assisted chemistry research:\\n- Machine Learning Approach. Molecular features are selected and encoded through human-developed algorithms. These features are then used to train an artificial structure-property prediction system.\\n- Machine Intelligence Approach. Molecular features are selected and encoded automatically during the training of a structure-property prediction system.\\nThe development of predictive models in chemistry has been dominated by the first approach. Chemists apply their deep domain knowledge to define those molecular features deemed important for predicting a molecular behavior. These features, usually in the form of \"molecular descriptors,\" are then used to train a model. The development of molecular descriptors (of which extended connectivity/circular fingerprints are an example) has been ongoing area of research since the 1940s with well over 3,000 having been described to date.\\nBy accepting raw input in the form of 2D structure images, Chemception eliminates the need for the human subject matter expert during training. Considering the astonishing rate at which artificial pattern recognition has caught up to and exceeded human capability in areas like image recognition , the economic and scientific implications of the Machine Intelligence Approach embodied by Chemception are provocative.\\n\\n## Deep Neural Networks', '## Deep Neural Networks\\n\\nChemception is an example of an artificial neural network (ANN). Neural networks come in a variety of configurations, but all can be modeled as directed graphs in which nodes map to functions, inbound edges map to arguments, and outbound edges map to return values. A neural network is typically arranged into layers of nodes disconnected from each other but connected to nodes in adjacent layers. At a minimum, an ANN contains two layers: an \"input layer\" that directly receives test or training data; and an \"output layer\" producing a classification or regression output. Layers between input and output are known as \"hidden layers.\" By one widely-used definition , an ANN containing two or more hidden layer is considered \"deep.\" Machine learning that uses a deep neural network is called \"deep learning.\"\\nChemical structure-property machine learning studies often use a kind of an ANN configuration known as a multilayer perceptron (MLP, aka \"vanilla\" neural network). An MLP is characterized by at least one hidden layer, the nodes of which connect to all of the nodes in adjacent layers, but which do not connect to each other. Excellent introductions to the structure and function of MLPs are available in the video series embedded above and in the free book Neural Networks and Deep Learning . The image below, excerpted from the book, illustrates the organization of layers in an MLP.\\nWhen applied to chemistry, MLPs typically accept binary fingerprints as input.\\n\\n## Convolutional Neural Networks\\n\\nChemception uses a kind of augmented MLP known as a convolutional neural network (CNN). A CNN employs one or more convolutional layers immediately following the input layer. In contrast to the nodes in the hidden layer of an MLP, nodes in a convolutional layer only connect to a subset of the nodes in the preceding layer. A convolutional layer detects patterns in the input layer through the stepwise application of one or more \"filters\" during a sweeping \"convolve\" operation.\\nCNNs have proven extremely adept at image recognition and for this reason are currently the subject of intense interest. One of the most successful designs ( Inception ) was integrated into Chemception.\\n\\n## Estimating Accuracy\\n\\nTwo of the predictions made by Chemception, HIV activity and toxicity, were binary classifications (molecules were marked as either \"active\" or \"inactive\"). The accuracy of such classifications can be estimated by measuring the area under the curve (AUC) of the Receiver Operator Characteristic (ROC) graph. This graph plots the false positive rate (a number in the range 0.0-1.0) on the x-axis and the true positive rate (a number in the range 0.0-1.0) on the y-axis for varying classification thresholds. AUC ranges from 0.0 (model only predicts false positives) to 1.0 (model only predicts true positives). The video below explains AUC in detail.\\nFor example, the Chemception paper reports AUC for several configurations of the neural network tested against the Tox21 dataset (below). This study revealed one network configuration ( Chemception_T3_F16 ) that worked the best. Additionally, the comparable AUCs for Train, Validation, and Test modes suggests that little overfitting occurred.\\n\\n## Performance Benchmarks\\n\\nUsing ROC AUC, Chemception\\'s performance was compared to that of a previously-reported multi-layer perceptron deep neural network ( MoleculeNet ). In that study, extended connectivity fingerprints were inputs. The prediction accuracies for both systems were comparable, as shown in the graph below.\\nEven better results were obtained against the same MoleculeNet network using the HIV dataset (below).\\nThe Chemception paper notes two points that may account for the differences between the Tox21 and HIV studies. First, the Tox21 comparison represents a worst case for Chemception because MoleculeNet was operating in multi-task mode, giving it a theoretical advantage. (Recall that the Tox21 dataset is comprised of a panel of twelve assays.) The ideal comparison would have pitted MoleculeNet against Chemception in single-task mode, but single-task data were not reported in the MoleculeNet study. Second, the HIV dataset is larger than the Tox21 dataset. Larger datasets are expected to help Chemception given that it must synthesize all of its own chemical feature identifications.\\nChemception\\'s performance in regression against published techniques was also evaluated for the FreeSolv hydration free energy dataset. Here, two previous benchmarks were compared using root mean square (RMS) analysis: MoleculeNet and FEP (molecular dynamics simulation). The results show Chemception outperforming MoleculeNet, and approaching the performance of the molecular dynamics simulation.\\n\\n## AugChemception', '## AugChemception\\n\\nThe most surprising result from the Chemception study was how little chemical information was needed to produce competitive predictions. This raises the question of whether more information-rich 2D structure images might perform better. To address this question, the Chemception team conducted a follow-up study in which a new system, AugChemception was developed and tested.\\nUsing up to four color channels, 2D structure images were augmented with four chemical information properties:\\n- partial charge\\n- atom hybridization\\nThese properties were encoded into four images channels in analogy with the standard CMYK model . Ten schema were developed to aid in understanding the minimum chemical information requirements for effective model training:\\nFor example, \"Std\" represents the encoding used in the original Chemception paper: atomic numbers were encoded as shaded dots using a normalized grayscale. Bonds were assigned the relative grayscale 2 (a convenient choice given the lack of helium-containing structures). \"RedA\" images contained the least structure information because only atom dots were rendered, and each with the same shade of gray. \"RedB\" extends RedA with bond lines of uniform grayscale value. \"EngA\" encodes structural properties over four color channels. The last three schemes represent controls designed to detect artifacts and overtraining. Although the AugChemception paper doesn\\'t give good examples of what these images actually look like, a subsequent study, described below, does.\\nThe \"engineered\" schema EngA, EngB, EngC, and EngD yielded clear increases in accuracy for all datasets tested. For example, against the Tox21 dataset, the engineered AugChemception images performed about 4% better in terms of ROC AUC compared to the grayscale \"Std\" Chemception scheme. Similar results were obtained against the HIV dataset.\\nAn apparent split in accuracy favoring EngA/EngD over EngB/EngC schemes against the hydration free energy dataset was noted. Although all four schemes outperformed the original Std Chemception grayscale scheme, EngA/EngD performed the best of all. The authors note:\\nUpon further examination, both EngA and EngD had partial charge and hybridization information encoded, but EngB and EngC do not.\\nIndeed, it can be seen from Table 2 that EngA/EngD encode both partial charge and hybridization. EngB/EngC only encode one property or the other. The combination of visual cues for both partial charge and hybridization appears to yield a significant increase in accuracy. Unfortunately, no scheme encoding just partial charge and hybridization was included in the AugChemception study.\\nPerhaps the most interesting finding of the AugChemception study was the one involving RedA. This schema only encodes atom positions with a uniform grayscale value of 1. In other words, these images consist of nothing more than uniformly shaded points representing the relative positions of atoms laid out on a 2D coordinate system.\\nReferring back to Figure 3, which records accuracies against Tox21, RedA produces about 72% AUC. While this is significantly below the original Chemception Std schema, it is well above the 0.5 AUC for random noise. Although RedA performed much more poorly against the other two datasets, its performance in Tox21 demonstrates that images carrying no chemical information other than 2D atom layout can produce models much more accurate than noise and within the range of more chemically-rich schemas.\\nAs the authors note:\\nThese results suggest that explicit knowledge of bonds was apparently not the most important requirement for determining molecular toxicity. Given how fundamentally important the concept of chemical bonds is in chemistry, at first glance this observation is paradoxical. However, one has to recall that a bond is an artificial construct introduced to denote the linkages between various atoms, and one of its key role in chemistry research is to make it easier for chemists to formulate more sophisticated concepts starting with the notion of a bond.\\nThe best AugChemception schema was then compared with reported model building systems. The results show AugChemception to be competitive across the board.\\n\\n## Whither Fingerprints?', '## Whither Fingerprints?\\n\\nChemception offers a compelling demonstration of the power of CNNs to extract chemically-meaningful features with very little human guidance. However, it\\'s not alone. Within the last two years, at least two other deep learning systems have been reported that forgo molecular fingerprints for more direct graph representations:\\n- ConvGraph . Implements convolutional layers designed for use on graph representations directly.\\n- Continuous Representations from SMILES . Maps an arbitrary raw SMILE string to a coordinate in continuous \"latent space.\" After translation by some distance, the new point can be decoded to a new molecular representation.\\nGiven the novelty of these fingerprint-free approaches and large space for optimization, it seems likely that systems based on this new paradigm will continue to advance and may one day begin to consistently outperform fingerprint-based methods.\\n\\n## Hands-On Practice\\n\\nA recent tutorial by Esben Bjerrum reported a detailed, step-by-step procedure for creating and training an AugChemception network with open source tools.\\nIn addition to source code and package assembly instructions, Bjerrum\\'s tutorial depicts examples of AugChemception images. For example, the following image illustrates variable bond shading and atom colorings:\\nChemcepterized Image.\\nCopyright 2017 Esben Jannik Bjerrum, reprinted with permission.\\nCNNs offer the capability to peek into various layers by examining the filters or \"kernels\" being used. The following example from layer 15 gives an idea of the features being detected by Chemception:\\nCopyright 2017 Esben Jannik Bjerrum, reprinted with permission.\\nI doubt that the current accuracy of Chemception\\'s predictions would be of practical use today. Rather, Chemception provides a platform from which such systems may eventually emerge. Recent history suggests that such an emergence may be closer than it seems.\\nChemception offers a glimpse into a future in which lightly processed chemical datasets can be fed directly into off-the-shelf data learning pipelines to yield highly accurate predictive models. In this future, an iteratively hand-crafted molecular representation is no longer necessary. Instead, the system adapts itself to a much more raw form of structural data, identifying and classifying molecular features on its own and in a matter that may significantly diverge from human intuition.\\nChemception also hints at the potential for deepening insights into numerous kinds of structure-property relationships. Already it\\'s clear that some predictions will be more amenable to this streamlined approach than others. Could, for example, well-suited problems have some deeper, as yet unidentified, physical connection? Investigations along these lines may be aided by probing the convolutional layers generated during training.\\nFinally, it\\'s worth mentioning the uncanny way in which Chemception seems to operate. For many chemists, and medicinal chemists in particular, 2D structure images are the bread-and-butter for the work they do. Structure images are information-rich, readily parsed, and instantly recognizable. They are the go-to resource for just about any project focused on getting small organic molecules to do something useful. The fact that Chemception is using a nearly identical form of input to perform what is essentially the same task as many highly-trained chemists is… unsettling.\\nIt\\'s one thing for a neural network to beat the pants off of someone classifying cat pictures or playing Go. It\\'s an entirely different matter to ponder a neural network that routinely surpasses the world\\'s best medicinal chemists.', 'Molecular Structure Extraction From Documents Using Deep Learning Joshua Staker\\u200b†\\u200b*\\u200b, Kyle Marshall\\u200b†\\u200b*\\u200b, Robert Abel\\u200b‡\\u200b, Carolyn McQuaw\\u200b† †\\u200bSchrödinger, Inc., 101 SW Main Street, Portland Oregon 97204, United States ‡\\u200bSchrödinger, Inc., 120 West 45th Street, New York, New York 10036, United States * {joshua.staker,kyle.marshall}@schrodinger.com ABSTRACT Chemical structure extraction from documents remains  hard problem due to both false positive identification of structures during segmentation and errors in the predicted structures. Current approaches rely on handcrafted rules and subroutines that perform reasonably well generally, but still routinely encounter situations where recognition rates are not yet satisfactory and systematic improvement is challenging. Complications impacting performance of current approaches include the diversity in visual styles used by various software to render structures, the frequent use of ad hoc annotations, and other challenges related to image quality, including resolution and noise. We here present end-to-end deep learning solutions for both segmenting molecular structures from documents and for predicting chemical structures from these segmented images. This deep learning-based approach does not require any handcrafted features, is learned directly from data, and is robust against variations in image quality and style. Using the deep-learning approach described herein we show that it is possible to perform well on both segmentation and prediction of low resolution images containing moderately sized molecules found in journal articles and patents. Introduction For drug discovery projects to be successful, it is often crucial that newly available data are quickly processed and assimilated through high quality curation. Furthermore, an important initial step in developing  new therapeutic includes the collection, analysis, and utilization of previously published experimental data. This is particularly true for small-molecule drug discovery where collections of experimentally tested molecules are used in virtual screening programs, quantitative structure activity/property relationship (QSAR/QSPR) analyses, or validation of physics-based modeling approaches. Due to the difficulty and expense of generating large quantities of experimental data, many drug discovery projects are forced to rely on  relatively small pool of in-house experimental data, which in turn may result in data volume as  limiting factor in improving in-house QSAR/QSPR models. One promising solution to the widespread lack of appropriate training set data in drug discovery is the amount of data currently being published.\\u200b Medline reports more than 2000+ new life science papers published per day,\\u200b and this estimate does not include other literature indexes or patents that further add to the volume of newly published data. Given this high rate at which new experimental data is entering the public literature, it is increasingly important to address issues related to data extraction and curation, and to automate these processes to the greatest extent possible. One such area of data curation in life sciences that continues to be difficult and time consuming is the extraction of chemical structures from publicly available sources such as journal articles and patent filings. Most publications containing data related to small molecules do not provide the molecular structures in  computer readable format (.., SMILES, connection table, etc.). Instead, computer programs are used by authors to draw the corresponding structures, and are included in the document via an image of the resulting drawing. Publishing documents with only images of structures necessitates the manual', 'redrawing of the structures in chemical sketching software as  means of converting the structures into computer readable formats for use in downstream computation and analysis. Redrawing chemical structures can be time consuming and often requires domain knowledge to adequately resolve ambiguities, interpret variations in style, and decide how annotations should be included or ignored. Solutions for automatic structure recognition have been described previously.\\u200b- These methods utilize sophisticated rules that perform well in many situations, but can experience degradation in output quality under commonly encountered conditions, especially when input resolution is low or image quality is poor. One of the challenges to improving current extraction rates is that rule-based systems are necessarily highly interdependent and complex, making further improvements difficult. Furthermore, rule-based approaches can be demanding to build and maintain because they require significant domain expertise and require contributors to anticipate and codify rules for all potential scenarios the system might encounter. Developing hand-coded rules is particularly difficult in chemical structure extraction where  wide variety of styles and annotations are used, and input quality is not always consistent. The goal of this work is twofold: ) demonstrate it is possible to develop an extraction method to go from input document to SMILES without requiring the implementation of hand-coded rules or features; and ) further demonstrate it is possible to improve prediction accuracy on low quality images using such  system. Deep learning and other data-driven technologies are becoming increasingly widespread in life sciences, particularly in drug discovery and development.\\u200b10-13 In this work we leverage recent advances in image processing, sequence generation, and computing over latent representations of chemical structures to predict SMILES for molecular structure images. The method reported here takes an image or PDF and performs segmentation using  convolutional neural network. SMILES are then generated using  convolutional neural network in combination with  recurrent neural network (encoder-decoder) in an end-to-end fashion (meaning, the architecture computes SMILES directly from raw images). We report results based on preliminary findings using our deep learning-based method and provide suggestions for potential improvements in future iterations. Using  downsampled version of  published dataset, we show that our deep learning method performs well under low quality conditions, and may operate on raw image data without hand-coded rules or features. Related Work Automatic chemical structure extraction is not  new idea. Park et al.,\\u200b McDaniel et al.,\\u200b Sadawi et al.,\\u200b Valko & Johnson,\\u200b and Filippov & Nicklaus\\u200b each utilize various combinations of image processing techniques, optical character recognition, and hand-coded rules to identify lines and characters in  page, then assemble these components into molecular connection tables. Similarly, Frasconi et al.\\u200b utilize low level image processing techniques to identify molecular components but rely on Markov logic to assemble the components into complete structures. Park et al.\\u200b demonstrated the benefits of ensembling several recognition systems together in  single framework for improved recognition rates. Currently available methods rely on low-level image processing techniques (edge detectors, vectorization, etc.) in combination with subcomponent recognition (character and bond detection) and high-level rules that arrange recognized components into their corresponding structures. There are continuing challenges, however, that limit the usefulness of currently available methods. As discussed in Valko & Johnson\\u200b there are many situations in the literature where designing specific rules to handle inputs becomes quite challenging. Some of these include wavy bonds, lines that overlap (.., bridges), and ambiguous atom labels. Apart from complex, ambiguous, or uncommon representations, there are other challenges that currently impact performance, including low resolution or noisy images. Currently available solutions require relatively high resolution input, .., 300+ dpi,\\u200b,19 and tolerate only small amounts of noise. Furthermore, rule-based systems can be difficult to improve due to the complexity and interconnectedness of the various recognition components. Changing  heuristic in one area of the algorithm can impact and require adjustment in another area, making it difficult to improve', 'components to fit new data while simultaneously maintaining or improving generalizability of the overall system. Apart from accuracy of structure prediction, filtering of false positives during the structure extraction process also remains problematic. Current solutions rely on users to manually filter out results, choosing which predicted structures should be ignored if tables, figures, etc. are predicted to be molecules. In order to both improve extraction and prediction of structures in  wide variety of source materials, particularly with noisy or low quality input images, it is important to explore alternatives to rule-based systems. The deep learning-based method outlined herein provides () improved accuracy for poor quality images and (ii)  built-in mechanism for further improvement through the addition of new training data. Deep Learning Method The deep learning model architectures for segmentation and structure prediction described in this work are depicted in Figure . The segmentation model identifies and extracts chemical structure images from input documents, and the structure prediction model generates  computer-readable SMILES string for each extracted chemical structure image. Figure . In subpanel () we depict the segmentation model architecture and in subpanel () we depict the structure prediction model architecture. For brevity, similar layers are condensed with  multiplier indicating how many layers are chained together, .., “2X”. All convolution (conv) layers are followed by  parametric ReLU activation function. “Pred Char” represents the character predicted at  particular step and “Prev Char” stands for the predicted character at the previous step. Computation flow through the diagrams is left to right (and top to bottom in ()). Segmentation When presented with  document containing chemical structures, the initial step in the extraction pipeline is first identifying what are structures and segment these from the rest of the input. Successful segmentation is important to () provide cleanest possible input for accurate sequence prediction, and (ii) exclude objects that are not structures but contain similar features, .., charts, graphs, logos, and annotations. Segmentation herein utilizes  deep convolutional neural network to predict which pixels in', 'input images are likely to be part of  chemical structure. In designing the segmentation model we followed the “-Net” design strategy outlined in Ronneberger et al.\\u200b20 which is especially well suited for full-resolution detection at the top of the network and enables fine-grained segmentation of structures in the experiments reported here. The -Net supports full-resolution segmentation by convolving (with pooling) the input to obtain  latent representation, then upsampling the latent representation using deconvolution with skip-connections until the output is at  resolution that matches that of the input. In our experiments, the inputs to our model were preprocessed to be grayscale and downsampled to approximately 60 dpi resolution. We found 60 dpi input resolution to be sufficient for segmentation while  providing significant speed improvements versus higher resolutions. We fed the preprocessed inputs into our implementation of the -Net and the logits generated at the top of the network were scaled using  softmax activation, and provided  predicted probability between -. for each pixel, identifying the likelihood pixels belonged to  structure. The predicted pixels formed masks generated at the same resolution as the original input images and allowed for sufficiently fine-grained extraction. The masks obtained from the segmentation model were binarized to remove low confidence pixels, and contiguous areas of pixels that were not large enough to contain  structure were removed. To remove areas too small to be structures, we counted the number of pixels in  contiguous area and deemed the area  non-structure if the number of pixels was below  threshold. We also tested the removal of long, straight horizontal and vertical lines in the input image using the Hough transform.\\u200b21 Line removal improved mask quality in many cases, especially in tables where structures were very close to grid lines, and was included in the final model. Individual entities ( single, contiguous group of positively predicted pixels) in the refined masks were assumed to contain single structures and were used to crop structures from the original inputs, resulting in  collection of individual structure images. During inference we observed qualitatively better masks when generating several masks at different resolutions and averaging the masks together into  final mask used to crop out structures. Averaged masks were obtained by scaling inputs to each resolution within the range 30 to 60 dpi in increments of  dpi, then generating masks for each image using the segmentation model. The resulting masks were scaled to the same resolution (60 dpi) and averaged together. The averaged masks were then scaled to the original input resolution (usually 300 dpi) and then used to crop out individual structures. Figure  shows an example journal article page along with its predicted mask. Structure Prediction The images of individual structures obtained using the segmentation model were automatically transcribed into the corresponding SMILES sequences representing the contained structures using another deep neural network. The purpose of this network was to take an image of  single structure and, in an end-to-end fashion, predict the corresponding SMILES string of the structure contained in the image. The network comprised an encoder-decoder strategy where structure images were first encoded into  fixed-length latent space (state vector) using  convolutional neural network and then decoded into  sequence of characters using  recurrent neural network. The convolutional network consisted of alternating layers of 5x5 convolutions, 2x2 max-pooling, and  parameterized ReLU activation function,\\u200b30 with the overall network conceptually similar to the design outlined in Krizhevsky et al.\\u200b22 but without the final classification layer. To help mitigate issues in which small but important features are lost during encoding, our network architecture did not utilize any pooling method in the first few layers of the network. The state vector obtained from the convolutional encoder is passed into  decoder to generate  SMILES sequence. The decoder consisted of an input projection layer, three layers of GridLSTM cells,\\u200b23 an attention mechanism (implemented similarly to the soft attention method described in Xu et al.\\u200b24 and Bahdanau et al.,\\u200b25\\u200b and the global attention method in Luong et al.\\u200b15\\u200b), and an output projection layer. The  Resolution is expressed in dpi rather than in raw pixel lengths because the data was derived from PDFs where conversion of the PDF pages into images was  necessary step and was performed in dpi.', \"Figure . An example showing the output of the segmentation model when processing  journal article page from Salunke et al.\\u200b14 All of the text and other extraneous items are completely removed with the exception of  few faint lines that are amenable to automated post processing. state vector from the encoder was used to initialize the GridLSTM cell states and the SMILES sequence was then generated  character at  time, similar to the decoding method described in Sutskever et al.\\u200b26 (wherein sentences were generated  word at  time while translating English to French). Decoding is started by projecting  special start token into the GridLSTM (initialized by the encoder and conditioned on an initial context vector as computed by the attention mechanism), processing this input in the cell, and predicting the first character of the output sequence. Subsequent characters are produced similarly, with each prediction conditioned on the previous cell state, the current attention, and the previous output projected back into the network. The logits vector for each character produced by the network is of length , where  is the number of available characters (65 characters in this case).  softmax activation is applied to the logits to compute  probability distribution over characters, and the highest scoring character is selected for  particular step in the sequence. Sequences were generated until  special end-of-sequence token was predicted, at which point the completed SMILES string was returned. During inference we found accuracy improved when predicting images at several different (low) resolutions and returning sequences of the highest confidence, which was determined by multiplying together the softmax output of each predicted character in the sequence. The addition of an attention mechanism in the decoder helped solve several challenges. Most importantly, attention enabled the decoder to access information produced earlier in the encoder and minimized the loss of important details that may otherwise be overly compressed when encoding the state vector. Additionally, attention enabled the decoder to reference information closer to the raw input during the prediction of each character and was important considering the significance of pixelwise features in low resolution structure images. See Figure  for an example of the computed attention and how the output corresponds to various characters recognized during the decoding process. Apart from using attention for improved performance, the attention output is useful for repositioning  predicted structure into an orientation that better matches the original input image. This is done by converting the SMILES into  connection table using the open source Indigo toolkit,\\u200b27 and repositioning each atom in 2D space according to the coordinates of each character' computed attention. The repositioned structure then more closely matches the original positioning and orientation in the input image, enabling users to more easily identify and correct mistakes when comparing the output with the original source.\", 'Figure . Heatmaps are here depicted representing the computed attention during character prediction in the order left to right, top to bottom: [, @, , /, =, . The complete encoder-decoder framework is fully differentiable and was trained end-to-end using  suitable form of backpropagation, enabling SMILES to be fully generated using only raw images as input. During decoding SMILES were generated  character at  time, from left to right. Additionally, no external dictionary was used for chemical abbreviations (superatoms) rather these were learned as part of the model, thus images may contain superatoms and the SMILES are still generated  character at  time. This model operates on raw images and directly generates chemically valid SMILES with no explicit subcomponent recognition required. Datasets Segmentation Dataset To our knowledge, no dataset addressing molecular structure segmentation has been published. To provide sufficient data to train  neural network while minimizing manual effort required to curate such  dataset, we developed  pipeline for automatically generating segmentation data. To programmatically generate data, in summary, the following steps were performed: ) remove structures from journal and patent pages, ii) overlay structures onto the pages, iii) produce  ground truth mask identifying the overlaid structures, and iv) randomly crop images from the pages containing structures and the corresponding mask. In detail, OSRA\\u200b was utilized to identify bounding boxes of candidate molecules within the pages of  large number of publications, both published journal articles and patents. The regions expected to contain molecules were whited-out, thus leaving pages without molecules. OSRA was not always correct in finding structures and occasionally non-structures (.., charts) were removed suggesting that cleaner input may further improve model performance. Next, images of molecules made publically available by the United States Patent and Trademark Office (USPTO)\\u200b28 were randomly overlaid onto the pages while ensuring that no structures overlapped with any non-white pixels. Structure images were occasionally perturbed using affine transformations, changes in background shade, and/or lines added around the structure (to simulate table grid lines). We also generated the true mask for each overlaid page; these masks were zero-valued except where pixels were part of  molecule (pixels assigned  value of ). During training, samples of 128x128 pixels were randomly cropped from the', 'overlaid pages and masks, and arranged into mini-batches for feeding into the network for training; example image-mask pairs are shown in Figure . Figure . Examples sampled from the generated segmentation dataset. Inputs to the network are shown on the left with the corresponding masks used in training shown on the right. White indicates which pixels are part of  chemical structure. Molecular Image Dataset An important goal of this work was to improve recognition of low resolution or poor quality images. Utilizing training data that is of too high quality or too clean could negatively impact the generalizability of the final model. Arguably, the network should be capable of becoming invariant to image quality when trained explicitly on both high and low quality examples, at the expense of more computation and likely more data. However, we opted to handle quality implicitly by scaling all inputs down considerably. To illustrate the impact of scaling images of molecular structures, consider two structures in Figure  that are chemically identical but presented with different levels of quality. The top-left image is of fairly high quality apart from some perforation throughout the image. In the top-right image the perforation is much more pronounced with some bonds no longer continuous (small breaks due to the excessive noise). When these images are downsampled significantly using bilinear interpolation, the images appear similar and it is hard to differentiate which of the two began as  lower quality image, apart from one being darker than the other. Figure . Noisy images of the same structure from Bonanomi et al.\\u200b16 The top two images are the original images with the corresponding downsampled image below. Image quality appears similar when downsampled considerably. Chemical structures vary significantly in size, some being small fragments with just  few atoms, to very large structures, including natural products or peptide sequences. This necessitates using an image size that is not too large to be too computationally intensive to train, but large enough to fully fit reasonably sized structures, .., drug-like small molecules, into the image. Although structures themselves can be large the individual atoms and their relative connectivity may be contained within  small number of pixels', 'regardless of the size of the overall structure. Thus, scaling an image too aggressively will result in important information being lost. To train  neural network that can work with both low and high quality images, we utilized an image size of 256x256 and scaled images to fit within this size constraint (bond lengths resulting in approximately the -12 pixels range). Training  neural network over higher resolution images is an interesting research direction and may improve results, but was here left for future work. To ensure that the training data contained  variety of molecular image styles we used three separate datasets, each sampled uniformly during training. Additionally, we focused on drug-like molecules and imposed the following restrictions while preparing data and training the model: ● Structures with  SMILES length of 21-100 characters ( range that covers most drug-like small molecules) were included, all others removed. ● Attachment placeholders of the format R1, R2, R3, etc., were included but other forms of placeholder notation were not included. ● Enumeration fragments were not predicted or attached to the parent structure. ● Salts were removed and each image was assumed to contain only one structure. ● All SMILES were kekulized and canonicalized. The first utilized dataset consisted of  57 million molecule subset of molecules available in the PubChem database\\u200b29 rendered into images of 256x256 pixels of various styles (bond thicknesses, character sizes, etc.) using Indigo. PubChem structures were available in InChI format and were converted to SMILES using Indigo. To evaluate performance of the model during training the dataset was split into train/validation subsets; 90% of the dataset was used to train the model and the remaining 10% was reserved for validation. The second dataset comprised 10 million images rendered using Indigo in OS . Because Indigo rendering output can vary significantly between operating systems, we included these images to supplement training with additional image styles. The third dataset consisted of . million image/molecule pairs curated from data made publicly available by the USPTO.\\u200b28 Many of these images contain extraneous text and labels and were preprocessed to remove non-structure elements before training. For some files in the USPTO dataset, we observed that Indigo does not correctly retain stereochemistry when converting MOL format into canonical SMILES, which resulted in some SMILES not containing identical stereochemistry to that in images. Results may improve with cleaner data. Similar to the Indigo set, the dataset was split into training and validation portions; 75% of the set was used to train with 25% reserved for validation. Apart from the training and validation sets just described, two additional test sets were utilized in evaluating the performance of the method. The first is the dataset published with Valko & Johnson\\u200b (Valko dataset) consisting of 454 images of molecules cropped from literature. The Valko dataset is interesting because it contains complicated molecules with challenging features such as bridges, stereochemistry, and  variety of superatoms. The second dataset consists of  proprietary collection of image-SMILES pairs from 47 published articles and  patents. The molecules in the proprietary dataset are drug-like and some of the images contain small amounts of extraneous artifacts, .., surrounding text, compound labels, lines from enclosing table, etc. and was used to evaluate overall method effectiveness in examples extracted for use in real drug discovery projects. With the focus of this work being on low quality/resolution images, rather than predicting high resolution images, we tested our method on downsampled versions of the Valko and proprietary datasets. During the segmentation phase each Valko dataset image was downsampled to -10% of its original size, and during the sequence prediction phase, images were downsampled to 10-22.%, with similar scaling performed on the proprietary dataset. These scale ranges were chosen so that the resolution used during prediction approximately matched the (low) resolutions of the images used during training.', 'list of 65 characters containing all the unique characters in the Indigo dataset was assembled. These characters served as the list of available characters that can be selected at each SMILES decoding step. Four of these characters are special tokens that are not part of SMILES notation but were necessary for successfully implementing the model. The special tokens indicate the beginning or end of  sequence, replace unknown characters, or pad sequences that were shorter than the maximum length (during training and testing 100 characters were generated for each input and any characters generated after the end-of-sequence token were ignored). Training The segmentation model had 380,000 parameters and was trained on batches of 64 images (128x128 pixels in size). In our experiments, training converged after 650,000 steps and took  days to complete on  single GPU. The sequencing model had 46. million parameters and was trained on batches of 128 images (256x256 pixels in size). During training, images were randomly affine transformed, brightness scaled, and/or binarized. Augmenting the dataset while training using random transformations ensured that the model would not become too reliant on styles either generated by Indigo or seen in the patent images. In our experiments, training converged after  million training steps (26 days on  GPUs). Both models were constructed using TensorFlow\\u200b18 and were trained using the Adam optimizer\\u200b17 on NVIDIA Pascal GPUs. Results and Discussion During training, metrics were tracked for performance on both the Indigo and USPTO validation datasets. We observed no apparent overfitting over the Indigo dataset during training but did experience some overfitting over USPTO data (Figure ). Due to the large size of the Indigo dataset (52 million examples used during training) and the many rendering styles available in Indigo it is not surprising that the model did not experience any apparent overfitting on Indigo data. Conversely, the USPTO set is much smaller (.27 million examples used during training) with each example sampled much more frequently (approximately 40 times more often), increasing the risk of overfitting. Figure .\\u200b Training and validation curves for both the Indigo and USPTO datasets are here depicted. After the models were trained, performance was measured on the Valko and proprietary test sets. The test sets were evaluated using the full segmentation and sequence generation pipeline described above, and accuracies for the validation and test sets are reported in Table . In order for  result to contribute to accuracy, it must be chemically identical to the ground truth, including stereochemistry. Any error results in the structure being deemed incorrect. We observed that despite the method requiring low resolution inputs, accuracy was generally high across the datasets. Additionally, accuracy for the validation sets and', 'the proprietary set were all similar (77-83%) indicating that the training sets used in developing the method reasonably approximate data useful in actual drug discovery projects as represented by the proprietary test set. Table  Dataset Accuracy Indigo Validation 82% USPTO Validation 77% Valko Test Set 41% Proprietary Test Set 83% On the Valko test set we observed an accuracy of 41% over the full, downsampled dataset, which is significantly lower than the accuracies observed in the other datasets. The decrease in performance is likely due to the higher rate of certain challenging features seen less frequently in the other datasets, including superatoms. Superatoms are the single largest contributor to prediction errors in the Valko dataset (21% of samples containing one or more incorrectly predicted superatoms). In our training sets, superatoms were only included in the USPTO dataset and were not generated as part of the Indigo dataset resulting in  low rate of inclusion during training (.% of total images seen contain some superatom, with most superatoms included at  rate of <<%). An increased sampling rate of images containing superatoms will likely provide  significant accuracy improvement in this area. In further exploring incorrectly generated superatoms we discovered, unsurprisingly, that larger or more uncommon superatoms were recognized with less success than smaller, more common types. For example, “Me” (methyl) is predicted correctly about half the time (other times being mistaken for  nitrogen or oxygen) while some larger superatoms are not predicted well at all (“\\u200b+\\u200b(\\u200b\\u200b\\u200b\\u200b)\\u200b\\u200b”, “(\\u200b\\u200b)\\u200b\\u200b”, and “\\u200b+\\u200b(-Bu)\\u200b\\u200b” all predicted incorrectly). In some cases, however, large superatoms were recognized correctly, .., the single example of “-\\u200b\\u200b\\u200b17\\u200b” in the dataset is predicted correctly, and in Figure  we show an example structure with the “OTBS” (tert-butyldimethylsilyl ether) superatom predicted correctly despite aggressive downsampling and cluttering of characters. Figure . An example structure where  highly downsampled input () is used during prediction. The predicted structure () has  number of errors, likely due to this particular example being extremely low resolution, but the silyl ether is predicted correctly when compared against the ground truth (). Another interesting case in the dataset regarded the prediction of “NEt\\u200b\\u200b” (diethylamine) superatom. In Figure  three similar input images are shown, each containing the diethylamine functional group. In the results only one image had the functional group predicted correctly (the rightmost example in the figure) while the other two were incorrect, but interestingly the two incorrect examples were not incorrect in the same way. The middle example was predicted to contain an aniline while the leftmost example was 10', 'predicted to contain an azide. This was despite the functional groups appearing nearly identical and occupying the same locality in the input images. =[+]=[-] NC1C=CC=CC= CCN(CC) Figure . Three similar compounds containing the diethylamine superatom, only one of which was predicted correctly (rightmost). Below each image are the characters predicted in the SMILES for the diethylamine superatom. In analyzing stereochemistry-related errors in the Valko dataset we observed that 60% of compounds with incorrectly predicted stereochemistry had explicitly assigned stereochemistry in both the ground truth and the predicted result, but the assignments in the predicted SMILES were incorrect. In other words, the model most often correctly predicted which atoms have explicit stereochemistry assigned, but occasionally assigned the wrong configuration (.., predicted  configuration when it should have been ). Intuitively, stereochemistry assignment is not  strictly local decision, .., observing  hash or  wedge is not sufficient information to make  configuration assignment, and more information about the neighboring atoms and connectedness is required for correct assignment.  possible explanation for the difficulty in learning stereochemistry from images is that our current model architecture may be insufficient in incorporating large enough context when computing certain features. Some images failed to produce  valid structure (either output SMILES was not valid or output confidence was <%). Common issues that resulted in  structure failing or otherwise being severely incorrect included structures that were too large, macrocycles with large rings that were cleaved during prediction, structures with many superatoms or stereocenters, or images where downscaling was too aggressive and resolution too low for adequate recognition. The Valko set also contains images with multiple structures or that are inverted (white structures on black background), neither of which were supported in our validation scheme and are reported as incorrect. Across both test sets we observed low error rates due to segmentation and predicted masks appeared quite clean and were generated at reasonably high resolution (see Figure  in the method section above for an example). Only .% of the Valko dataset and .% proprietary images failed to segment properly. To further analyze performance over the validation and test sets, we explored distributions for several metrics. In Figure  we report distributions of correct and incorrect examples for molecular weight, number of heavy atoms contained in the molecules, number of characters in the ground truth SMILES, and the types of heavy atoms contained in the molecules. In exploring molecular weight and heavy atoms of both correct and incorrect molecules in the USPTO validation set we observed that the model slightly favors smaller molecules. Predicting more errors on larger molecules was not surprising considering large 11', 'Figure . Distributions of metrics for both correct (left) and incorrect (right) predictions from the USPTO validation set. Statistics for incorrect examples were taken from the reference SMILES. 12', 'molecules have longer SMILES strings and necessitates the model to compress more information during encoding and predict more characters during decoding. It was surprising, however, that the difference between correct and incorrect distributions is not more pronounced. Our expectation was that larger molecules would be significantly more challenging to predict and that the model would heavily favor smaller molecules. Incorrect SMILES tend to shift toward heavier or larger molecules, but correctness cannot be adequately attributed to either metric. Predicting well on large molecules is encouraging, and suggests that the model may be easily extended to molecules larger than 100 characters in SMILES length. In exploring the types of heavy atoms seen in both the correct and incorrect examples, once again, both distributions appeared similar. Particularly interesting are the atoms that appear much more rarely in SMILES, .., Na, Sn, . In predicting rarer atoms, the network performed surprisingly well on some (Na, Ar) but not well at all on others (, ). Further work is needed to explore the distribution of rare atoms across the full dataset and ensure that all atom types are sampled sufficiently during training. Similar to the analysis on the USPTO dataset reported above, we explored distributions of simple molecular properties for SMILES predicted correctly in the Valko dataset (Figure 10). Interestingly, the distributions all appear to be more narrow than in the USPTO dataset and the SMILES strings are longer. It is worth noting that the Valko dataset is quite small and  larger dataset containing  broader distribution of molecules would be interesting for the community to benchmark against, but is left for future work. Figure 10.\\u200b Distributions of metrics for correct predictions from the Valko et al. test set. In reviewing structures that were predicted correctly, we observe that the methods described in this work show promise in their ability to predict valid and correct SMILES for low resolution images. We showcase  few examples in Figure 11. These examples contain  variety of atom types, some examples of stereochemistry and superatoms, and are not trivial in size. Further progress may require developing methods which eliminate the restriction of downsampling all inputs by supporting high resolution data when available, and supporting structures larger than 100 characters in length. 13', 'Figure 11.\\u200b Examples of low resolution structures predicted correctly. Conclusion In this work we presented deep learning solutions to both extract structures from documents and predict SMILES for structure images. The method does not rely on handcrafted features or rules and operates directly on raw pixels enabling the method to learn from and predict images of virtually any style. Using datasets containing molecule images cropped from journal articles and patents we showed that deep learning can learn to predict images of molecules from literature at reasonably high accuracy. The method herein was trained exclusively on low resolution data, and thus only supported prediction over low resolution input. Training over high resolution images as well may greatly improve results, particularly when high resolution inputs are available. All images used in the reported results were highly downsampled, demonstrating the ability to predict low resolution images of chemical structures using an automated method, which was not previously possible. We anticipate the use of chemical structure extraction algorithms, such as those described herein as well as future generalizations and improvements, may greatly accelerate drug discovery efforts in many ways. Most immediately, such algorithms may help to greatly accelerate curation of published journal article and patent data to facilitate routine QSAR/QSPR modelling work. However, given the very high rate at which 14', 'data is being introduced into the public academic and patent literature, expeditious and efficient curation of public data may in the future become  chief bottleneck in the construction of maximally optimal global ADMET property prediction models for drug discovery. Steps toward fully automating data curation may enable drug discovery projects to more routinely utilize all relevant available data for ADMET property prediction at all moments in time in the progression of the project. Given the widespread recognition of the dependence of ADMET property prediction on data set size and cleanliness, we anticipate such technologies should broadly improve the quality of drug discovery ADMET property modeling in the future. Acknowledgements The authors would like to thank Hercules Silverstein for documenting and refactoring code, Shawn Watts and Ken Dyall for useful discussions, and the Schrödinger Machine Learning and Data Teams for useful discussions and feedback. References . Gaulton, Anna, John  Overington. 2010. “Role of open chemical data in aiding drug discovery and design.” Future Science. URL: https://www.future-science.com/doi/pdf/10.4155/fmc.10.191 . “Key MEDLINE Indicators.” \\u200b.. National Library of Medicine\\u200b, National Institutes of Health, www.nlm.nih.gov/bsd/bsd_key.html. . Park, Jungkap, Gus . Rosania, Kerby . Shedden, Mandee Nguyen, Naesung Lyu, and Kazuhiro Saitou. 2009. “Automated Extraction of Chemical Structure Information from Digital Raster Images.” \\u200bChemistry Central Journal\\u200b  (February):. . Park, Jungkap, Kazuhiro Saitou, and Gus Rosania. 2010. “Image-Based Automated Chemical Database Annotation with Ensemble of Machine-Vision Classifiers.” In \\u200b2010 IEEE International Conference on Automation Science and Engineering\\u200b. https://doi.org/\\u200b10.1109/coase.2010.5584695\\u200b. . McDaniel, Joe ., and Jason . Balmuth. 1992. “Kekule: OCR-Optical Chemical (structure) Recognition.” Journal of Chemical Information and Modeling\\u200b 32 ():373–78. . Sadawi, Noureddin ., Alan . Sexton, and Volker Sorge. 2012. “Chemical Structure Recognition:  Rule-Based Approach.” In \\u200bDocument Recognition and Retrieval XIX\\u200b, edited by Christian Viard-Gaudin and Richard Zanibbi, 8297:82970E. SPIE Proceedings. SPIE. . Valko, Aniko ., and . Peter Johnson. 2009. “CLiDE Pro: The Latest Generation of CLiDE,  Tool for Optical Chemical Structure Recognition.” \\u200bJournal of Chemical Information and Modeling\\u200b 49 ():780–87. . Filippov, Igor ., and Marc . Nicklaus. 2009. “Optical Structure Recognition Software to Recover Chemical Information: OSRA, an Open Source Solution.” \\u200bJournal of Chemical Information and Modeling\\u200b 49 ():740–43. . Frasconi, Paolo, Francesco Gabbrielli, Marco Lippi, and Simone Marinai. 2014. “Markov Logic Networks for Optical Chemical Structure Recognition.” \\u200bJournal of Chemical Information and Modeling\\u200b 54 ():2380–90. 10. Altae-Tran, Han, Bharath Ramsundar, Aneesh . Pappu, and Vijay Pande. 2017. “Low Data Drug Discovery with One-Shot Learning.” \\u200bACS Central Science\\u200b  ():283–93. 11. Ragoza, Matthew, Joshua Hochuli, Elisa Idrobo, Jocelyn Sunseri, and David Ryan Koes. 2017. “Protein-Ligand Scoring with Convolutional Neural Networks.” \\u200bJournal of Chemical Information and Modeling 57 ():942–57. 12. Gómez-Bombarelli, Rafael, Jennifer . Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy . Hirzel, Ryan . Adams, and Alán Aspuru-Guzik. 2018. “Automatic Chemical Design Using  Data-Driven Continuous Representation of Molecules.” \\u200bACS Central Science Article ASAP 13. Goh, Garrett ., Charles Siegel, Abhinav Vishnu, Nathan . Hodas “ChemNet:  Transferable and Generalizable Deep Neural Network for Small-Molecule Property Prediction” \\u200barXiv preprint arXiv:1712.02734\\u200b. 14. Salunke, Deepak ., Euna Yoo, Nikunj . Shukla, Rajalakshmi Balakrishna, Subbalakshmi . Malladi, Katelyn . Serafin, Victor . Day, Xinkun Wang, and Sunil . David. 2012. “Structure-Activity Relationships in Human Toll-like Receptor -Active ,-Diamino-furo[,-]pyridines.” \\u200bJournal of Medicinal Chemistry\\u200b 55 (18):8137–51. 15. Luong, Thang, Hieu Pham, and Christopher . Manning. 2015. “Effective Approaches to Attention-Based 15', 'Neural Machine Translation.” In \\u200bProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing\\u200b. https://doi.org/\\u200b10.18653/v1/d15-1166\\u200b. 16. Bonanomi, ., .. Estrada, .. Feng, . Fox, .. Leslie, .. Lyssikatos, . Napolitano, . Pozzan, . Sudhakar, .. Sweeney, . Tonelli, and Javier De Vicente Fidalgo. 2017. “Isoxazolidine derived inhibitors of receptor interacting protein kinase  (ripk )” \\u200bPatent WO2017096301 (A1) 17. Kingma, ., Jimmy Ba. 2014. “Adam:  method for stochastic optimization.” \\u200barXiv preprint arXiv:1412.6980\\u200b. 18. Abadi, ., . Agarwal, . Barham, . Brevdo, . Chen, . Citro, . . Corrado, . Davis, . Dean, . Devin, . Ghemawat, . Goodfellow, . Harp, . Irving, . Isard, . Jia, . Jozefowicz, . Kaiser, . Kudlur, . Levenberg, . Mane, . Monga, . Moore, . Murray, . Olah, . Schuster, . Shlens, . Steiner, . Sutskever, . Talwar, . Tucker, . Vanhoucke, . Vasudevan, . Viegas, . Vinyals, . Warden, . Wattenberg, . Wicke, . Yu, and . Zheng. 2015. “TensorFlow: Large-scale machine learning on heterogeneous systems.” \\u200bSoftware available from tensorflow.org. 19. Maeda, Miki . 2015. “Current Challenges in Development of  Database of Three-Dimensional Chemical Structures.” \\u200bFrontiers in Bioengineering and Biotechnology\\u200b  (May):66. 20. Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “-Net: Convolutional Networks for Biomedical Image Segmentation.” In \\u200bLecture Notes in Computer Science\\u200b, 234–41. 21. Galamhos, ., . Matas, and . Kittler. .. “\\u200bProgressive Probabilistic Hough Transform for Line Detection\\u200b.” In \\u200bProceedings. 1999 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No PR00149)\\u200b. https://doi.org/\\u200b10.1109/cvpr.1999.786993\\u200b. 22. Krizhevsky, ., . Sutskever, and . . Hinton. 2012. “Imagenet classification with deep convolutional neural networks.” \\u200bAdvances in Neural Information Processing Systems\\u200b. 23. Kalchbrenner, Nal, Ivo Danihelka, Alex Graves, 2015. “Grid Long Short-Term Memory” \\u200barXiv preprint arXiv:1507.01526\\u200b. 24. Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention” Proceedings of the 32nd International Conference on Machine Learning 25. Bahdanau, Dzmitry, Kyunghyun Cho, Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate” \\u200barXiv preprint arXiv:1409.0473\\u200b. 26. Sutskever, Ilya, Oriol Vinyals, and Quoc Le. 2014. “Sequence to Sequence Learning with Neural Networks” Advances in Neural Information Processing Systems. 27. Epam Systems. Indigo Toolkit. \\u200bURL: http://lifescience.opensource.epam.com/indigo/ 28. United States Patent and Trademark Office. Grant Red Book. \\u200bURL: https://bulkdata.uspto.gov/ 29. Kim, , PA Thiessen, EE Bolton, JChen,  Fu,  Gindulyte,  Han,  He,  He, BA Shoemaker,  Wang,  Yu,  Zhang, SH Bryant. 2015. “PubChem Substance and Compound databases.” \\u200bNucleic Acids Res. 2016 Jan ; 44(D1):D1202-13\\u200b. 30. He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification.” In \\u200b2015 IEEE International Conference on Computer Vision (ICCV)\\u200b. https://doi.org/\\u200b10.1109/iccv.2015.123\\u200b. 16', 'CheMixNet: Mixed DNN Architectures for Predicting Chemical Properties using Multiple Molecular Representations Arindam Paul, Dipendra Jha, Reda Al-Bahrani, Wei-keng Liao, Alok Choudhary, and Ankit Agrawal Department of Electrical Engineering and Computer Science, Northwestern University Abstract SMILES is  linear representation of chemical structures which encodes the connection table, and the stereochemistry of  molecule as  line of text with  grammar structure denoting atoms, bonds, rings and chains, and this information can be used to predict chemical properties. Molecular ﬁngerprints are representations of chemical structures, successfully used in similarity search, clustering, classiﬁcation, drug discovery, and virtual screening and are  standard and computationally efﬁcient abstract representation where structural features are represented as  bit string. Both SMILES and molecular ﬁngerprints are different representations for describing the structure of  molecule. There exist several predictive models for learning chemical properties based on either SMILES or molecular ﬁngerprints. Here, our goal is to build predictive models that can leverage both these molecular representations. In this work, we present CheMixNetset of neural networks for predicting chemical properties from  mixture of features learned from the two molecular representations - SMILES as sequences and molecular ﬁngerprints as vector inputs. We demonstrate the efﬁcacy of CheMixNet architectures by evaluating on six different datasets. The proposed CheMixNet models not only outperforms the candidate neural architectures such as contemporary fully connected networks that uses molecular ﬁngerprints and - CNN and RNN models trained SMILES sequences, but also other state-of-the-art architectures such as Chemception and Molecular Graph Convolutions.  Introduction Traditionally, chemists and materials scientists have relied on experimentally generated or simulationbased computational data to discover new materials and understand their characteristics. The slow pace of development and deployment of new/improved materials has been considered as the main bottleneck in the innovation cycles of most emerging technologies []. Data-driven techniques provide faster methods to identify important properties of chemical compounds and to predict feasibility to synthesize in chemical laboratories and thus promise to accelerate the research process of new materials development. There have been many initiatives to computationally assist molecular and materials discovery using machine learning (ML) techniques [, , , , , , , ]. Conventional machine learning approaches for predicting chemical properties have emphasized the importance of leveraging domain knowledge when designing model inputs. Current research has demonstrated that deep neural networks (DNNs) have generally outperformed traditional machine learning models. DNN models are capable of learning representations, which sets it apart from conventional ML algorithms used in chemistry. Representation learning is the process of transforming input data into  set of features that can be effectively exploited to identify patterns from the data. In the context of chemistry, the analogous process would be to use deep learning (DL) to examine chemical structures 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada.', 'and to construct features similar to engineered chemical features, with minimal assistance from an expert chemist. This approach that leverages representation learning of deep neural networks is  signiﬁcant departure from the traditional research paradigm in chemistry. In this work, we develop CheMixNetset of neural networks for predicting chemical properties by leveraging multiple molecular representations as inputs. We used simpliﬁed molecular-input line-entry system (SMILES) [10] notations as sequence inputs and molecular ﬁngerprints as vector inputs. SMILES is  line notation of chemical structures which encodes the connection table and the stereochemistry of  molecule as  line of text. Our work improves upon the existing state-ofthe-art approach of directly learning from vector representations such as molecular ﬁngerprints or chemical text representations such as SMILES by harnessing the network structure of both forms of representations. CheMixNet is  variation of multi-input-single-output (MISO) [11] architectures that learn the chemical properties from  mix of intermediate features learned from two different input representations -  vector input in the form of molecular ﬁngerprints and  sequence input in the form of SMILES strings. In our experiments, we used MACCS ﬁngerprints -  ﬁrst 2D representation of chemical structure using 167 features. Although MACCS usually perform worse than other molecular ﬁngerprints, we chose MACCS because of its simplicity and ease of interpretation. We perform signiﬁcant experimentation to determine the best neural network structure for the CheMixNet architectures. We evaluated the effectiveness of our mixed approach for building DNN architectures by training CheMixNet on six different datasetslarge dataset composed of . million samples from the Harvard Clean Energy Project (CEP) database and ﬁve other relatively smaller datasets from the MoleculeNet [12, 13] benchmark. Compared to other DL models, CheMixNet architecture outperforms fully connected MLP models trained on molecular ﬁngerprints, recurrent neural networks (RNN) and -dimensional convolutional neural network (CNN) models trained on SMILES, as well as other models - convolutional molecular graphs (ConvGraph) [14] and Chemception [15]. For instance, we achieved  mean absolute percentage error (MAPE) of .24 % on the CEP dataset; this is signiﬁcantly better than the MAPE of .43 % using CNN-RNN model. The CheMixNet architectures, as well as the benchmark models, are made accessible for the research community at https://github.com/paularindam/CheMixNet [16].  Background and Related Works In this section, we present  description of the two molecular representations we use in this work - SMILES and molecular ﬁngerprints, and discuss existing deep neural architectures for predictive modeling of chemical properties in the Quantitative structure-activity relationship (QSAR)/Quantitative structure-property relationship (QSPR) [17] modeling. . SMILES & Fingerprints Line notations are linear representations of chemical structures which encode the connection table and the stereochemistry of  molecule as  line of text [18]. SMILES [10] is the most popular speciﬁcation in the form of  line notation to describe the structure of chemical species using short ASCII strings encoding molecular structures and speciﬁc instances. One or more organic molecules attach to form long continuous chains known as branches. SMILES has  grammar structure in which alphabets denote atoms, special characters such as = and ≡bond denote the type of bonds, encapsulated numbers indicate rings, and parentheses represent side chains. In this work, we limit ourselves to character level representation and do not explicitly encode the grammar. Molecular ﬁngerprints are representations of chemical structures, successfully used in similarity search [19], clustering [20], classiﬁcations [21], drug discovery [22], and virtual screening [23],  standard and computationally efﬁcient abstract representation where structural features are represented by either bits in  bit string or counts in  count vector. Fingerprints were motivated by the need to ﬁnd materials that match target material properties. They follow the assumptions that the properties of the material is  direct function of its structure and that materials with similar structure are likely to have similar physical-chemical character. Different ﬁngerprints represent different aspects of  molecule, and thus each type of ﬁngerprint can have different suitability for mapping to particular physical property. Various machine learning (ML) algorithms have been used to predict the activity or property of chemicals using molecular descriptors and/or ﬁngerprints as input features. In our', 'experiments, we used MACCS ﬁngerprints [24, 25] -  primitive 2D representation of chemical structure using 167 features. MACCS ﬁngerprints were originally developed for the purpose of substructure screening. Unlike other hashed 1024 bit ﬁngerprint representations such as Atom Pair and Topological Torsion that are difﬁcult to comprehend, MACCS ﬁngerprints represent the counts of the presence or absence of chemical fragments, and are easily comprehensible with each key having its own deﬁnition (.. key 99 indicates if there is  = bond, key 165 indicating if there is  ring present, key 125 representing if there are more than one aromatic rings in the structure). . Related Works In their SMILES2vec [26] paper, Goh et al. developed  RNN neural network architecture trained on SMILES for predicting chemical property. SMILES2vec was inspired by language translation using RNN. Goh et al. did not explicitly encode information about the grammar of SMILES. Instead, they anticipate RNN units to learn these patterns implicitly and develop intermediate features that would be useful for predicting  variety of chemical properties. RNNs, particularly those based on LSTMs [27] or GRUs [27] are effective neural network designs for learning from text data. Their effectiveness has been demonstrated in multiple works such as the Google Neural Translation Machine that uses an architecture of + layers of residual LSTM unit [28]. In SMILES2vec, they modeled sequenceto-vector predictions, where the sequence is  SMILES string, and the vector is  measured chemical property. As SMILES is  chemical language and different from spoken language, commonly-used techniques in natural language processing (NLP) research, embeddings such as Word2vec [29] cannot be directly applied. In addition, they explored the utility of adding  1D convolutional layer between the embedding and GRU/LSTM layers. Goh et al. [15] developed “Chemception”,  deep CNN for the prediction of chemical properties, using only the images of 2D drawings of molecules. It was inspired by Google’ Inception-ResNet [30] deep CNN for image classiﬁcation. They utilized “raw data” in the form of 2D drawings of molecules that requires the minimal amount of chemical knowledge to create, and investigated the viability of augmenting and possibly eliminating human-expert feature engineering in speciﬁc computational chemistry applications. Chemception was developed based on the Inception-ResNet v2 neural network architecture that combines arguably the two most important architecture advances in CNN design since the debut of AlexNet [31] in 2012 - Inception modules and deep residual learning. During the training of Chemception, additional real-time data augmentation to the image was performed so as to bolster the limited number of data available for each task. Finally, fully connected (MLP) architectures trained on ﬁngerprint representations [32, 33, 34] are very popular in the cheminformatics community for predicting chemical properties. Although, MLP architectures trained on ﬁngerprints are one of the earliest applications of deep learning in QSAR/QSPR modeling, they have consistently outperformed traditional ML models such as random forest and logistic regression, and DNN methods such as convolutional molecular graphs.  Method . Motivation Several works [, 35] have demonstrated the effectiveness of ensemble of different kinds of neural networks for improvement in model performance over the individual candidate neural networks. Fully connected deep neural network architectures trained on ﬁngerprint representations [32, 33, 34] are very popular in the cheminformatics community for predicting chemical properties. Goh et al. [36] propounded the SMILES2vec architecture for treating SMILES strings as text sequences and trained recurrent neural network architectures. SMILES2vec and MLP architectures have been among the most successful neural network architectures in predicting chemical properties. In this paper, we harness the efﬁcacy of these architectures and mix them into one architecture which we refer as CheMixNet. SMILES and ﬁngerprints are the two most common representations of chemical molecules. By allowing  neural network to learn from both these representations, we could increase the generalizability and the scope of the architecture. Sequence classiﬁcation on shorter texts is generally harder than on longer texts and usually has worse performance than longer texts. As SMILES2vec essentially treats the SMILES as  text with character level embedding, the performance of SMILES2vec degrades on shorter strings. Also, the performance of MLP models trained on molecular ﬁngerprints generally varies based on the size of the molecule (performance varies based on small versus large organic molecules). CheMixNet provides  model architecture that can leverage the best of both forms of representation learned from the two inputs using appropriate', 'RNN RNN - CNN Softmax/Linear - CNN - layers - layers - Layers (LSTM or GRU cells) Fully Connected (FC) Layers - layers - Layers (LSTM or GRU cells) Different Architectures for Sequence Modeling Option  Option  Option  Concatenation Layer Fully Connected (FC) Layers - layers 000100000000000000001000001000000000000000 001000001000000000001001000000000010000001 000000000000101101000100000000000000110001 00000000000100000011000000000010000111010 C1C=CC=C1c1cc2[se]c3c(ncc4 ccccc34)c2c2=[SiH2]=c12 SMILES Fingerprint Figure : The proposed CheMixNet architecture for learning from the two molecular representations. The blue branch represents candidate neural networks for learning from SMILES sequences. Option  uses only LSTMs/GRUs for modeling the SMILES sequences, option  uses only - CNNs for sequence modeling, and option  uses - CNNs followed by LSTMs/GRUs. The fully connected (FC) branch of the model with molecular ﬁngerprint inputs is illustrated in red. The orange part represents the fully connected layers that learn the ﬁnal output from the mix of intermediate features learned by the two network branches. We exemplify the molecular ﬁngerprint and SMILES with one representative example in this illustration. neural network components for them. This provides the network with the ability to automatically assess the degree to which each representation can be leveraged for learning the given chemical property. . Design Figure  illustrates our design approach for building CheMixNet models. We present three different architectures where we mix the output features learned using different types of models to learn the chemical properties from the two molecular representations as inputsthe ﬁngerprints and the SMILES formula; hence, referred as CheMixNet. They are basically composed of two neural network branches -  sequence modeling branch that learns from the SMILES sequences using - CNN and/or RNN, and  fully connected (FC) branch that learns from the MACCS ﬁngerprint representation. The two input representations are shown in the top of Figure . Since the SMILES representations are composed of  sequence of characters, the ﬁrst network branch for learning from SMILES are composed of one of the following: ) RNN, ) - CNN, and ) - CNN followed by RNN. Since the ﬁngerprints are composed of bit or count vectors, the second branch for learning from them are composed of multiple layers of fully connected layers. The individual models that leverage  single input representation are composed of one of these components along with some fully connected layers at the end. To build the mixed DNN architectures, we combine the intermediate features learned using the two branches and applied some fully-connected layers for learning the ﬁnal regression or classiﬁcation output. Depending on the learning task, the output layer uses sigmoid activation for the binary classiﬁcation tasks or linear activation for the regression tasks. Since we have three candidate neural networks for sequence modeling using SMILES, ChemixNet contains three neural network architectures which we refer as CNN-FC, RNN*FC, and CNN-RNN*FC where the ‘-’ represents networks stacked in sequence and ‘*’ represents the combination of intermediate features learned using two parallel network branches.', 'Table : Description of all the  datasets used to evaluate the performance of CheMixNet architectures. The original HIV dataset had 41,193 compounds but reduced to ,886 after under-sampling. Dataset Property Task Size CEP Highest Occupied Molecular Orbital Energy Regression ,322,849 HIV* Activity Classiﬁcation ,886 Tox21 Toxicity Classiﬁcation ,981 FreeSolv-Exp (Experimental) Solvation Energy Regression 643 FreeSolv-Comp (Computed) Solvation Energy Regression 643 ESOL Solubility Regression ,128  Data . Description of the Datasets We demonstrate the effectiveness of our approach for designing DNN architectures for learning chemical properties from SMILES and ﬁngerprints using six different datasets as shown in Table . First, Harvard CEP Dataset [33, 37] contains molecular structures and properties for . million candidate donor structures for organic photovoltaic cells. Organic Photovoltaic cells (OPVs) [38, 39, 40, 41] are lightweight, ﬂexible, inexpensive and more customizable compared to traditional silicon-based solar cells [42]. For  solar cell, the most important property is power conversion efﬁciency or the amount of electricity which can be generated due to the interaction of electron donors and acceptors, which are dependent on the HOMO values of the donor molecules. In this work, we considered highest occupied molecular orbitals (HOMO) as the target property as it determines the power conversion efﬁciency of  solar cell according to the Scharber model [43]. Next, we used the Tox21, HIV, ESOL, and FreeSolv (Experimental and Computed) datasets from the MoleculeNet [12, 13] benchmark repository; they involve two classiﬁcation and two regression tasks. The Tox21 dataset is an NIHfunded public database of toxicity measurements comprising of 8981 compounds on 12 different measurements ranging from stress response pathways to nuclear receptors. This dataset provides  binary classiﬁcation problem of labeling molecules as either “toxic” or “non-toxic”. The FreeSolv dataset is comprised of 643 compounds that have computed and experimental hydration free energies of small-molecules ranging from –25. to . kcal/mol; we refer to the dataset containing experimental values as FreeSolv-Exp and the one with computed property values as FreeSolv-Comp. Hydration free energy is  physical property of the molecule which can be computed from ﬁrst principles. ESOL is  dataset containing 1128 compounds with water solubility (log solubility in mols per litre) for common organic small molecules. Lastly, we evaluated the performance of CheMixNet on the HIV dataset obtained from the Drug Therapeutics Program AIDS Antiviral Screen, which measured the ability of 41,913 compounds to inhibit HIV replication in vitro. Using the curation methodology adopted by MoleculeNet, this dataset was reduced to  binary classiﬁcation problem of “active” and “inactive” compounds. The original HIV dataset was very imbalanced with the minority class comprising less than  % of the dataset. In our work, we decided to balance the minority classes by under-sampling the majority class by randomly selecting ,443 (size of samples from the minority class) samples. Although this led to  signiﬁcant reduction in the size of the dataset, it also allowed us to investigate the viability of CheMixNet architecture on smaller datasets without signiﬁcant network topology changes. . Dataset Preparation For the SMILES sequence, we used -hot encoding to convert the SMILES into  ﬁxed length representation. The length of the sequence was determined by the length of the longest SMILES sequence in each dataset. We applied zero padding for shorter strings so that we had  uniform sequence of size equal to the maximum length for each dataset. The vocabulary size was determined by ﬁnding the number of unique characters in each dataset. Table  describes the size of vocabulary and the maximum input length for all datasets. The datasets were randomly split in the ratio of : into training and test sets. Further, the training set was split into : ratio for training and validation.', 'Table : Vocab size and Maximum Input length for the datasets Dataset Size of Vocabulary Maximum Input Sequence Length CEP 23 83 HIV 54 400 Tox21 42 940 FreeSolv-Exp 32 83 FreeSolv-Comp 32 83 ESOL 33 98  Experiments & Results In this section, we present the experimental settings and results of the CheMixNet architectures and the comparison with other contemporary DL models on the . million CEP dataset as well as the ﬁve datasets from the MoleculeNet benchmark. . Experimental Settings The DNN models were implemented using Python and Keras [44] with TensorFlow [45] as the backend. They are trained using Adam as the optimization algorithm with  mini-batch size of 32. For generating the MACCS ﬁngerprints, we used RDKIT [46] library. Scikit-Learn [47] was used for data preprocessing and for evaluating the test set errors. All experiments are carried out using NVIDIA DIGITS DevBox with  Core i7-5930K  Core .5GHz desktop processor, 64GB DDR4 RAM, and  TITAN  GPUs with 12GB of memory per GPU. For our experiments, we used  learning rate of .001. We used the mean squared error (MSE) as the loss function for the regression tasks and used the mean absolute % error (MAPE) as the performance metric. For classiﬁcation tasks, we used the binary cross-entropy as the loss function and used the area under the ROC curve (AUC) as  performance metric. Early stopping was used during training to avoid over-ﬁtting. For the benchmark results for graph convolution networks, we used the DeepChem [48] library. For benchmarking with Chemception, there is no ofﬁcial public library, so we implemented the network using Keras which is also available in the CheMixNet repository [16]. We used the libraries hyperas [49] and hyperopt [50] to perform Bayesian hyperparameter search [51] to obtain the best choice of layer depth (for MLPs, - convolutional and LSTM/GRU units), number of recurrent units for LSTMs/GRUs and learning rate. Further, the Bayesian hyperparameter search was performed only for the CEP database. Once we determined the best hyperparameters for CEP, we did not change any hyperparameter except the batch size. For the CEP dataset, we used  batch size of 64; for the two classiﬁcation datasets (HIV and Tox21), we used  batch size of 32; for the ESOL and FreeSolv, we used  batch size of 16. . Results We evaluated the effectiveness of our mixing approach for building DNN architectures to learn from both molecular representations by training the CheMixNet using six different datasets. To compare their performance, we also trained other state-of-the-art architectures for all datasets used. This includes the fully connected (FC) networks trained on the MACCS ﬁngerprints, the two broad classes of SMILES2vec architectures - RNN and CNN-RNN, novel experimentation on SMILES using - convolutions, ConvGraph and Chemception model. For the RNN and CNN-RNN architectures, we experimented using both LSTM and GRU units. As previous works [52, 53] have demonstrated the efﬁcacy of - CNN to perform effectively in text prediction without any recurrent component, we compared against - CNN trained on SMILES sequences. Lastly, we compare against ConvGraph architecture that uses the molecular structure encoded as graphs as input, and Chemception architecture that uses chemical images as input. .. Performance on the CEP Dataset Figure  demonstrates the performance results of different DNN models on the CEP dataset. For the presented results, we used the MACCS ﬁngerprints; similar metrics were observed using other types of ﬁngerprints. The existing models trained on the SMILES generally perform better than the models', 'Epoch . . . . . . . . Mean Abs Percent Error (%) RNN*FC CNN*FC CNN-RNN*FC FC CNN RNN CNN-RNN () Comparison of training curves . . . . . . % Mean Absolute Percentage Error CNN-RNN*FC RNN*FC CNN*FC Chemception ConvGraph CNN-RNN RNN CNN FC CEP () MAPE comparison Figure : Comparison of the training error curves and mean absolute percentage error on the test set for different DNN architectures on the CEP dataset. The ’-’ sign indicates when the networks are trained in sequence and ’*’ when two parallel multi-input networks (one with SMILES as input and the other with ﬁngerprints as input) are concatenated. In these experiments, we use MACCS ﬁngerprints - however, metrics from other ﬁngerprints were similar. Our results demonstrate that the CNN*FC model performs the best. The three mixed networks perform comparatively better than the other state-of-the-art models. Since we use ConvGraph module from deepchem repository out of the box which does not give any information about convergence while Chemception usually takes about 100 epochs to converge, the training curve for Chemception and ConvGraph is not shown. trained using only ﬁngerprints; CNN-RNN and RNN perform signiﬁcantly better than the FC model. We conjecture the difference in performance results from the difference in feature representation using SMILES and ﬁngerprints. Since ﬁngerprints are generated from SMILES, ﬁngerprints are supposed to contain less information. We experimented using both LSTM and GRU for building the models composed of RNNs; for the CNN-RNN*FC, LSTM performs better than GRU while GRU performs best for CNN-RNN. Our results illustrate that the three mixed networks perform comparatively better than the existing candidate model architectures; the CNN*FC model performing signiﬁcantly better. The CNN branch of CNN*FC model is composed of an embedding layer of length 32 followed by two - convolutional layers with 32 ﬁlters with  kernel size of  (same padding and ReLU as the activation function). The FC branch is composed of four fully connected layers with 1024, 512, 256 and 64 units respectively. The ﬁnal network that learns on the mixed intermediate features is composed of two layers with 64 and  outputs respectively. Since we perform an architecture search for each network independent of other networks, the architecture conﬁguration for the mixed networks are different from the individual networks that leverage one input representation; hence, this is different from the current model ensemble approach where the outputs from different trained networks are aggregated to predict the output. The derivation of ﬁngerprints from SMILES involves simple logic and computation. However, we ﬁnd that the mixing of intermediate features learned using the two network branches from the two molecular representations trained resulted in  signiﬁcant gain in performance. It demonstrates the effectiveness of CheMixNet architectures in learning from multiple types of feature representations for better performance. .. Performance on MoleculeNet Datasets We further analyzed the effectiveness of using mixed networks in learning from multiple inputs by evaluating on ﬁve datasets from the MoleculeNet benchmark. Two of these datasets involves classiﬁcation tasks while the rest involves regression tasks. Figure  illustrates the performance results of different types of model architectures on these datasets. For the two classiﬁcation tasks, we observe that CNN*FC performs better than all other models. The other two mixed models CNN-RNN*FC and RNN*FC perform better than the existing models except for the FC model. FC model performs better than all other existing models on the classiﬁcation tasks from the HIV and Tox21 datasets. For the regression problems, we observe similar patternsone of the mixed networks performing the best among all the networks. For the two FreeSolv datasets, CNN-RNN*FC performs the best; there is no one single best model among the existing network that works best for both these datasets. For', 'the ESOL dataset, RNN*FC performs the best among all models; CNN model performing slightly worse. In general, we always observe beneﬁt in performance from using mixed networks which can learn from both inputsSMILES, and ﬁngerprints. Since ﬁngerprints are derived from SMILES, we conjecture the gain in performance not only comes from multiple inputs but also and more importantly from the use of different types of networks for different input representations. .75 .80 .85 .90 .95 .00 AUC CNN-RNN*FC RNN*FC CNN*FC Chemception ConvGraph CNN-RNN RNN CNN FC HIV () HIV ROC AUC comparison .75 .80 .85 .90 .95 .00 AUC CNN-RNN*FC RNN*FC CNN*FC Chemception ConvGraph CNN-RNN RNN CNN FC Tox21 () Tox21 ROC AUC comparison       10 % Mean Absolute Percentage Error CNN-RNN*FC RNN*FC CNN*FC Chemception ConvGraph CNN-RNN RNN CNN FC Freesolv-Calc () FreeSolv-Comp MAPE comparison       10 % Mean Absolute Percentage Error CNN-RNN*FC RNN*FC CNN*FC Chemception ConvGraph CNN-RNN RNN CNN FC FreeSolv-Exp () FreeSolv-Exp MAPE comparison       10 % Mean Absolute Percentage Error CNN-RNN*FC RNN*FC CNN*FC Chemception ConvGraph CNN-RNN RNN CNN FC ESOL () ESOL MAPE comparison Figure : Performance of CheMixNet models against contemporary DNN models for the datasets from MoleculeNet benchmark. Tox21 and HIV datasets involve classiﬁcation tasks while FreeSolvExp, FreeSolv-Comp and ESOL involve regression tasks; higher AUC is better for classiﬁcation tasks while lower MAPE is better for regression tasks. CheMixNet outperforms the existing state-of-the-art models on all datasets. In general, the mixed models are better than the existing models.  Conclusions and Future Work In this paper, we developed CheMixNet, the ﬁrst mixed deep neural network that leverages both chemical text (SMILES) as well as molecular descriptors (MACCS ﬁngerprints) for predicting chemical properties. Compared to existing DL models trained on single molecular representations, the proposed CheMixNet architectures perform signiﬁcantly better on all the six datasets used in our study. The results provide proof of concept of the efﬁcacy of using mixed input architectures', 'for chemical property prediction. We demonstrate that by using  mixed deep learning approach, we can leverage the features of both sequence and ﬁngerprint representations and achieve much better results, even with only  few hundred training samples. Further, the results demonstrate that CheMixNet architectures can be generalized over  different range of chemical properties independent of the type of supervised learning tasks (classiﬁcation or regression) and the type and size of datasets. The range of chemical properties predicted in our study is relevant across solar cell technology, pharmaceuticals, biotechnology, and consumer products. The CheMixNet architectures, as well as the benchmark models, are made accessible for the research community at https://github.com/paularindam/CheMixNet [16]. Although we limit the scope of our work in developing the CheMixNet architectures, we plan to extend this current work by trying to develop  method for interpreting the proposed mixed-input architecture as part of future work. For future CheMixNet architectures, we plan to explore ConvGraph and Chemception architectures as candidate input neural networks for the mixed neural network. Further, as chemical signiﬁcance is present in both the character following as well as preceding  given character in  SMILES string, we believe bidirectional RNNs can perform better than vanilla onedirectional RNNs. Lastly, we believe that Hierarchical Attention Networks (HANs) [54] that combine character level and word level sequences for text prediction could present superior performance to the aforementioned architectures. Acknowledgments This work was performed under the following ﬁnancial assistance award 70NANB14H012 from .. Department of Commerce, National Institute of Standards and Technology as part of the Center for Hierarchical Materials Design (CHiMaD). Partial support is also acknowledged from the following grants: NSF award CCF-1409601; DOE awards DE-SC0007456, DE-SC0014330; AFOSR award FA9550-12--0458.', 'References [] Surya  Kalidindi. Data science and cyberinfrastructure: critical enablers for accelerated development of hierarchical materials. International Materials Reviews, 60():150–168, 2015. [] Victor  Kuz’min, Pavel  Polishchuk, Anatoly  Artemenko, and Sergey  Andronati. Interpretation of qsar models based on random forest methods. Molecular informatics, 30(- ):593–603, 2011. [] Ankit Agrawal and Alok Choudhary. Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science. APL Materials, ():053208, 2016. [] XJ Yao, Annick Panaye, Jean-Pierre Doucet, RS Zhang, HF Chen, MC Liu, ZD Hu, and Bo Tao Fan. Comparative study of qsar/qspr correlations using support vector machines, radial basis function neural networks, and multiple linear regression. Journal of chemical information and computer sciences, 44():1257–1266, 2004. [] Adam  Gagorik, Brett Savoie, Nick Jackson, Ankit Agrawal, Alok Choudhary, Mark  Ratner, George  Schatz, and Kevin  Kohlstedt. Improved scaling of molecular network calculations: the emergence of molecular domains. The journal of physical chemistry letters, ():415–421, 2017. [] Bryce Meredig, Ankit Agrawal, Scott Kirklin, James  Saal, JW Doak,  Thompson, Kunpeng Zhang, Alok Choudhary, and Christopher Wolverton. Combinatorial screening for new materials in unconstrained composition space with machine learning. Physical Review , 89():094104, 2014. [] Logan Ward, Ankit Agrawal, Alok Choudhary, and Christopher Wolverton.  general-purpose machine learning framework for predicting properties of inorganic materials. arXiv preprint arXiv:1606.09551, 2016. [] Ankit Agrawal, Parijat  Deshpande, Ahmet Cecen, Gautham  Basavarsu, Alok  Choudhary, and Surya  Kalidindi. Exploration of data science techniques to predict fatigue strength of steel from composition and processing parameters. Integrating Materials and Manufacturing Innovation, ():–19, 2014. [] Mojtaba Mozaffar, Arindam Paul, Reda Al-Bahrani, Sarah Wolff, Alok Choudhary, Ankit Agrawal, Kornel Ehmann, and Jian Cao. Data-driven prediction of the high-dimensional thermal history in directed energy deposition processes via recurrent neural networks. Manufacturing Letters, 18:35 – 39, 2018. [10] Daylight Toolkit. Daylight chemical information systems. Inc.: Aliso Viejo, CA, 1997. [11]  Abdullah, Zainal Ahmad, and  Aziz. Multiple input-single output (miso) feedforward artiﬁcial neural network (fann) models for pilot plant binary distillation column. In Bio-Inspired Computing: Theories and Applications (BIC-TA), 2011 Sixth International Conference on, pages 157–160. IEEE, 2011. [12] Zhenqin Wu, Bharath Ramsundar, Evan  Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh  Pappu, Karl Leswing, and Vijay Pande. Moleculenet:  benchmark for molecular machine learning. Chemical science, ():513–530, 2018. [13] Glambard/molecules_dataset_collection: Collection of data sets of molecules for  validation of properties inference. https://github.com/GLambard/Molecules_Dataset_ Collection. (Accessed on 10/15/2018). [14] David  Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Alán Aspuru-Guzik, and Ryan  Adams. Convolutional networks on graphs for learning molecular ﬁngerprints. In Advances in neural information processing systems, pages 2224– 2232, 2015. 10', '[15] Garrett  Goh, Charles Siegel, Abhinav Vishnu, Nathan  Hodas, and Nathan Baker. Chemception:  deep neural network with minimal chemistry knowledge matches the performance of expert-developed qsar/qspr models. arXiv preprint arXiv:1706.06689, 2017. [16] Ofﬁcial repository for the chemixnet library. https://github.com/paularindam/ CheMixNet. (Accessed on 11/01/2018). [17] Mati Karelson, Victor  Lobanov, and Alan  Katritzky. Quantum-chemical descriptors in qsar/qspr studies. Chemical reviews, 96():1027–1044, 1996. [18] Wendy  Warr. Representation of chemical structures. Wiley Interdisciplinary Reviews: Computational Molecular Science, ():557–579, 2011. [19] Mark  Johnson and Gerald  Maggiora. Concepts and applications of molecular similarity. Wiley, 1990. [20] Malcolm  McGregor and Peter  Pallai. Clustering of large databases of compounds: Using the mdl “keys” as structural descriptors. Journal of chemical information and computer sciences, 37():443–448, 1997. [21] Leo Breiman, Jerome Friedman, Charles  Stone, and Richard  Olshen. Classiﬁcation and regression trees. CRC press, 1984. [22] Márton Vass, Albert  Kooistra, Tina Ritschel, Rob Leurs, Iwan JP de Esch, and Chris de Graaf. Molecular interaction ﬁngerprint approaches for gpcr drug discovery. Current Opinion in Pharmacology, 30:59–68, 2016. [23] Peter Willett. Similarity-based virtual screening using 2d ﬁngerprints. Drug discovery today, 11(23):1046–1053, 2006. [24] Joseph  Durant, Burton  Leland, Douglas  Henry, and James  Nourse. Reoptimization of mdl keys for use in drug discovery. Journal of chemical information and computer sciences, 42():1273–1280, 2002. [25] MACCS Structural Keys. Mdl information systems inc. San Leandro, CA, 2005. [26] Garrett  Goh, Nathan  Hodas, Charles Siegel, and Abhinav Vishnu. Smiles2vec: An interpretable general-purpose deep neural network for predicting chemical properties. arXiv preprint arXiv:1712.02034, 2017. [27] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, ():1735–1780, 1997. [28] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc  Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’ neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144, 2016. [29] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. [30] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander  Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In AAAI, volume , page 12, 2017. [31] Alex Krizhevsky,  Sutskever, and  Hinton. Imagenet classiﬁcation with deep convolutional neural. In Neural Information Processing Systems, pages –, 2014. [32] Junshui Ma, Robert  Sheridan, Andy Liaw, George  Dahl, and Vladimir Svetnik. Deep neural nets as  method for quantitative structure–activity relationships. Journal of chemical information and modeling, 55():263–274, 2015. [33] Edward  Pyzer-Knapp, Kewei Li, and Alan Aspuru-Guzik. Learning from the harvard clean energy project: The use of neural networks to accelerate materials discovery. Advanced Functional Materials, 25(41):6495–6502, 2015. 11', '[34] Bharath Ramsundar, Steven Kearnes, Patrick Riley, Dale Webster, David Konerding, and Vijay Pande. Massively multitask networks for drug discovery. arXiv preprint arXiv:1502.02072, 2015. [35] Guangyuan Kan, Cheng Yao, Qiaoling Li, Zhijia Li, Zhongbo Yu, Zhiyu Liu, Liuqian Ding, Xiaoyan He, and Ke Liang. Improving event-based rainfall-runoff simulation using an ensemble artiﬁcial neural network based hybrid data-driven model. Stochastic environmental research and risk assessment, 29():1345–1370, 2015. [36] Garrett  Goh, Nathan Hodas, Charles Siegel, and Abhinav Vishnu. Smiles2vec: Predicting chemical properties from text representations. 2018. [37] Cepdata.csv.zip. https://www.dropbox.com//3kqzt9u1ryflls0/CEPData.csv.zip? dl=. (Accessed on 10/15/2018). [38] Wichien Sang-aroon, Seksan Laopha, Phrompak Chaiamornnugool, Sarawut Tontapha, Samarn Saekow, and Vittaya Amornkitbamrung. Dft and tddft study on the electronic structure and photoelectrochemical properties of dyes derived from cochineal and lac insects as photosensitizer for dye-sensitized solar cells. Journal of molecular modeling, 19():1407–1415, 2013. [39] Zhong Hu, Vedbar  Khadka, Wei Wang, David  Galipeau, and Xingzhong Yan. Theoretical study of two-photon absorption properties and up-conversion efﬁciency of new symmetric organic -conjugated molecules for photovoltaic devices. Journal of molecular modeling, 18():3657–3667, 2012. [40] Mazmira Mohamad, Rashid Ahmed, Amirudin Shaari, and Souraya Goumri-Said. First principles investigations of vinazene molecule and molecular crystal:  prospective candidate for organic photovoltaic applications. Journal of molecular modeling, 21():27, 2015. [41] Natalia Inostroza, Fernando Mendizabal, Ramiro Arratia-Pérez, Carlos Orellana, and Cristian Linares-Flores. Improvement of photovoltaic performance by substituent effect of donor and acceptor structure of tpa-based dye-sensitized solar cells. Journal of molecular modeling, 22():25, 2016. [42] Claudia  Hoth, Roland Steim, Pavel Schilinsky, Stelios  Choulis, Sandro  Tedde, Oliver Hayden, and Christoph  Brabec. Topographical and morphological aspects of spray coated organic photovoltaics. Organic Electronics, 10():587–593, 2009. [43] Markus  Scharber, David Mühlbacher, Markus Koppe, Patrick Denk, Christoph Waldauf, Alan  Heeger, and Christoph  Brabec. Design rules for donors in bulk-heterojunction solar cells—towards 10% energy-conversion efﬁciency. Advanced materials, 18():789–794, 2006. [44] François Chollet et al. Keras, 2015. [45] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow:  system for large-scale machine learning. In OSDI, volume 16, pages 265–283, 2016. [46] Greg Landrum. Rdkit: Open-source cheminformatics. (04):2012, 2006. [47] Fabian Pedregosa, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikitlearn: Machine learning in python. Journal of machine learning research, 12(Oct):2825–2830, 2011. [48] deepchem/deepchem: Democratizing deep-learning for drug discovery, quantum chemistry, materials science and biology. https://github.com/deepchem/deepchem. [49] maxpumperla/hyperas: Keras + hyperopt:  very simple wrapper for convenient hyperparameter optimization. https://github.com/maxpumperla/hyperas. (Accessed on 10/16/2018). [50] hyperopt/hyperopt: Distributed asynchronous hyperparameter optimization in python. https: //github.com/hyperopt/hyperopt. (Accessed on 10/16/2018). 12', '[51] Jasper Snoek, Hugo Larochelle, and Ryan  Adams. Practical bayesian optimization of machine learning algorithms. In Advances in neural information processing systems, pages 2951–2959, 2012. [52] Xiang Zhang, Junbo Zhao, and Yann LeCun. Character-level convolutional networks for text classiﬁcation. In Advances in neural information processing systems, pages 649–657, 2015. [53] Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. Bag of tricks for efﬁcient text classiﬁcation. arXiv preprint arXiv:1607.01759, 2016. [54] Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. Hierarchical attention networks for document classiﬁcation. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1480–1489, 2016. 13', 'Download PDF\\n- Open access\\n- Published: 14 February 2022\\n\\n# A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals\\n\\n- Zheni Zeng ORCID: orcid.org/0000-0001-6901-5292 1 na1 ,\\n- Yuan Yao 1 na1 ,\\n- Zhiyuan Liu ORCID: orcid.org/0000-0002-7709-2543 1 &\\n- Maosong Sun ORCID: orcid.org/0000-0002-6011-6115 1\\nShow authors\\nNature Communications volume 13 , Article\\xa0number: 862 ( 2022 ) Cite this article\\n- 25k Accesses\\n- 70 Citations\\n- 11 Altmetric\\n- Metrics details\\n- Drug discovery\\n- Machine learning\\nTo accelerate biomedical research process, deep-learning systems are developed to automatically acquire knowledge about molecule entities by reading large-scale biomedical data. Inspired by humans that learn deep molecule knowledge from versatile reading on both molecule structure and biomedical text information, we propose a knowledgeable machine reading system that bridges both types of information in a unified deep-learning framework for comprehensive biomedical research assistance. We solve the problem that existing machine reading models can only process different types of data separately, and thus achieve a comprehensive and thorough understanding of molecule entities. By grasping meta-knowledge in an\\xa0unsupervised\\xa0fashion within and across different information sources, our system can facilitate various real-world biomedical applications, including molecular property prediction, biomedical relation extraction and so on. Experimental results show that our system even surpasses human professionals in the capability of molecular property comprehension, and also reveal its promising potential in facilitating automatic drug discovery and documentation in the future.\\n\\n### Similar content being viewed by others\\n\\n### Coverage bias in small molecule machine learning\\n\\nOpen access\\n09 January 2025\\n\\n### Differentiable biology: using deep learning for biophysics-based and data-driven modeling of molecular mechanisms\\n\\n04 October 2021\\n\\n### Molecular function recognition by supervised projection pursuit machine learning\\n\\nOpen access\\n19 February 2021\\n\\n## Introduction', 'Understanding molecule entities (i.e., their properties and interactions) is fundamental to most biomedical research areas. For instance, experts study the structural properties of protein molecules to understand their mechanisms of action 1 , and investigate the interactions between drugs and target molecules to prevent adverse reactions 2 . To this end, people have built many biomedical knowledge bases (KBs), including PubChem 3 , Gene Ontology 4 , and DrugBank 5 . However, existing KBs are still far from complete due to the rapid growth of biomedical knowledge and the high cost of expert annotation. With the rapid progress of deep learning, machine reading systems are developed to automatically acquire biomedical knowledge by reading large-scale data, accelerating recent biomedical research in many cases 6 .\\nHowever, compared to human learners, machine reading systems still have a huge gap in terms of both versatile reading and knowledgeable learning 7 . In the acquisition of biomedical molecule knowledge, humans are capable of versatilely reading different types of information that complementarily characterize molecule entities, including molecule structures and biomedical text. Specifically, molecule structures provide concise standardized internal information, where functional groups and their positions are strong indicators of molecular properties and interactions.\\nIn comparison, biomedical text provides abundant flexible external information of molecule entities reported from wet-lab experiments. Utilizing complementary information is typically crucial for human learners to achieve comprehensive molecule understanding. Moreover, humans are able to knowledgeably learn and leverage meta-knowledge within and across different information—establishing fine-grained mappings between semantic units from different information sources, e.g., functional groups and natural language phrases—for deep molecule understanding.\\nTo the best of our knowledge, all existing machine reading systems for biomedical knowledge acquisition are confined to either internal molecule structure information or external biomedical text information in isolation, and different models have to be developed to process each type of information. This limits not only the generality of machine reading systems, but also the performance of knowledge acquisition due to the intrinsic nature of each information. Specifically, information from molecule structure is concise but typically limited compared to information from wet-lab experiments, while information from biomedical text enjoys better abundance and flexibility but usually suffers from noisy extraction processes. Moreover, confined to single information sources, machine reading systems can hardly learn meta-knowledge beyond single information for deep molecule understanding. Inspired by human learners, it is desirable to build a knowledgeable machine reading system that versatilely learns from both information sources to better master molecule knowledge so as to assist biomedical research. However, it is non-trivial to jointly model the heterogeneous data in a unified framework, and challenging to learn the meta-knowledge without explicit human annotation.\\nIn this work, we pioneer a knowledgeable machine reading system, establishing connections between internal information from molecule structures and external information from biomedical text, as shown in Fig. 1 . We jointly model the heterogeneous data in a unified language modeling framework, and learn the meta-knowledge by self-supervised language model pre-training techniques on large-scale biomedical data without using any human annotation. Specifically, for molecule encoding, there are various plausible choices such as descriptor-based models 8 and simplified molecular-input line-entry system (SMILES)-based models 9 . In this work, we serialize molecule structures using SMILES for programmatic simplicity, since they can be easily unified with textual tokens and processed by the Transformer architecture. Then the SMILES representations are segmented into frequent substring patterns using the byte pair encoding (BPE) 10 algorithm in a purely data-driven approach inspired by the tokenization and encoding method of predecessors 11 . Interestingly, we observe that the resultant substring patterns are chemically explainable (e.g., carbon chains and functional groups), and can potentially be aligned to molecule knowledge distributed in biomedical text. Therefore, we insert the segmented SMILES-based representations of molecules into their corresponding mentions in biomedical papers, and model the resultant data under a unified language modeling framework. Finally, the meta-knowledge is learned via self-supervised language model pre-training on the large-scale biomedical data. After pre-training, the meta-knowledge can be readily transferred via fine-tuning to facilitate various real-world biomedical applications.', 'Fig. 1: Conceptual diagram of knowledgeable and versatile machine reading.\\nHere we take salicylic acid as an example. Inspired by humans that versatilely learn meta-knowledge within and across different information, our machine reading system first serializes, a molecule structures via BPE on SMILES strings, then inserts the substrings into c . large-scale corpus and learns b fine-grained mapping between different semantic units by d mask language modeling. In this way, the system can perform e knowledgeable and versatile reading, achieving good performance on both mono-information downstream tasks and versatile reading tasks.\\nFull size image\\nComprehensive experiments demonstrate that, by learning deep meta-knowledge of molecule entities, the proposed model achieves promising performance on various biomedical applications in both molecule structure and biomedical text, including molecular property prediction, chemical reaction classification, named entity recognition and relation extraction. More importantly, by grasping meta-knowledge between molecule structures and biomedical text, our model enables promising cross-information capabilities. Our model is able to produce natural language documentation for molecule structures, and retrieve molecule structures for natural language queries. Such intelligent capabilities can provide convenient assistants and accelerate biomedical research. Through the multiple-choice questions about molecular properties, our model, which gets an accuracy score over 0.83, is proved to have deeper comprehension towards molecule structure and biomedical text than human professionals that get 0.77 accuracy. In the case study of six functional natural language queries towards 3,000 candidate molecule entities, 30 out of 60 retrieved entities can be supported by wet-lab experiments, among which 9 entities are not reported in PubChem (thus newly discovered), showing the promising potential of our model in assisting biomedical research in the future.\\nOur contributions are summarized as follows:\\n- (1) We present a knowledgeable and versatile machine reading system that bridges molecule structures and biomedical text.\\n- (2) Our major contribution lies in the application of the proposed model in assisting drug discovery and documentation for biomedical research.\\n- (3) Comprehensive experiments show the effectiveness of the proposed model.', '### Overview of KV-PLM\\n\\nWe propose KV-PLM, a unified pre-trained language model processing both molecule structures and biomedical text for knowledgeable and versatile machine reading. KV-PLM takes the popular pre-trained language model BERT 12 as the backbone. To process the heterogeneous data in a unified model, molecule structures are first serialized into SMILES 9 strings, and then segmented using BPE 10 algorithm. To learn the meta-knowledge between different semantic units, we pre-train KV-PLM using the masked language modeling task 12 . During pre-training, part of the tokens (including tokens from molecule structure and biomedical text) are randomly masked, and the model is asked to reconstruct the masked tokens according to the context. In this way, the model can grasp the correlation between molecule structure and biomedical text without any annotated data. After pre-training, the model can be readily fine-tuned to facilitate various mono-information and cross-information biomedical applications.\\nTo comprehensively investigate the biomedical capabilities of KV-PLM, we conduct experiments in different aspects. We first evaluate KV-PLM on mono-source biomedical tasks, including molecule structure tasks and biomedical text tasks. Then we test KV-PLM on challenging versatile reading tasks that require a deep understanding of both molecule structures and biomedical text. In the following sections, we present experimental results in Table 1 from each aspect, and then draw the main conclusions. Finally, we present a case study, showing the potential of our knowledgeable machine reading system in assisting biomedical research in real-world scenarios.\\nTable 1 The main experimental results on mono-information tasks and versatile reading tasks.\\nFull size table\\n\\n### Baseline models\\n\\nWe compare KV-PLM with strong baseline models to demonstrate the effectiveness of our method.\\nRXNFP . RXNFP 13 is the state-of-the-art model for chemical reaction classification. The model is based on Transformer architecture 14 and pre-trained by masked language modeling task on chemical reaction formulas. However, tailored for processing molecule structure tasks, RXNFP cannot be applied to natural language tasks.\\nBERT wo . To observe the effect of pre-training, we adopt BERT without any pre-training as a baseline. Notice that this model tokenizes SMILES strings altogether with natural language text using the tokenizer from the frequently-used Sci-BERT model 15 , thus gets piecemeal subwords which can hardly be read by humans.\\nSMI-BERT . For molecule structure tasks, one commonly-used method is to conduct mask language modeling on SMILES strings. We take SMI-BERT, which is only pre-trained on SMILES strings as a mono-information pre-trained baseline. The tokenizer is also the same as Sci-BERT.\\nSci-BERT . One of the most frequently used pre-trained language models in biomedical domain. It is trained on plenty of natural language data and could solve natural language tasks well. In other words, Sci-BERT is also a mono-information pre-trained baseline.\\nKV-PLM . According to our idea, the model can be pre-trained on a special corpus in which SMILES strings are inserted, and in this way KV-PLM can learn mono-information knowledge. It is expected to have obviously better performance on versatile reading tasks.\\nKV-PLM* . As we have mentioned above, SMILES strings can be tokenized with a separate tokenizer and form chemically explainable substring patterns, which have no overlap with natural language tokens. We improve KV-PLM by adopting double tokenizers to process SMILES strings in a more appropriate way.\\n\\n### Molecule structure tasks', '### Molecule structure tasks\\n\\nFor molecule structure, SMILES strings are commonly used molecule and chemical reaction representations. We choose molecular property learning benchmark MoleculeNet 16 and chemical reaction dataset USPTO 1k TPL 13 as our experimental materials.\\nFor SMILES property classification task on MoleculeNet , We choose four commonly-used classification task themes including BBBP, SIDER, TOX21, and HIV to evaluate the capability of reading SMILES strings and analyzing properties of molecules. The properties these tasks focus on are blood-brain barrier penetration, the ability to inhibit HIV replication, Toxicology in the 21st Century and Side Effect Resource in order. We follow the setting recommended by the benchmark and present ROC-AUC score for evaluation. Table 1 only presents average score for the four themes. Please refer to Table 3 for more details and baselines.\\nFor chemical reaction classification task on USPTO 1k TPL , it is a newly-released dataset that contains the 1000 most common reaction template classes. Previous study 13 has proved that BERT pre-trained on the large scale of SMILES strings can solve the task quite well. To make it more challenging, we generalize a few-shot learning subset (hereinafter referred to as USP-few) containing 32 training items for each class. We follow the original setting and present macro F1 for classification evaluation.\\nFrom the results we can see that pre-training greatly improves the performance. Mono-information pre-training model SMI-BERT already gets a high average score on property classification themes, showing that focusing on internal knowledge mining may finish MoleculeNet tasks quite well. Pre-training on natural language shows a further positive effect for molecule structure tasks, indicating the value of external knowledge. Sci-BERT surprisingly achieves good performance without pre-training on SMILES strings, and this leads to the assumption that there is a certain connection between SMILES patterns and natural language patterns, which is a quite interesting discovery worthy of further investigation.\\nComparing KV-PLM with KV-PLM* we could see that the separate tokenizer works worse than the original natural language tokenizer on molecule structure tasks. This is because that a few atoms or functional groups and their spatial structures are ignored by the separate tokenizer for the convenience of forming substring patterns, while attention for specific atoms or functional groups is important especially for chemical reaction classification.\\n\\n### Natural language tasks\\n\\nTo recognize entities and extract their relations from unstructured text is a fundamental application for machine reading, and by this way we can form easy-to-use structured knowledge automatically. During the process, there are two important tasks including named entity recognition (NER) and relation extraction (RE). We choose BC5CDR NER dataset (hereinafter referred to as BC5CDR) and ChemProt dataset as our experimental materials.\\nFor biomedical NER task on BC5CDR , models are required to perform sequence labeling, where each textual token is classified into semantic labels that indicate locations and types of named entities. This is an important evaluation task because entities are the main processing objects in biomedical domain, and linking between structural knowledge and raw text is also based on entity recognition. Notice that the type of entities is usually specified for biomedical NER, and BC5CDR mainly focuses on recognition for chemical molecules and diseases.\\nFor RE task on ChemProt , models are required to perform relation classification for entity pairs. We expect machine reading systems to recognize the relationships between the given entities so that the raw text could be formalized into easy-to-use formats including graph and triplet. There are 13 relation classes between chemical and protein pairs. Entities are annotated in the sentences.\\nResults for NER and RE are shown in Table 1 . We take span-level macro F 1 score for NER and sentence-level micro F 1 score for RE as usual. As we can see, pre-training is of key importance for natural language tasks, and cross-information pre-training achieves better performance than mono-information pre-training, which proves that KV-PLM successfully learns internal structural knowledge and this can help it understand natural language. Pre-training on pure SMILE strings also helps natural language tasks, verifying the assumption that a connection exists between SMILES patterns and natural language patterns.\\n\\n### Versatile reading tasks', '### Versatile reading tasks\\n\\nSince the biomedical text in natural language form is the most comprehensive material for humans, and molecule structure is the most direct information of molecules, we expect our model to process both of the two information, and understand the global and local properties of molecules.\\nThere are few ready-made suitable datasets for versatile reading of SMILES strings and natural language documentation. We collect 15k substances in PubChem which have names, SMILES and corresponding paragraphs of property descriptions. We name our cross-information fine-tuning data as PCdes.\\nFor cross-information retrieval , it is formulated as a bidirectional retrieval task for the chemical-description pairs. We evaluate the capability of understanding paragraph-level descriptions and describing global properties of molecules. KV-PLM is fine-tuned on PCdes, trying to pick the best match SMILES string or property description sentence for each other. The matching score is obtained by the cosine similarity of text representations. For evaluation metrics, we report the accuracy of the top retrieval result in randomly sampled mini-batches (64 pairs in each mini-batch). Models are also required to rank the average matching score for all the 3k molecules and description paragraphs. We present r e c a l l @ 20 for both directions.\\nFor match judging , we evaluate the capability of understanding sentence-level descriptions and distinguishing the local properties of molecules. To this end, we propose the multiple-choice task CHEMIchoice.\\nBased on descriptions in PCdes, 1.5k multiple choices are automatically generated. For the given SMILES string of substance in the test set, there are four choices of a single description sentence. Negative samples similar to the positive sample are removed, helping decrease the possibility of false negative for ground-truth answers. The system is required to choose the correct answer just like a student completing an exam, which is a quite realistic situation. The schematic diagram for CHEMIchoice is shown in Fig. 2 .\\nFig. 2: Schematic diagram for KV-PLM* finishing CHEMIchoice task.\\nFor the given unfamiliar molecule entity, we get a versatile materials including structure and description, from which we know the correct sentence and randomly pick wrong sentences from the pool to form four choices. b Molecule structure and text of choices are fed into KV-PLM* and get their representations, based on which the confidence scores of choices are calculated by cosine similarity. c The tokenizers for structures and biomedical text are different. In this instance, KV-PLM* successfully finds out the correct description sentence for the given substance.\\nFull size image\\nWe report the results of the experiments above in Table 1 . Distinct data samples are used in repeating experiments with the random generation process of CHEMIchoice. As expected, with the help of cross-information pre-training on heterogeneous data, KV-PLM* can process versatile reading tasks well and achieve the best performance on most of the metrics.\\nFor human professional performance , we recruited six undergraduates and postgraduates from top universities who major in chemistry without exam failure record. Given 200 questions randomly sampled from CHEMIchoice, they are required to choose the best match property description sentence for each chemical structure.\\nHuman professionals are told that they are participating in a study to provide human performance evaluation and the experimental remuneration is determined by the rationality of their answers, thus they would not deliberately lower their level. All participants gave informed consent for their test data for this study. This research does not involve ethical issues. Academic Committee of the Department of Computer Science and Technology of Tsinghua University approved the protocol.\\nThe performances of the six professionals exhibit diversity as shown in Fig. 3 . The average score of them is 64.5 and the highest score is 76.5. We take the highest score to represent the human level since it shows the property prediction capability of an expert who is well-trained and has abundant knowledge about this type of questions, while the score is still significantly lower than our model performance. We analyze their incorrect answers and find that human professionals tend to choose common property descriptions that do not necessarily match the target substance (e.g., irritates skin, eyes, and mucous membranes), and they are not strong in judging the unique properties of the substance to be analyzed.\\nFig. 3: Score comparison of CHEMIchoice task.\\nOur model successfully surpasses human professionals, showing its promising capability of comprehending molecule structure and biomedical text. Error bars indicates standard deviation over six runs.\\nFull size image\\n\\n### Result analysis', 'From the experimental results in Table 1 , we draw three main findings as below:\\n- (1) Pre-training on mono-information data can greatly improve model performance on corresponding downstream tasks. Specifically, SMI-BERT outperforms BERT wo on molecule structure tasks, and Sci-BERT works better than BERT wo on natural language tasks. In addition, mono-information pre-trained models also achieve reasonable performance on versatile tasks. The results show that pre-training can effectively grasp meta-knowledge within each type of information to help biomedical tasks.\\n- (2) Interestingly, we find that mono-information pre-training also brings improvements to downstream tasks from other information types. Specifically, despite being pre-trained on natural language data, when fine-tuned on molecule structure tasks, Sci-BERT even outperforms strong SMI-BERT and RXNFP models that are tailored for and pre-trained on molecule structure data. This indicates that there may exist certain connections between the patterns of molecule structures and natural language. For example, compositionality and hierarchy are important properties of both molecule structures and natural language, which can be transferred to help tasks from different information sources.\\n- (3) Cross-information pre-training enables unified machine reading systems that outperform the baseline methods on biomedical tasks from both information sources. Moreover, our models also achieve state-of-the-art performance on versatile tasks, showing their promising potential in assisting biomedical research in these significant scenarios in the future. The results show the importance of integrating both internal and external molecule information, and the effectiveness of the proposed machine reading method for biomedical tasks.\\nIn this subsection, we first give observations about the properties of substring patterns learned by models. From Fig. 4 we can see that substring patterns, which we think are of similar properties tend to have closer fingerprints due to pre-training, showing that mask learning helps model build mapping correlation in an unsupervised fashion.\\nFig. 4: Visualizing substring pattern embeddings using t-SNE\\nParts of substring pattern fingerprints are randomly chosen and processed for dimensionality reduction. Similar substring patterns are marked in the same colors. The upper one shows fingerprints from pre-trained KV-PLM*, and the lower one is from the model finetuned on PCdes.\\nFull size image\\nThe clusters become tighter after being given the alignment supervised information just as the lower subgraph shows. Moreover, we can look at the vectors in purple and find that the model can correctly distinguish between alcohol and phenol, and also understand the meaning of acid and organic salt. This proves the capability of our model to learn not only isolated but also combined properties of substring patterns and mapping between SMILES and text when finetuned on versatile reading tasks.\\nFurther, we mainly discuss the KV-PLM* fine-tuned on PCdes due to the novelty of versatile reading tasks. To observe the retrieval capability and further potential, we can conduct both description retrieval and molecule retrieval.\\nFor description retrieval , the system finds appropriate descriptive sentences and generates a paragraph of natural language description for the given SMILE string. Sentences and substances are randomly selected from PCdes test set. Figure 5 shows the property description of Tuberin predicted by KV-PLM*. The ether bond is predicted as alcohol at first, and successfully recognized as aromatic ether after getting the input benzene ring pattern. The model even predicts that Tuberin has a role as an antioxidant mainly due to the double bonds, which is not recorded in PubChem. Crystalline is also a correctly predicted property.\\nFig. 5: Case study for property prediction.\\nThe molecular structures are first serialized in SMILES strings. With more SMILES sub-groups provided (in purple), the model can predict the properties more precisely.\\nFull size image\\nAnother instance is 4-hydroxychalcone. Aromatic and benzoic properties are predicted after the phenol group is shown. Fruity taste and relatively neural are newly supplemented properties when given the double bond. After seeing the whole structure, the system gives out a more precise property description, predicting that it has a role as a plant metabolite and inhibitor, and also prevents oxidation.\\nSimpler compounds are also tested. For Chloroacetonitrile, the carbon nitrogen triple bond helps predict that it is toxic by ingestion. Combining with the chlorine the model eventually knows that it is a colorless toxic gas and has a role as an organic pollutant.', 'For molecule retrieval , the system reads natural language instructions and retrieves matching SMILES strings in turn. We require our model to find an anti-inflammatory agent from PCdes test set, and ten substances with the highest similarity scores are listed in Table 2 . Most of them are related to inflammation or the immune system, and there are four substances clearly proved to be an anti-inflammatory agent. For Elocalcitol and Marinobufagenin, data in PubChem doesn’t show this information, or to say, the two agents are “newly-discovered”.\\nTable 2 Case study for drug discovery.\\nFull size table\\nOther queries including antineoplastic agent, antioxidant agent, herbicide, dye, and antidepressant drug are tested, and half of all substances the model retrieved are sure to meet the requirements. There are also several properties for substances that are missed in PubChem. For those newly-discovered molecule entities, supporting details can be found in corresponding references.\\nResults above show that our model catches the separation and combination properties of SMILES substring patterns, and aligns the semantic space of SMILES strings and natural language quite well. There is a chance for our method to contribute to open property prediction of molecules and drug discovery process.\\nIn this article, we show the possibility of bridging the SMILES string and natural language together and propose the BERT-based model KV-PLM for knowledgeable and versatile machine reading in the biomedical domain. Through pre-training on the special corpus, external knowledge from language and internal knowledge from molecule structure can fuse with each other unsupervisedly. KV-PLM has a basic understanding of molecule entities, and the satisfying performance when fine-tuned on various downstream tasks proves the effectiveness of molecular knowledge. Our model achieves higher accuracy than baseline models on MoleculeNet benchmark, and brings significant improvement for the more challenging task on USP-few. Even as a plain language model, our model can process classical tasks including Chemprot RE and CDR NER quite well. KV-PLM shows its capability to be a general biomedical machine reading model.\\nMeanwhile, the advantages for bridging the two formats of text are not restricted to the applications in mono-information form. Since there exists a correspondence between SMILES strings and natural language descriptions, we process them with a method similar to cross-information learning. By fine-tuning on PCdes data, KV-PLM can achieve cross retrieval between substances and property descriptions. We propose a new task CHEMIchoice to evaluate the reading ability on SMILES strings and natural language and also the alignment ability between them. Further, we take qualitative analysis about the potential of KV-PLM on open property prediction and drug discovery.\\nStill, there are some problems waiting for us to solve. First we need a better definition and evaluation for cross-domain reading tasks. Considering that models may only rely on a few sentences if training by paragraphs, we align the SMILES strings and descriptions by sentence. However, this method brings noises because randomly picked negative samples from other paragraphs may also be correct for the given substances. Besides, we simplify the SMILES strings to get more concise substring pattern results, while by removing brackets and number labels we lose information about the spatial structure. What’s more, it is a simple and rude way to linearly connect SMILES strings and natural language in series. More clever structures for fusing internal and external knowledge about chemicals and other types of entities are expected to be proposed.\\nOur future work will focus on the problems above, trying to get a more perfect model structure, training method and also benchmark. Graph structure and more complicated molecule representations may be adopted. Generation systems instead of retrieval may also bring different but interesting effects.', '### Related work', 'Various structural machine reading systems have been developed to read molecule structures for molecular knowledge acquisition. In early years, machine learning algorithms help with molecular dynamics simulation 17 , 18 and energy calculations 19 . Recently, neural networks have been one of the most popular tools for analyzing molecule properties. Molecule fingerprints computed by neural networks have achieved competitive performance as compared with expert-crafted descriptors 8 , 20 , 21 . Notably, recent studies show promising results in modeling serialized molecule structures using powerful neural language models 11 , 22 , 23 , 24 .\\nSince it is nearly impossible for human experts to read such a huge number of papers, machine reading systems, powered by natural language processing (NLP) techniques, are developed to extract molecule entities and their relations by reading large-scale biomedical literature 25 , 26 , 27 , 28 , 29 , 30 . To this end, researchers have proposed various neural language models to understand biomedical text, including convolutional neural networks, recurrent neural networks, recursive neural networks, and self-attention-based neural networks 31 , 32 , 33 . Recently, neural language models equipped with self-supervised pre-training techniques 34 , 35 have greatly pushed the state-of-the-art on a broad variety of biomedical information extraction tasks 15 , 36 .\\nIn this work, we bridge molecule structures and biomedical text in a unified multimodal deep learning framework. Previous works explore employing deep learning models to connect multimodal information, including medical images and text 37 , natural images and text 38 , molecules and reactions 39 and molecules and protein sequences 40 . There are also some works investigating pre-training vision-language models 41 , 42 . In comparison, our model jointly learns molecule structure and biomedical text representations, and establishes convenient interaction channels between biomedical molecule knowledge and researchers for comprehensive biomedical research assistance.\\nOur pre-training corpus comes from S2orc 43 which is a PDF-parse English-language academic papers corpus.\\nWe take over 0.3 million papers which contain 1 billion tokens for pre-training. 75% of the papers are under Medicine , Biology or Chemistry fields, and the other 25% are under Computer Science field. In order to reduce the number of special characters related to experimental data in the text, we choose the abstract, introduction, and conclusion sections of the papers. No other special preprocessing is applied. For chemical substances we use documents from PubChem 3 , in which there are over 150 million chemicals with SMILES strings and synonyms. To insert SMILES strings for chemicals, we need to do entity linking for the corpus. Since high precision and comparably low recall is acceptable for the large scale of unsupervised data, we first recognize possible entities with the help of SciSpacy 44 , and then link these words with KB entities if the words can exactly match the high confidence synonyms. Notice that some substances have the same name with common objects, including “dogs”, “success” and so on. Thus we filter out common words from the synonym dictionary.\\nThere are altogether 10k chemicals with 0.5 million times of occurrence being detected in our corpus.\\nFor the natural language part, we use the vocabulary list exactly the same as Sci-BERT, which is more appropriate than the original BERT vocabulary on academic papers. For the SMILES strings part, we apply the BPE 10 encoding method ( https://github.com/rsennrich/subword-nmt ) to 20,000 SMILES strings randomly chosen from PubChem and get a special vocabulary list. It is already stated that brackets and number labels are ignored. Finally, we filter out those whose frequency is lower than 100 and get 361 substring patterns among which functional groups can be observed, indicating the effectiveness of the splitting method. All the SMILES strings are split into substring patterns separately with natural language.\\nFor SMILES strings processing tasks, we adopt MoleculeNet 16 , a wide standard benchmark where molecule properties for SMILES string are concluded into 17 types, and expressed in the form of classification or regression tasks. We adopt four representative tasks from MoleculeNet including BBBP, HIV, Tox21, and SIDER. We use the official training, validation and test sets provided by DeepChem 45 package to ensure that the performance is relatively stable and reproducible. For the two multilabel datasets Tox21 and SIDER, we report the average scores for all the tasks.\\nSpecifically, we adopt the following tasks and datasets:\\n- (1) BBBP , the blood-brain barrier penetration dataset. It includes binary labels for 2053 compounds on their permeability properties. Binary labels for penetration/non-penetration are given.', '- (1) BBBP , the blood-brain barrier penetration dataset. It includes binary labels for 2053 compounds on their permeability properties. Binary labels for penetration/non-penetration are given.\\n- (2) SIDER , the Side Effect Resource database of marketed drugs and adverse drug reactions. It groups drug side effects into 27 system organ classes and includes binary labels for 1427 drugs. Symptoms waited for binary classification including endocrine disorders, eye disorders and so on.\\n- (3) Tox21 , a public database measuring the toxicity of compounds created by the “Toxicology in the 21st Century”. It contains qualitative toxicity measurements for 8014 compounds on 12 different targets, including nuclear receptors and stress response pathways. Molecules are supposed to be classified between toxic and nontoxic on each target.\\n- (4) HIV , a dataset introduced by the DTP AIDS Antiviral Screen. It tests the ability to inhibit HIV replication for 41,127 compounds required to classify between inactive and active.\\nBesides, we adopt USPTO 1k TPL dataset ( https://github.com/rxn4chemistry/rxnfp ) and create a few-shot subset. The original set has 410k data items. We randomly pick 32 items for each class and get 32k items in total. In prepossessing SMILES representations, to prevent sparse SMILES tokenization (i.e., producing over-specific infrequent tokens), we remove numbers and brackets before feeding them to KV-PLM* tokenizer. No other prepossessing steps are conducted.\\nFor natural language processing, we adopt Chemprot and BC5CDR dataset. Chemprot is a text mining chemical-protein interactions corpus with 13 classes of relation types including inhibitor , product-of and so on. There are 1020 abstracts (230k tokens) for train set, 612 abstracts (110k tokens) for dev set and 800 abstracts (180k tokens) for test set. BC5CDR is a chemical-disease relation detection corpus with 1500 abstracts in total and equally divided into train set, dev set and test set. There are over 5k mentions of chemicals in each set. More researches focus on the NER task than the relation detection task of BC5CDR. We use the version of the two datasets provided by Sci-BERT ( https://github.com/allenai/scibert ) and there is no special preprocessing of the data.\\nFor cross-information tasks, we evaluate our retrieval model on PCdes. Specifically, we substitute all the synonyms of the ground-truth substances into the word it to avoid information leakage. 15k SMILES-description pairs in PCdes are split into training, validation and test sets with ratio 7:1:2. For matching judging, we first construct a choice base consisting of the 870 description sentences that occur more than five times except for derivation descriptions. Then we sort the sentence strings to assign similar sentences with closer index. To generate multiple-choice questions for CHEMIchoice, for each of the 1428 test substances, we randomly choose a sentence from the corresponding ground-truth descriptions as the positive choice. The negative choices are sampled from the choice base, where the difference between indexes of positive and negative sentences is greater than 10. In this way, we largely avoid false negative choices.\\nKV-PLM is based on the BERT model, which is one of the most popular language models in recent years. Specifically, KV-PLM has 12 stacked Transformer layers with 110M parameters in total, where each Transformer layer consists of a self-attention sub-layer followed by a feedforward sub-layer.\\nThere are plenty of ready-made frameworks for BERT. For computation efficiency, we initialize our model by Sci-BERT uncased version. To adapt to downstream tasks, following previous works 12 , 15 , we introduce a classification layer on top of the model, which can perform sequence classification and sequence labeling for SMILES string classification, RE and NER tasks.\\nTo apply deep models to SMILES strings processing, there are different strategies for tokenization 46 . In one of our model variants KV-PLM, we directly take the tokenizer of Sci-BERT to tokenize SMILES strings, regarding them exactly the same as general text. In the other variant KV-PLM*, inspired by the SPE-based generative models 47 , we apply BPE to SMILES strings to better control the tokenization of SMILES strings. For example, by controlling the vocabulary size of the SMILES string tokenizer, we can largely prevent over-specific infrequent tokenization results.\\nFor the retrieval system, we regard SMILES strings as queries and descriptions retrieval candidates. The core idea is to focus on the nearest negative samples instead of all. The encoder for SMILES strings is the same one with descriptions since SMILES strings are also linear text and can be easily fused. Let f( t ) be a feature-based representation computed by encoder from text t . Define the retrieval score between SMILES string of a molecule m and a unit of description d as:', '$${{{{{{{\\\\rm{s}}}}}}}}({{{{{{{\\\\bf{m}}}}}}}},{{{{{{{\\\\bf{d}}}}}}}})=\\\\frac{{{{{{{{\\\\rm{f}}}}}}}}({{{{{{{\\\\bf{m}}}}}}}})\\\\cdot {{{{{{{\\\\rm{f}}}}}}}}({{{{{{{\\\\bf{d}}}}}}}})}{| {{{{{{{\\\\rm{f}}}}}}}}({{{{{{{\\\\bf{m}}}}}}}})| \\\\cdot | {{{{{{{\\\\rm{f}}}}}}}}({{{{{{{\\\\bf{d}}}}}}}})| },$$\\nwhich is the cosine similarity of two representations. We refer to the loss function in VSE++ 48 which is a similar representative image-caption retrieval method. For a positive pair ( m , d ), we calculate the Max of Hinges (MH) loss:\\n$${{{{{{{{\\\\mathcal{L}}}}}}}}}_{{{{{{{{\\\\rm{MH}}}}}}}}}= \\\\, \\\\mathop{\\\\max }\\\\limits_{{{{{{{{\\\\bf{d}}}}}}}}^{\\\\prime} }[\\\\alpha +{{{{{{{\\\\rm{s}}}}}}}}({{{{{{{\\\\bf{m}}}}}}}},{{{{{{{\\\\bf{d}}}}}}}}^{\\\\prime} )-{{{{{{{\\\\rm{s}}}}}}}}({{{{{{{\\\\bf{m}}}}}}}},{{{{{{{\\\\bf{d}}}}}}}})]\\\\\\\\ \\\\, +\\\\mathop{\\\\max }\\\\limits_{{{{{{{{\\\\bf{m}}}}}}}}^{\\\\prime} }[\\\\alpha +{{{{{{{\\\\rm{s}}}}}}}}({{{{{{{\\\\bf{m}}}}}}}}^{\\\\prime} ,{{{{{{{\\\\bf{d}}}}}}}})-{{{{{{{\\\\rm{s}}}}}}}}({{{{{{{\\\\bf{m}}}}}}}},{{{{{{{\\\\bf{d}}}}}}}})],$$\\nwhere α is a margin hyperparameter, \\\\({{{{{{{\\\\bf{d}}}}}}}}^{\\\\prime}\\\\) and \\\\({{{{{{{\\\\bf{m}}}}}}}}^{\\\\prime}\\\\) are negative descriptions and SMILES strings from the batch.', '### Baselines tailored for mono-information tasks\\n\\nFor mono-information tasks including natural language tasks and molecule tasks, there are plenty of mature methods specially designed for them. Here, we compare our models with baselines tailored for mono-information tasks.\\nExperiment results on 4 MoleculeNet themes are shown in Table 3 . D-MPNN 8 is a supervised graph convolution based method combined with descriptors. Random forest (RF) 49 is a representative method for statistical machine learning which also takes descriptors as the input. DMP 50 is an unsupervised pre-training method that takes SMILES strings and molecular graphs as the input.\\nTable 3 Experiment results on 4 MoleculeNet themes. Baseline results are cited from ref.\\nFull size table\\nAlthough not tailored for molecule tasks, our models still achieve reasonable performance compared to strong baselines. It is promising to leverage more advanced molecule encoders in PLMs to further improve the results, which we leave for future research.\\nExperiment results for ChemProt relation extraction and BC5CDR NER are shown in Table 4 . We observe that pre-trained language models are generally the best solutions for these natural language processing tasks. We report the results of BioBERT (+PubMed) and RoBERTa 51 , which are both popular models and achieve comparable results with Sci-BERT. Note that the initial version of BioBERT 36 underperforms Sci-BERT, while the recently released version is additionally trained on PubMed corpus, which helps it become the state-of-the-art model on ChemProt and BC5CDR. Our models achieve comparable performance with BioBERT (+PubMed).\\nTable 4 Experiment results on ChemProt and BC5CDR.\\nFull size table\\n\\n### Training settings\\n\\nFor Natural Language tasks, the authors of BERT provided range of possible values to work well across various tasks: batch size [8, 16, 32], Adam learning rate [2 e −5, 3 e −5, 5 e −5], epoch number [2, 3, 4]. We conduct grid search in the hyper-parameters above. In ChemProt RE task, we set batch size as 8, learning rate as 2 e −5 and epoch number as 4. In BC5CDR NER task, we set batch size as 16, learning rate as 3 e −5 and epoch number as 4. Grid search is also done for the strongest baseline model Sci-BERT and the best hyper-parameters are proved to be the same.\\nFor the MoleculeNet tasks, we search suitable learning rate in [5 e −6, 5 e −5, 5 e −4] and batch size in [64, 128, 256], which are bigger than above because data points are more than natural sentences. It turns out to be relatively insensitive to hyperparameters changes as long as convergence is guaranteed. In MoleculeNet tasks, we set batch size as 128, learning rate as 5 e −6 and epoch number as 20. In USP-few task, we set batch size as 256, learning rate as 5 e −5 and epoch number as 30.\\nFor retrieval training, since the batch size recommended in VSE++ 48 is 128 while our training data scale is not so huge, we finally set batch size as 64. We set epoch number as 30, learning rate as 5 e −5 and margin as 0.2.\\nNotice that in all the experiments above, BertAdam optimizer is used and warmup proportion is 0.2. Max length is 128 for sentences and 64 for SMILES strings. For rxnfp model, since it is the only model of which the hidden size is 256 instead of 768, we set the learning rate as 5 e −4 due to the smaller scale of parameters and get better performance.\\nTools and packages that we used in the experiments include: torch, transformers, numpy, sklearn, tqdm, seqeval, chainer-chemistry, rdkit, subword-nmt, boto3, and requests.\\n\\n### Reporting summary\\n\\nFurther information on research design is available in the Nature Research Reporting Summary linked to this article.\\n\\n## Data availability\\n\\nData that support the findings of this study have been deposited in Google Drive: https://drive.google.com/drive/folders/1xig3-3JG63kR-Xqj1b9wkPEdxtfD_4IX .\\n\\n## Code availability', 'The code of this study 52 can be obtained from GitHub: https://github.com/thunlp/KV-PLM . The zip file of code can be downloaded via the Google Drive link above.\\n- Yang, F., Larry, G. M. & George, N. P. The molecular structure of green fluorescent protein. Nat. Biotechnol. 14.10 , 1246–1251 (1996). Article Google Scholar\\n- Lounkine, E. et al. Large-scale prediction and testing of drug activity on side-effect targets. Nature 486.7403 , 361–367 (2012). Article Google Scholar\\n- Yanli, W. et al. PubChem: a public information system for analyzing bioactivities of small molecules. Nucleic Acids Res. 37 , W623–W633 (2009). Article Google Scholar\\n- Gene Ontology Consortium. Creating the gene ontology resource: design and implementation. Genome Res. 11.8 , 1425–1433 (2001). Article Google Scholar\\n- Wishart, D. S. et al. DrugBank: a knowledgebase for drugs, drug actions and drug targets. Nucleic Acids Res. 36 , D901–D906 (2008). Article CAS Google Scholar\\n- Lixiang, H. et al. A novel machine learning framework for automated biomedical relation extraction from large-scale literature repositories. Nat. Mach. Intell. 2.6 , 347–355 (2020). Google Scholar\\n- Xu, H., Zhang, Z. & Liu, Z. Knowledgeable machine learning for natural language processing. Commun. ACM 64.11 , 50–51 (2021). Google Scholar\\n- Kevin, Y. et al. Analyzing learned molecular representations for property prediction. J. Chem. Inf. Model. 59.8 , 3370–3388 (2019). Google Scholar\\n- David, W. SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci. 28.1 , 31–36 (1988). Google Scholar\\n- Sennrich, R., Barry, H. & Alexandra, B. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics Vol. 1 (Long Papers, 2016).\\n- Chithrananda, S., Gabriel G. & Bharath, R. ChemBERTa: large-scale self-supervised pretraining for molecular property prediction. Preprint at https://arXiv.org/2010.09885 (2020).\\n- Devlin, J., Ming-Wei Chang, M.-W. & Toutanova, L. K. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings on 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT2019) (2020).\\n- Philippe, S. et al. Mapping the space of chemical reactions using attention-based neural networks. Nat. Mach. Intell. 3.2 , 144–152 (2021). Google Scholar\\n- Vaswani, A. et al. Attention is all you need. Adv. Neural Inf. Process. Syst . 30 , 5998–6008 (2017).\\n- Beltagy, I., Lo, K. & Cohan, A. SciBERT: a pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (Association for Computational Linguistics, 2019).\\n- Zhenqin, W. et al. MoleculeNet: a benchmark for molecular machine learning. Chem. Sci. 9.2 , 513–530 (2018). Google Scholar\\n- Junmei, W. & Hou, T. Application of molecular dynamics simulations in molecular property prediction. 1. density and heat of vaporization. J. Chem. Theory Comput. 7.7 , 2151–2165 (2011). Google Scholar\\n- Venkatesh, B. & Ramprasad, R. Adaptive machine learning framework to accelerate ab initio molecular dynamics. Int. J. Quantum Chem. 115.16 , 1074–1083 (2015). Google Scholar\\n- Katja, H. et al. Assessment and validation of machine learning methods for predicting molecular atomization energies. J. Chem. Theory Comput. 9.8 , 3404–3419 (2013). Google Scholar\\n- Duvenaud, D. K. et al. Convolutional networks on graphs for learning molecular fingerprints. Adv. Neural Inf. Process. Syst. 28 , 2224–2232 (2015). Google Scholar\\n- Coley, C. W. et al. Convolutional embedding of attributed molecular graphs for physical property prediction. J. Chem. Inf. Model. 57.8 , 1757–1772 (2017). Article Google Scholar\\n- Wang, S. et al. SMILES-BERT: large scale unsupervised pre-training for molecular property prediction. In Proceedings of the 10th ACM International Conference on Bioinformatics, Computational Biology and Health Informatics (ACM, 2019).\\n- Shion, H., Shi, S. & Ueda, H. R. Smiles transformer: Pre-trained molecular fingerprint for low data drug discovery. Preprint at https://arxiv.org/1911.04738 (2019).\\n- Lim, S. & Lee, Y. O. Predicting chemical properties using self-attention multi-task learning based on SMILES representation. In 2020 25th International Conference on Pattern Recognition (ICPR) (IEEE, 2021).\\n- Nanyun, P. et al. Cross-sentence n-ary relation extraction with graph lstms. Trans. Assoc. Comput. Linguist. 5 , 101–115 (2017). Article Google Scholar', '- Nanyun, P. et al. Cross-sentence n-ary relation extraction with graph lstms. Trans. Assoc. Comput. Linguist. 5 , 101–115 (2017). Article Google Scholar\\n- Patrick, V., Strubell, E. & McCallum, A. Simultaneously self-attending to all mentions for full-abstract biological relation extraction. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , Vol. 1 (Long Papers, 2018).\\n- Yuan, Y. et al. DocRED: a large-scale document-level relation extraction dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics, 2019).\\n- Guoshun, N. et al. Reasoning with latent structure refinement for document-level relation extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics, 2020).\\n- Bowen, D. et al. Meta-information guided meta-learning for few-shot relation classification. In Proceedings of the 28th International Conference on Computational Linguistics (2020).\\n- Jenny, C. et al. Named entity recognition in chemical patents using ensemble of contextual language models. In Proceedings of the CLEF 2020 Conference (CLEF, 2020).\\n- Bin, H., Guan, Y. & Dai, R. Classifying medical relations in clinical text via convolutional neural networks. Artif. Intell. Med. 93 , 43–49 (2019). Article Google Scholar\\n- Li, D. et al. Biomedical event extraction based on knowledge-driven tree-lstm. In Proceedings on 2019 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT2019) (2020).\\n- Papanikolaou, Y., Roberts, I. & Pierleoni, A. Deep bidirectional transformers for relation extraction without supervision. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019) (2019).\\n- Han, X. et al. Pre-trained models: past, present and future. AI Open (2021).\\n- Jie, Z. et al. Graph neural networks: a review of methods and applications. AI Open 1 , 57–81 (2020). Article Google Scholar\\n- Jinhyuk, L. et al. BioBERT: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics 36.4 , 1234–1240 (2020). Google Scholar\\n- Zhang, Y. et al. Contrastive learning of medical visual representations from paired images and text. Preprint at https://arXiv.org/2010.00747 (2020).\\n- Radford, A. et al. Learning transferable visual models from natural language supervision. International Conference on Machine Learning (PMLR, 2021).\\n- Seidl, P. et al. Modern hopfield networks for few-and zero-shot reaction template prediction. Preprint at https://arXiv.org/2104.03279 (2021).\\n- Lenselink, E. B. et al. Beyond the hype: deep neural networks outperform established methods using a ChEMBL bioactivity benchmark set. J. Cheminform. 9.1 , 1–14 (2017). Google Scholar\\n- Xu, Y. et al. LayoutLMv2: multi-modal pre-training for visually-rich document understanding. Preprint at https://arXiv.org/2012.14740 (2020).\\n- Ni, M. et al. M3p: learning universal representations via multitask multilingual multimodal pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2021).\\n- Lo, K. et al. S2ORC: the semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (Association for Computational Linguistics, 2020).\\n- Neumann, M. et al. ScispaCy: fast and robust models for biomedical natural language processing. In Proceedings of the 18th BioNLP Workshop and Shared Task (Association for Computational Linguistics, 2019).\\n- Ramsundar, B. Molecular machine learning with DeepChem. Dissertion . (Stanford University, 2018).\\n- Omote, Y. et al. Transformer-based approach for predicting chemical compound structures. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing (Association for Computational Linguistics, 2020).\\n- Xinhao, L. & Fourches, D. SMILES pair encoding: a data-driven substructure tokenization algorithm for deep learning. J. Chem. Inf. Model. 61.4 , 1560–1569 (2021). Google Scholar\\n- Faghri, F. et al. VSE++: improving visual-semantic embeddings with hard negatives. British Machine Vision Conference (BMVA, 2018).\\n- Ho, T. K. Random decision forests. In Proceedings of 3rd international conference on document analysis and recognition , Vol. 1 (IEEE, 1995).\\n- Zhu, J. et al. Dual-view molecule pre-training. Preprint at https://arXiv.org/2106.10234 (2021).\\n- Liu, Y. et al. RoBERTa: a robustly optimized bert pretraining approach. Preprint at https://arXiv.org/1907.11692 (2019).', '- Zhu, J. et al. Dual-view molecule pre-training. Preprint at https://arXiv.org/2106.10234 (2021).\\n- Liu, Y. et al. RoBERTa: a robustly optimized bert pretraining approach. Preprint at https://arXiv.org/1907.11692 (2019).\\n- Zheni, Z. et al. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. In KV-PLM https://doi.org/10.5281/zenodo.5835754 , (2021).\\n- Giuseppe, P. et al. The vitamin D receptor agonist elocalcitol inhibits IL-8-dependent benign prostatic hyperplasia stromal cell proliferation and inflammatory response by targeting the RhoA/Rho kinase and NF-kB pathways. Prostate 69.5 , 480–493 (2009). Google Scholar\\n- Carvalho, D. et al. Marinobufagenin inhibits neutrophil migration and proinflammatory cytokines. J. Immunol. Res. 2019 , 1094520 (2019).\\n- Michelle, P. Rebeccamycin analogues as anti-cancer agents. Eur. J. Med. Chem. 38.2 , 123–140 (2003). Google Scholar\\n- Jeremić, S. R. et al. Antioxidant and free radical scavenging activity of purpurin. Monatshefte f.ür. Chem. 143.3 , 427–435 (2012). Article Google Scholar\\n- A-Reum, K. et al. Isolation and identification of phlorotannins from Ecklonia stolonifera with antioxidant and anti-inflammatory properties. J. Agric. Food Chem. 57.9 , 3483–3489 (2009). Google Scholar\\n- Jae-Hoon, C. et al. Hematein inhibits atherosclerosis by inhibition of reactive oxygen generation and NF-B-dependent inflammatory mediators in hyperlipidemic mice. J. Cardiovasc. Pharmacol. 42.2 , 287–295 (2003). Google Scholar\\n- Kleemann, A. Ullmann’s Encyclopedia of Industrial Chemistry (Wiley, 2000).\\n- Bruno, P. et al. Nanoscale probing of adsorbed species by tip-enhanced Raman spectroscopy. Phys. Rev. Lett. 92.9 , 096101 (2004). Google Scholar\\n- Maria, W., Holmgren, P. & Ahlner, J. A2 (N-benzylpiperazine) a new drug of abuse in Sweden. J. Anal. Toxicol. 28.1 , 67–70 (2004). Google Scholar\\n- Van der Maaten, L. & Hinton, G. Visualizing data using t-SNE. J. Mach. Learn. Res. 9 , 11 (2008).\\nDownload references', '## Acknowledgements\\n\\nAll authors are supported by the National Key Research and Development Program of China (No. 2020AAA0106501) and a grant from the Institute Guo Qiang, Tsinghua University (No. 2019GQB0004).\\n\\n## Author information\\n\\nAuthor notes\\n- These authors contributed equally: Zheni Zeng, Yuan Yao.\\n\\n### Authors and Affiliations\\n\\n- Department of Computer Science and Technology, Tsinghua University, Beijing, China Zheni Zeng,\\xa0Yuan Yao,\\xa0Zhiyuan Liu\\xa0&\\xa0Maosong Sun\\n- Zheni Zeng View author publications Search author on: PubMed Google Scholar\\n- Yuan Yao View author publications Search author on: PubMed Google Scholar\\n- Zhiyuan Liu View author publications Search author on: PubMed Google Scholar\\n- Maosong Sun View author publications Search author on: PubMed Google Scholar\\n\\n### Contributions\\n\\nZ.Z. and Y.Y. contributed to the conception of the study and wrote the manuscript; Z.Z. implemented the model framework and performed the experiment; Z.L. and M.S. led and provided valuable advice to the research.\\n\\n### Corresponding authors\\n\\nCorrespondence to Zhiyuan Liu or Maosong Sun .\\n\\n## Ethics declarations\\n\\n### Competing interests\\n\\nThe authors declare no competing interests.\\n\\n## Peer review\\n\\n### Peer review information\\n\\nNature Communications thanks the anonymous reviewer(s) for their contribution to the peer review of this work. Peer reviewer reports are available.\\n\\n## Additional information\\n\\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\\n\\n## Supplementary information\\n\\n### Reporting Summary\\n\\n### Peer Review File\\n\\n## Rights and permissions\\n\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/ .\\nReprints and permissions\\n\\n## About this article\\n\\n### Cite this article\\n\\nZeng, Z., Yao, Y., Liu, Z. et al. A deep-learning system bridging molecule structure and biomedical text with comprehension comparable to human professionals. Nat Commun 13 , 862 (2022). https://doi.org/10.1038/s41467-022-28494-3\\nDownload citation\\n- Received : 11 August 2021\\n- Accepted : 25 January 2022\\n- Published : 14 February 2022\\n- DOI : https://doi.org/10.1038/s41467-022-28494-3\\n\\n### Share this article\\n\\nAnyone you share the following link with will be able to read this content:\\nGet shareable link\\nSorry, a shareable link is not currently available for this article.\\nCopy to clipboard\\nProvided by the Springer Nature SharedIt content-sharing initiative', '# Prediction of chemical compounds properties using a deep learning model\\n\\n- Original Article\\n- Open access\\n- Published: 04 June 2021\\n- Volume\\xa033 ,\\xa0pages 13345–13366, ( 2021 )\\n- Cite this article\\nDownload PDF\\nYou have full access to this open access article\\nNeural Computing and Applications\\nAims and scope\\nSubmit manuscript\\nPrediction of chemical compounds properties using a deep learning model\\nDownload PDF\\n- Mykola Galushka 1 ,\\n- Chris Swain 2 ,\\n- Fiona Browne 3 ,\\n- Maurice D. Mulvenna 4 ,\\n- Raymond Bond 4 &\\n- Darren Gray 5\\nShow authors\\n- 12k Accesses\\n- 29 Citations\\n- 8 Altmetric\\n- Explore all metrics\\nThe discovery of new medications in a cost-effective manner has become the top priority for many pharmaceutical companies. Despite decades of innovation, many of their processes arguably remain relatively inefficient. One such process is the prediction of biological activity. This paper describes a new deep learning model, capable of conducting a preliminary screening of chemical compounds in-silico. The model has been constructed using a variation autoencoder to generate chemical compound fingerprints, which have been used to create a regression model to predict their LogD property and a classification model to predict binding in selected assays from the ChEMBL dataset. The conducted experiments demonstrate accurate prediction of the properties of chemical compounds only using structural definitions and also provide several opportunities to improve upon this model in the future.\\n\\n### Similar content being viewed by others\\n\\n### Deep Learning Based-Virtual Screening Using 2D Pharmacophore Fingerprint in Drug Discovery\\n\\n24 May 2022\\n\\n### A Comparison of Different Compound Representations for Drug Sensitivity Prediction\\n\\n### Prediction of pharmacological activities from chemical structures with graph convolutional neural networks\\n\\nOpen access\\n12 January 2021\\n\\n### Explore related subjects\\n\\nDiscover the latest articles and news from researchers in related subjects, suggested using machine learning.\\n- Cheminformatics\\n- Computational Chemistry\\n- Compound Screening\\n- Machine Learning\\n- Molecular Modelling\\n- Structure Prediction\\nUse our pre-submission checklist\\nAvoid common mistakes on your manuscript.\\n\\n## 1 Introduction\\n\\nDeep learning [ 24 ] has been successfully applied in a number of problem domains from natural language processing [ 39 ], medical imaging analysis [ 32 ] to finance [ 26 ]. Deep learning architectures are also successfully used for many predictive tasks in chemistry and biology domains [ 55 ]. One application of deep learning in the chemistry domain is to predict important properties of chemical compounds. It allows for the assessment of chemical compounds before committing to an expensive synthesis process [ 7 , 8 , 20 , 43 ].\\nDeep learning theory is based on deep neural networks (DNN) which consist of many layers. Each layer is comprised of a number of neurons. Higher levels of the DNN represent more complex concepts. To improve network performance, layers are often implemented using different methodologies. The overall topology of a neural network is selected based on the problem to be solved and often is tuned during an experimental phase.\\nIn this paper, the DNN learns to represent compounds by their chemical descriptors. This opens a wide range of opportunities to build sophisticated machine learning applications for predicting different properties of chemical compounds.\\nThis research study investigates how the pretrained autoencoder can be used for building classification and regression models for predicting the LogD property and target binding of chemical compounds using data obtained from different sources.\\nThe remainder of the paper is outlined as follows. Section 2 provides background information which includes a description of predicting properties of chemical compounds and other machine learning models used for their prediction. Section 3 introduces the mathematics behind the developed autoencoder together with visualizations of the learning mechanisms. Section 4 provides detail on the data sets used and experiments carried out, while Sect. 5 describes the obtained results, followed by discussion and conclusions in Sect. 6 .\\n\\n## 2 Background\\n\\n### 2.1 Predicting properties', '## 2 Background\\n\\n### 2.1 Predicting properties\\n\\nLipophilicity is possibly one of the most important physicochemical properties of a potential drug. It plays a role in solubility, absorption, membrane penetration, plasma protein binding, distribution, CNS penetration and partitioning into other tissues or organs such as the liver and has an impact on the routes of clearance. It is important in ligand recognition, not only to the target protein but also CYP450 interactions, HERG binding, and PXR mediated enzyme induction. Most drugs entering a market are designed for oral administration. The absorption of drugs can either be via passive diffusion across membranes or via carrier mediated transport. Carrier mediated transport is energy dependent and requires a specific transporter protein. In contrast passive diffusion does not require the presence of a specific carrier transporter protein and is less structure specific than carrier-mediated transport; there is a general dependence on lipophilicity for structurally diverse compounds. However, the relationship with LogD is non-linear with an optimum of LogD 1-2.\\nMeasurement of LogP can be undertaken in a variety of ways, the most common is the shake-flask method, which consists of dissolving some of the solute in question in a volume of octanol and water, shaking for a period of time, then measuring the concentration of the solute in each solvent. This can be time-consuming particularly if there is no quick spectroscopic method to measure the concentration of the molecule in the phases. A faster method of logP determination makes use of high-performance liquid chromatography.\\nHowever, the majority of known drugs contain ionizable groups, as shown in Fig. 1 , which shows the distribution of small molecule drugs with DrugBank [ 53 ] and are likely to be charged at physiological pH and LogP only correctly describes the partition coefficient of neutral (uncharged) molecules. LogD, the distribution constant is a better descriptor of the lipophilicity of a molecule. This can be determined in a similar manner to LogP but instead of using water, the aqueous phase is adjusted to a specific pH using a buffer. LogD is thus pH dependent, hence one must specify the pH at which the logD was measured. Of particular interest is the logD at pH = 7.4 (the physiological pH of blood serum).\\nDistribution of small molecule drugs with DrugBank\\nFull size image\\nUsually, it is not practical to determine the LogD of every compound made experimentally (and it may be of interest to calculate logD prior to synthesis) and so calculated results are used.\\n\\n#### 2.1.2 Binding\\n\\nThe majority of drug-like small molecules are specifically designed to bind to protein targets involved in disease related pathways. The activity of molecules in a biological assay may be captured by a variety of different measures IC50, EC50, Ki, % inhib, etc., but most are a measure of a binding event in some manner. The biological results varies considerably in quality from single point high-throughput screening (HTS) data [ 15 ] to full dose response curves. Much of these data are captured in ChEMBL, a database of bioactive drug-like small molecules and abstracted bioactivities.\\n\\n### 2.2 Models for comparison', 'The developed DNN model provides an accurate prediction of LogD and binding properties. To gauge its performance, a total of ten machine learning (ML) techniques were used. Taking into consideration a large variety of different experimental setups, model implementations and evaluation metrics, authors tried to summarize the results from a number of sources and provide a ‘single’ performance metric to compare each of the techniques. Footnote 1\\n- Logistic regression (LR) [ 41 ] is a very straightforward and popular classification model, which has a long and successful history of been used in many ML applications and statistical modelling. It uses a logistic function for weighting a linear combination of input parameters. LR models have been used in a large number of publications involving chemical data types.\\n- Kernel ridge regression (KRR) [ 57 ] uses a modified approach to find a regression function by adding a bias, which causes a drop in variance. In other words a better prediction can be achieved by considering a slightly less fit model during the training process.\\n- Another very popular and well recognized model is random forests (RF) [ 56 ]. It can be applied for both classification and regression problems and uses an ensemble of decision trees, which are trained on different subsets of the original data.\\n- Extreme gradient boosting (XGB) [ 14 ] tree is similar to RF and is an ensemble method. During each training step it constructs a new tree model, which in combination with previous models, minimizes the overall prediction error. It is a very popular approach in the ML community, and has been successfully applied to many problems, consistently providing accurate results.\\n- Multitask neural network (MNN) [ 49 ] is a special modified neural network architecture for solving simultaneously multiple problems. There are a number of different designs of these neural networks. However, they are all constructed based on the principle that some fully-connected layers are shared between different tasks. In this way training processes for solving one task can influence other tasks and vice versa.\\n- Graph convolutional (GC) [ 18 ] models are designed to utilize molecules which can be transformed into undirected graphs where atoms are represented as nodes and bonds as edges respectively. A convolution is applied to expand a feature space, by creating multiple filters representing graph substructures. The aggregation of these substructures is performed via multiple convolution layers.\\n- Message passing neural network (MPNN) [ 23 ] model is based on mathematical framework, which generalizes a number of graph-based neural network designs. It performs computation in two phases: message passing phase (constantly updates a hidden state) and read-out phase (uses the final hidden state, used for making a prediction).\\n- Directed acyclic graph (DAG) [ 37 ] is another popular approach creating classification and regression models. It is based on a chemical compounds graph structure. The previously reviewed graph-based models are using undirected graphs (where molecular bonds between atoms naturally do not have directions). However, the latest DAG design synthetically introduces an additional directional feature for the graph-based molecular representation. It identifies a central atom and creates a directional-structure of chemical compound from this central point. Generating the additional features may provide an improvement in prediction accuracy in comparison to other graph-based models.\\n- WEAVE [ 31 ] supports graph-based model that utilizes both properties of chemical compounds: nodes (for atoms) and edges (for bonds). The constructed features matrix is processed by convolution-like filters. Similar to image convolution neural network, WEAVE provides more informative representations of chemical compound structures.\\n- Influence relevance voting (IRV) [ 51 ] is not the most common approach, but it has certain important advantages against other ML models. Its prediction can be easily interpreted in a similar fashion to k-nearest neighbours (kNN). IRV tries to identify k-nearest neighbours using a neural network to compute a more complex similarity function.\\nThe majority of these models rely on two key factors: availability of large volumes of training data and knowledge about the physical structure of chemical compounds (which can be used for converting them to graphs or fingerprints). Despite large collections of stock chemical compounds, an offering by many research and commercial entities, the number of compounds in individual assays investigating a specific problem, is relatively small. It may significantly impact the performance of many ML models. Besides, if a model relies on the physical structure of chemical compounds the training process can become very computationally expensive.', 'This study investigates transfer learning using the variational autoencoder . It reduces reliance on a large volume of training data from the specific assays. Footnote 2 This model also does not require any external knowledge about the physical structure of chemical compounds. All knowledge required for making accurate predictions derive from SMILES representation of chemical compounds.', '## 3 Methodology\\n\\nThe proposed methodology uses a pretrained autoencoder to build classification and regression models for predicting properties of chemical compounds. A high-level overview of the proposed approach is presented in Fig. 2 .\\nA summary of the proposed approach where ChEMBL data are used to train the autoencoder and screening data are used to build and evaluate a model for predicting desirable properties of chemical compounds\\nFull size image\\nThe majority of the proposed workflow stays the same for predicting LogD and binding properties. It starts with selecting a collection of chemical compounds from the ChEMBLv23 database [ 11 ]. These compounds are used for training variational autoencoder . Then, a specially designed process isolates encoder layers of the variational autoencoder . These layers are combined with an additional sub-network for performing classification or regression tasks (depending on the problem being solved). The constructed regression or classification neural networks are trained using screening data derived from HTS. It is important to mention that the encoder layers remain frozen throughout this training. The obtained model is used for evaluation, and if the assessment is successful, the model can be deployed in a production environment.\\nThe rest of the methodology section is split into three parts. Section 3.1 describes a variational autoencoder . Section 3.2 focuses on classification and regression models for predicting desirable properties of chemical compounds. Section 3.3 explains a learning process using a simple and intuitive example.\\n\\n### 3.1 Variational autoencoder', 'An autoencoder is a neural network which can be used to address representational learning problems [ 30 ]. It learns to reconstruct the original input using an informational bottleneck. Such neural networks have been successfully applied in various domains such as noise filtering in audio-video content, translation, and compression. The proposed approach focuses on the autoencoder for reconstruction of chemical compounds. It takes simplified molecular-input line-entry system (SMILES) [ 52 ] and tries to reproduce these SMILES using latent space. SMILES are ASCII strings for describing the structure of chemical species. They can be imported by most molecule editors and converted into two-dimensional or three-dimensional structures of the molecules. For example Aspirin \\\\(C_{9}H_{8}O_{4}\\\\) is represented by the following SMILES: CC(=O)OC1=CC=CC=C1C(=O)O , whose 2D structure is shown in Fig. 3 .\\nAspirin 2D structure\\nFull size image\\nA generic autoencoder is trained to minimize the reconstruction error \\\\(\\\\mathcal {L}\\\\) defined by Eq. 1 :\\n$$\\\\begin{aligned} \\\\min \\\\{\\\\mathcal {L}(x, \\\\widehat{x}) + \\\\mathcal {R}\\\\}, \\\\end{aligned}$$\\nwhere x is the original input, \\\\(\\\\hat{x}\\\\) is the reconstructed output and \\\\(\\\\mathcal {R}\\\\) is a regularizer. The regularizer penalizes a model using large weights to prevent memorization and overfitting problems [ 17 ]. Ideally, a well-trained autoencoder should accurately reconstruct an input SMILES x such as already mentioned CC(=O)OC1=CC=CC=C1C(=O)O .\\nIn this paper, we are focusing on variational autoencoder [ 22 ], a special type of autoencoder , which uses a probability distribution to reconstruct the original input x . Suppose z represents hidden variables (from the latent space) used to reconstruct the original input:\\n$$\\\\begin{aligned} p(z|x)=\\\\frac{p(x|z)p(z)}{p(x)}=\\\\frac{p(z,x)}{p(x)}. \\\\end{aligned}$$\\nThe computation of the marginal distribution p ( x ) is very complex, since the following integral is intractable (in majority cases):\\n$$\\\\begin{aligned} p(x)=\\\\int p(x|z)p(z)dz. \\\\end{aligned}$$\\nThere are two main approaches available to tackle this problem: Monte Carlo [ 38 ] and variational inference (used to build variational autoencoder ) [ 27 ]. Let’s approximate p ( z | x ) with another distribution q ( z | x ), where q can be chosen as a tractable distribution (such as Gaussian) [ 29 ]. Then, it is possible to find distribution parameters when q becomes close enough to p by minimizing the Kullback–Leibler divergence [ 47 ], which measures an amount of lost information for the chosen approximation:\\n$$\\\\begin{aligned} KL(q(z|x)||p(z|x)) = -\\\\sum {q(z|x)\\\\log {\\\\frac{p(z|x)}{q(z|x)}}}. \\\\end{aligned}$$\\nBy replacing p ( z | x ) in Eq. 4 it is possible to derive to the following equation:\\n$$\\\\begin{aligned} KL(q(z|x)||p(z|x))= -\\\\sum q(z|x) \\\\log {\\\\frac{p(z,x)}{q(z|x)}} + p(x), \\\\end{aligned}$$\\nand express p ( x ) as:\\n$$\\\\begin{aligned} p(x) = KL(q(z|x)||p(z|x)) + \\\\sum q(z|x){\\\\log {\\\\frac{p(z,x)}{q(z|x)}}}, \\\\end{aligned}$$\\nwhere the second term is called a variational low bound :\\n$$\\\\begin{aligned} \\\\mathcal {L} = \\\\sum {q(z|x)log{\\\\frac{p(z,x)}{q(z|x)}}}. \\\\end{aligned}$$\\nThis allows us to rewrite Eq. 6 as:\\n$$\\\\begin{aligned} p(x) = KL(q(z|x)||p(z|x)) + \\\\mathcal {L}, \\\\end{aligned}$$\\nwhere p ( x ) in Eq. 8 can be considered as a constant, since x is given (as the original input). The rest of this equation represents a sum of two quantities, where KL-divergence needs to be minimized. The minimization of KL-divergence is effectively a maximization of the variational low bound \\\\(\\\\mathcal {L}\\\\) defined by Eq. 7 . By substituting p ( z , x ) in Eq. 7 it is possible to derive to the following equation:\\n$$\\\\begin{aligned} \\\\mathcal {L} = \\\\sum {q(z|x)\\\\log {p(x|z)}} + \\\\sum {q(z)\\\\log {\\\\frac{p(z)}{q(z|x)}}}, \\\\end{aligned}$$\\nwhere p ( x | z ) is the expectation with respect to q ( z ) and can be written as \\\\(E_{q(z)}\\\\log {p(x|z)}\\\\) . The second term \\\\(-KL(q(z|x)||p(z|x))\\\\) represents the KL-divergence. This allows us to rewrite Eq. 9 as following:\\n$$\\\\begin{aligned} \\\\mathcal {L} = E_{q(z)}\\\\log {p(x|z)} - KL(q(z|x)||p(z|x)). \\\\end{aligned}$$\\nLet’s build the variational autoencoder based on variational low bound Eq. 9 . The observed distribution q is a function mapping x to z , which should match an another distribution p . The observed distribution p is a function mapping z to \\\\(\\\\hat{x}\\\\) , where p can be chosen. Both q and p are implemented as neural networks and for the further references are called encoder and decoder accordingly. A visualization of the variational autoencoder model is shown in Fig. 4 .\\nA schema of variational autoencoder model\\nFull size image\\nLet’s select p to be the Gaussian distribution. This requires us to make the distribution q (in the latent layer) also similar to Gaussian. The cost function can then be expressed as:\\n$$\\\\begin{aligned} \\\\min {|x-\\\\hat{x}|^2-KL(q(z|x)||N(\\\\mu ,\\\\sigma ))}, \\\\end{aligned}$$', '$$\\\\begin{aligned} \\\\min {|x-\\\\hat{x}|^2-KL(q(z|x)||N(\\\\mu ,\\\\sigma ))}, \\\\end{aligned}$$\\nwhere N is a normal distribution defined by two parameters \\\\(\\\\mu \\\\) -mean and \\\\(\\\\sigma \\\\) -variance. \\\\(|x-\\\\hat{x}|^2\\\\) in 11 has been derived from the definition of the reconstruction error for the Gaussian distribution \\\\(p(x|\\\\hat{x})=e^{-|x-\\\\hat{x}|^2}\\\\) .\\nAs shown in Fig. 5 , the encoder learns to represent the original input x as a set of attributes z in the latent space, where each attribute is defined as the probability distribution (with parameters \\\\(\\\\mu \\\\) and \\\\(\\\\sigma \\\\) ). The decoder learns to reconstruct \\\\(\\\\hat{x}\\\\) close to the original input x using a set of attributes z from the latent space.\\nThe mechanism of the input reconstruction using latent space\\nFull size image\\nA set of attributes z in the latent space represents chemical compounds fingerprints and can be used for building classification and regression models. A typical approach for building classification and regression models is to join together trained encoder and problem-dependent prediction layers as shown in Fig. 6 .\\nA schema of built classification and regression models based on the variational autoencoder\\nFull size image\\nThe training process in such architecture learns some function f ( y | z ) which predicts y (category value for a classification problem or real value for a regression problem) using chemical compounds fingerprints z generated by encoder . In the classical approach the encoder neural network q ( z | x ) only generates z and does not take part in the training process (so it weights remain frozen thorough out all training cycle). Footnote 3', '### 3.2 Architecture\\n\\nThe variational autoencoder was implemented using a Convolution Neural Network [ 6 ] in combination with a few layers for supporting the variational training process. Its neural network topology is presented in Fig. 7 .\\nA neural network topology of variational autoencoder\\nFull size image\\nIt consists of two joined neural networks: encoder and decoder . The encoder neural network consists of nine layers. The Input layer takes SMILES transformed to the one-hot 150x78 matrix representation (each row represents a SMILES character and column its encoding). A visualization of SMILES transformation is shown in Fig. 8 .\\nSMILES one-hot encoding\\nFull size image\\nThis figure demonstrates a process of one-hot encoding for the Aspirin SMILES, introduced earlier. If the input SMILES is less than 150 characters, the original sequence is padded with empty spaces on the right. When the input sequence is adjusted, the transformation process creates a zero-matrix with 150 rows (equals to the maximum number of characters in the input sequence) and 78 column (equals to SMILES vocabulary size). A transformation loop selects each character in the padded sequence and use a dictionary to identify the character code. A position of the selected character and its code are used to set the according element to 1 in the one-hot matrix. Footnote 4\\nThe input layer is followed by three 1D-convolution layers ( Conv1D-1 , Conv1D-2 and Conv1D-3 ) with 512, 256, 128 filters and 7, 5, 3 kernel sizes accordingly. These convolution layers perform a very similar role to 2D-convolution layers in image processing. They identify specific patterns of elements (atom and bonds) in chemical compounds and aggregate them in bigger substructures at each consequent layer. A useful insight of how these layers work is presented in the discussion Sect. 6 .\\nThe Flatten layer vectorizes convolution weights, so they can be processed by the following Dense-1 layer. This layer has 1024 neurons, which is exactly the same size as an output latent vector z . The size of this layer has been defined via a hyper-parameters tuning procedure. Dense-2 and Dense-3 layers implement variational learning, which computes \\\\(\\\\mu \\\\) and \\\\(\\\\sigma \\\\) accordingly. The final Lambda layer combines \\\\(\\\\mu \\\\) and \\\\(\\\\sigma \\\\) into a single latent vector (consisting of 1024 real values). Effectively this latent vector represents a chemical compound fingerprint.\\nThe decoder consists of five layers. The input layer receives the latent vector from the encoder and passes it via two dense layers: Dense-4 and Dense-5 . The Dense-5 scale-up original dimensionality from 1024 to 11700. This step is needed to transform a 1D into a 2D vector which is performed by the Reshape layer. This layer passes a 2D vector straight to the output. The final output of all these layers is 150x78 matrix, which can be reversed to the SMILES sequence.\\nA neural network topology of classification and regression models constructed based on pretrained encoder is shown in Fig. 9 .\\nA neural network topology of classification and regression models\\nFull size image\\nThe encoder in classification and regression models has exactly the same topology structure as already described in variational autoencoder . However, the attached layers are designed to perform classification or regression tasks. The Dense-1 layer receives chemical compounds fingerprints and passes it to the next layer Dense-2 . There is no difference in the current neural network topology whether it’s applied for a classification or regression problem. The only difference is in an inactivation for the Dense-2 layer. Footnote 5 In case of the regression problem the activation function as relu and in case of the classification problem the activation function as sigmoid .\\nDue to the high level of complexity defined above methodology and architecture, it is helpful to review a simple example, which illustrates a prediction process.\\n\\n### 3.3 Prediction example', '### 3.3 Prediction example\\n\\nAn example of neural network classifier built based on an encoder is shown in Fig. 10 .\\nClassification example using the latent space\\nFull size image\\nThis is a high level of visualization intends to demonstrate some key concepts described above. Let us assume we have three types of compounds: red, green and blue. The red and green compounds have a very similar rectangular shape (despite the green compound has slightly rounded edges). The blue compound has the triangle shape. A constructed neural network should identify two classes: rectangle or triangle for the input compound.\\nAccording to the variational autoencoder methodology the trained encoder generates a latent vector representation for each input compound. This latent vector is a ‘compact’ representation Footnote 6 of the original input and in chemistry domain can be also refereed as a chemical compound fingerprint. It is likely that similar inputs will have a similar latent vector representation. It also means that similar compounds should be closely located in the latent space. The latent space is an abstract concept, which can be very useful to visualize a distributing of encoded compounds. As it can be seen from Fig. 10 red and green crosses represent location of ‘rectangular’ instances in close approximation from each other. The blue cross represents a ‘triangular’ sample accordingly. The dashed line represents areas of distribution of compounds with similar shapes in the latent space.\\nMLP neural network can provide an efficient architecture to learn a distribution of compounds in the latent space. Since the latent vector has much smaller size in comparison to the original input, MLP does not require a complex topology. One or two hidden layers can be sufficient to handle prediction of properties of chemical compounds for the majority of cases.\\nThe next section describes a series of experiments for evaluating the proposed solutions.\\n\\n## 4 Data and experiment design\\n\\nIn order to evaluate the proposed methodology three studies are presented in this paper:\\n- An evaluation of the variational autoencoder for reconstruction of SMILES Sect. 4.1 .\\n- An evaluation of regression model for predicting LogD properties of chemical compounds Sect. 4.2 .\\n- An evaluation of classification model for drug-target predictions Sect. 4.3 .\\n\\n### 4.1 Experimental data and setup for SMILES reconstruction problem', '### 4.1 Experimental data and setup for SMILES reconstruction problem\\n\\nChEMBL is a chemical database of bio-active molecules with drug-like properties [ 16 ]. It is maintained by the European Bioinformatics Institute (EBI) of the European Molecular Biology Laboratory (EMBL), located at the Wellcome Trust Genome Campus in Hinxton, UK. ChEMBL data are widely used by pharmaceutical companies and research organizations around the World for creating screening libraries in drug discovery. ChEMBLv23 (version 23) has been selected for the current study. It includes approximately 1.7M chemical compounds.\\nThe initial study [ 21 ] already showed an accurate SMILES reconstruction using a variational autoencoder neural network. This work is focusing on an optimization of a training process. To train a neural network based on all chemical compounds containing in ChEMBL already requires a powerful architecture. However, to process collections such as ZINC [ 28 ] or Enamine [ 50 ] with hundreds of million chemical compounds requires a much more sophisticated approach.\\nThis experiment tries to define, what is an optimal size of a training data set for an accurate reconstruction of SMILES. This will help to scale down training data set and to preserve reconstruction accuracy at the same time. A design of proposed experiments is shown in Fig. 11 .\\nDesign of experiments for autoencoder model\\nFull size image\\nAll data taking part in the experiment were normalized and filtered using MolVS Open Source software [ 4 ]. SMILES exceeding 150 characters were removed. This had no significant impact on overall quality of experimental results, since only a very small percentage of these compounds were discarded. After filtering the data set comprises 1688073 samples. Footnote 7 This data set had been randomly split into training and evaluation partitions in proportions of 75% and 25% of samples respectively. The evaluation data (422,019 samples) remained unchanged throughout all experiments. It helped to score all produced models against the same benchmark. The size of training data varied from 10% to 100% of the original size (1,266,054 samples) depending on an experiment configuration. Generators were developed to feed data to a training model during a fitting processes. These generators streamed data directly from files using fixed size batches (equals to 1024), preventing any memory overflow.\\nTen groups of experiments were carried out. Five tests were performed in each group, except for the last one. Footnote 8 Each test randomly selected a certain percentage of samples from the internal training partition. Then, these samples were divided into fitting and validation subsets in proportions of 75% to 25% respectively. The validation subset was used in each training epoch to assess model performance. This assessment was necessary to control learning rate, checkpoints and earlier stopping mechanisms. An approximate number of chemical compounds selected for each test is shown in Table 1 .\\nTable 1 ChEMBL data set split\\nFull size table\\nThe best setup (experiment 5) shown in Table 2 was selected to build a production autoencoder model (which was used in the further tests for building classification and regression models). A explanation for the selected data split (defined by the experiment 5) is provided in Sect. 5.1 and discussed in Sect. 6 .\\nTable 2 ChEMBL data set production split\\nFull size table\\nAutoencoder model efficacy was evaluated by reconstruction accuracy, Hamming [ 44 ] and Levenshtein [ 42 ] editing distances.\\n\\n### 4.2 Experimental data and setup for regression problem', '### 4.2 Experimental data and setup for regression problem\\n\\nLogD values for training and evaluation of a regression model were obtained from the ChEMBL database. To prevent influence outliers on the overall model only values in the interval between \\\\(LogD\\\\in [-20, +20]\\\\) were considered. Chemical compounds with normalization issues were also excluded during the pre-processing step. The total number of chemical compounds taking part in testing was 1669058.\\nA data set containing these values was obtained from ChEMBL using the following steps:\\n- 1. Data fields smi and val were retrieved from ChEMBL using the SQL statement, where ’smi’ is a chemical compound SMILES and ’val’ a LogD value.\\n- 2. MolVS Open Source software [ 4 ] was used to normalize each SMILES.\\n- 3. Records with \\\\(length(\\\\textit{smi})>150\\\\) were removed;\\n- 4. Records with LogD values outside the specified interval \\\\(\\\\textit{val} \\\\in [-20, +20]\\\\) were removed;\\n- 5. All LogD values were normalized between 0 and 1.\\n10-fold cross-validation was conducted to assess the model performance. The design is presented in Fig. 12 .\\nDesign of experiments for predicting LogD using a regression model and drug-target binding using a classification model\\nFull size image\\nThe data selected for cross-validation was split into 10 folds where nine folds (90% of data) were taken for training and one fold (10% of data) for evaluation accordingly. The data set allocated to training was also randomly split into fitting/validation partitions in the following proportion 75%/25%. The validation partition was used at each training epoch for assessing model quality. R2-score [ 10 ] metrics were recorded for each fold, and later generalized into the final result.\\nAn additional 10-cross validation test was introduced to evaluate the regression model based on the experimental LogD data. Data were downloaded from latest ChEMBL version (on 30 Sept 2018) and pre-processed by industry experts in drug-discovery.\\n- The data was curated to remove results obtained with solvents other than octanol/aqueous buffer. Footnote 9\\n- Results derived from HPLC retention times were also removed. Footnote 10\\n- Results obtained from experiments conducted at pH other than pH7.4 were also removed (both high and low pH. Footnote 11\\n- Duplicates were removed by using of InChiKeys. Footnote 12\\nAfter cleaning and pre-processing this data set consisted of 12413 samples which LogD property defined in the interval \\\\(-12.0, +12.0\\\\) .\\nThe second data set ‘Lipophilicity’ was obtained from the ML resource described in [ 55 ]. It consists of 4200 chemical compounds which LogD property is defined in the following interval \\\\([-1.5, +4.5]\\\\) . This data set was used to gauge the performance of developed system against other ML models.\\n\\n### 4.3 Experimental data and setup for classification problem', \"### 4.3 Experimental data and setup for classification problem\\n\\nChEMBL Footnote 13 contains approximately 13.5M bio-activity measurements, where \\xa01.1M assays are assigned to approximately 11K targets. The majority of available bio-activity data are highly unbalanced. More than 50% of assays have just a single measurement while others contain tens of thousands. On the other hand, a lot of targets belong only to a single assay, while others to hundreds. A large proportion of these data contain duplicate records. Such heterogeneity of data prevents clear identification, which measurements can be considered as active or inactive accordingly. A special protocol proposed in [ 40 ] helps to generate benchmark data sets for binary classification. It has the following six steps:\\n- 1. Data fields smi , typ , unt , rel were retrieved from ChEMBL using the SQL statement, where ’ smi ’ field represents a SMILES, ’ typ ’ a type of measurement, ’ val ’ a measurement value, ’ com ’ a measurement comment, ’ unt ’ a measurement unit and ’ rel ’ a measurement relation. These fields abbreviations are used throughout this study for referencing. All identified assays are belonged to the ’B’-type. These data are measures of compound binding to a molecular target, e.g. Ki, IC50, Kd.;\\n- 2. MolVS Open Source software [ 4 ] was used to normalize each of the SMILES.\\n- 3. Records with \\\\(length(\\\\textit{smi})>150\\\\) were removed;\\n- 4. A measurement was considered as active if \\\\(\\\\textit{com} \\\\in \\\\mathbb {A}\\\\) . A measurement was considered as inactive if \\\\(\\\\textit{com} \\\\in \\\\mathbb {I}\\\\) . Sets ( \\\\(\\\\mathbb {A}\\\\) and \\\\(\\\\mathbb {I}\\\\) ) of strings in comment-field are defined below. If a record is identified as inactive all further steps are discarded. \\\\(\\\\mathbb {A}\\\\) = (’active’, ’note: corresponding ic50 reported as active’) \\\\(\\\\mathbb {I}\\\\) =(’inconclusive’, ’not active’, ’inactive’, ’not active (inhibition < 50% @ 10 um and thus dose-response curve)’)\\n- 5. Removed all records passed the previous step where \\\\(\\\\textit{val} =\\\\emptyset \\\\mid \\\\textit{unt} \\\\ne '\\\\text {nM}' \\\\mid \\\\textit{rel} \\\\notin \\\\{'>', '\\\\ge ', '<', \\\\le ', '=', '\\\\sim '\\\\}\\\\) .\\n- 6. Assigned labels to each record according to the defined thresholds presented in Sect. 12 : $$\\\\begin{aligned} \\\\text {label} = \\\\left\\\\{ \\\\begin{array}{lr} 1 : val\\\\ge 5.5\\\\\\\\ 0 : val<5.5 \\\\end{array} \\\\right. \\\\end{aligned}$$ (12)\\n- 7. All records with duplicates and contradictory measurements obtained during the previous steps were discarded.\\n56 assays were identified for this study. Each assay reflects in vitro measurements obtained during HTS. A break down between active or inactive chemical compounds together with associated target are presented in Tables 3 and 4 .\\nTable 3 Classification ChEMBL data set statistics (Part I)\\nFull size table\\nTable 4 Classification ChEMBL data set statistics (Part II)\\nFull size table\\nThree cross-validation experiments were carried out for each data set. A design of these experiment is shown in Fig. 12 . This folds number was chosen due to the relatively small assay sizes. As it can be seen from Table 5 , a large proportion of assays have approximately 800 compounds. Three folds provide a fair representation of active or inactive chemical compounds across training, validation and evaluations partitions.\\nThe Maximum Unbiased Validation (MUV) [ 48 ] is another benchmark data set selected from PubChem BioAssay by applying a refined nearest neighbour analysis. The MUV data set contains 17 challenging tasks for around 90 thousand compounds and is specifically designed for validation of virtual screening techniques. The detail breakdown between active and inactive compounds is shown in Table 5 .\\nTable 5 Classification MUV data set statistics\\nFull size table\\nFor comparing prediction models the Receiver Operating Characteristics - Area Under The Curve (ROC-AUC) [ 12 ] was used. This metric is widely excepted by a ML community to assess performance of classification models.\\nAccording to the experimental setup defined in Sect. 4 , results are presented in three sub-sections. The first Sect. 5.1 shows results for variational autoencoder , the second Sect. 5.2 for prediction of LogD (regression problem) and the final Sect. 5.3 for prediction of compounds-targets binding (binary classification problem). The main rationale behind these experiments is to validate the versatility of latent vector based fingerprint, in other words, to prove that it works well regardless of the selected problem (classifier or regression).\\n\\n### 5.1 SMILES reconstruction\", '### 5.1 SMILES reconstruction\\n\\nForty-six tests (9x5+1 for more details see 4.1 ) were conducted to assess the accuracy of SMILES reconstruction on different portion of training data. The results obtained are detailed in Table 6 .\\nTable 6 SMILES reconstruction on different portion of training data\\nFull size table\\nChanges in accuracy and editing distance for different size of training data sets are presented in Figs. 13 and 14 . As it can be seen from Fig. 13 , the reconstruction accuracy increases from 0.247 \\\\(\\\\pm \\\\) 0.027 for 10% of randomly selected samples to 0.877 \\\\(\\\\pm \\\\) 0.009 for 50% of samples accordingly. From 60% onwards accuracy stays around 0.8 on average with slight fluctuations. This is an expected result. However slight variations in accuracy starting from 60% of samples needs to be addressed. It is a difficult task to identify the exact reason of what is influencing these changes in accuracy. One of the possible reasons for such behaviour is early signs of overfitting. Obviously there is no evidence of this phenomenon spotted during training. However, it is possible that with increasing of samples number a model starts to memorize the input. This can be addressed by a better sampling algorithm. For example, Butina clustering [ 13 ] can be a useful technique to design a sampling algorithm.\\nIn addition to accuracy, model performance was measured using Hamming and Levenshtein distances. Both belong to a family of editing distances and give a different perspective on the results obtained. They show a similar trend to accuracy. The Hamming distance decreased from 4.374 \\\\(\\\\pm \\\\) 0.817 to 0.663 \\\\(\\\\pm \\\\) 0.071 for 10% and 50% of sample cases respectively. The Levenshtein distance decreased from 4.299 \\\\(\\\\pm \\\\) 0.127 to 0.648 \\\\(\\\\pm \\\\) 0.031 for the same percentage of samples. Both show slight fluctuation in editing distance from 60% onwards.\\nSMILES reconstruction accuracy for different percentage of training sample\\nFull size image\\nSMILES reconstruction editing distances for different percentages of training samples\\nFull size image\\nAccording to the carried out experiment the best result (accuracy 0.877 \\\\(\\\\pm \\\\) 0.009 ) was obtained for 50% of randomly selected samples. This configuration was selected for building a production variational autoencoder , which can be later used for training classification and regression models with SMILES input. Training was conducted using 40 epochs, and produced a model with accuracy 0.872 . The editing distances for the production model were 0.682 and 0.677 for Hamming and Levenshtein respectively.\\n\\n### 5.2 LogD prediction', 'Two cross validation experiments described in Sect. 4.2 were carried out on ChEMBL and Lipophilicity data sets. Results of these experiments are presented in Table 7 .\\nTable 7 Cross-validation results prediction of the LogD property for ChEMBL and Lipophilicity data sets\\nFull size table\\nAs it can be seen from the obtained results the best performance is achieved for the ChEMBL data set, with an average coefficient of determination of 0.907 \\\\(\\\\pm \\\\) 0.008 . A scatter plot with an alignment of true and predicted values is shown in Fig. 15 . The training process was carried out with LogD values normalized in the interval \\\\([-20,+20]\\\\) Footnote 14 and all training cycles took 20 epochs.\\nThe high prediction accuracy is a very much expected result, since the majority of LogD data points in ChEMBL are computed using the ACD/Labs software [ 1 ]. In this scenario the regression model is simply learning to predict an outcome of another computational algorithm (such as ACD/Labs). It makes a learning task much more straightforward. This assumption is vindicated by results obtained in other studies. For example experiments with ChEMBL data set described in [ 1 ] also show a high accuracy using a SVM [ 19 ] model.\\nScatter plot of predicted LogD values on ChEMBL data\\nFull size image\\nMuch more interesting results are obtained for the Lipophilicity data set. The average R2 score equals to 0.542 \\\\(\\\\pm \\\\) 0.021 , which is noticeably less in comparison to ChEMBL data. The training process was carried out with the LogD normalized interval \\\\([-1.5,+4.5]\\\\) and all training cycles took 30 epochs. A longer training cycle reflects the complexity of building a predictive model on real experimental data. A scatter plot with an alignment of true and predicted values for the Lipophilicity data set is shown in Fig. 16 .\\nScatter plot of predicted LogD values on Lipophilicity data\\nFull size image\\nEight ML models were investigated on this data. A comparison chart for all these models is presented in Fig. 17 . The highest score 0.697 is obtained for MPNN, which uses a generalized model [ 23 ]. It is very suitable for processing graph structured data, which makes it efficient in predicting properties of chemical compounds (since chemical compounds can be easily represented as an undirected graph). MPNN is closely followed by GC model [ 31 ], with R2 score equals to 0.662 . GC utilities principles of circular fingerprints described in [ 35 ] representing molecular structures by atom neighbourhoods. Similar to GC, Weave [ 31 ] ( 0.636 ) is another graph-based model which processes chemical compounds as a undirected graph using a convolution approach. In contrast to the previous three models, XGB [ 14 ] provides a different approach for making predictions. It is an ensemble approach which combines predictions of individual decision trees. XGB coefficient of determination for Lipophilicity equals to 0.577 and is closely followed by the model developed with in this study ( 0.542 \\\\(\\\\pm \\\\) 0.021 ). Directed Acyclic Graph (DAG), Kernel Ridge Regression (KRR) and Random Forests (RF) are the lowest performing in this evaluation with R2 scores 0.507 , 0.496 and 0.483 respectively.\\nSA comparison chart for ML models of predicting Lipophilicity\\nFull size image\\nThe key challenge of developing ML models for predicting the properties of chemical compounds is to encode molecules into fixed-length strings or vectors representation [ 54 ]. Despite SMILES providing unique representations of molecules, the majority of ML models are also relaid on additional information such as electronic or topological, profiles of chemical compounds. To derive these features, the models, we are gauging against, applied different factorizations: Extended Connectivity Fingerprints (ECFP), Coulomb matrix, Grid features, etc. These approaches are computationally expensive, and there will always be a trade-off between speed, accuracy and expense. Because of this, it is not surprising that some models provide better performance compared to our approach. As has been already mentioned, our approach is purely data-driven and inspired by - et al.[ 25 ]. It should provide alternatives to replace crafted featurization methods with the learning ability of DNN. In the future development, we are planning to improve the encoding method and directly compare it to existing featurization approaches.\\nOpen-source research in AI always provides a solid benchmark for assessing in-house models. However, it would be interesting to compare the developed model against a commercial application. The following results show comparison of predictions made by ChemAxon software [ 5 ] against the developed regression model. This work was carried out in collaboration with Cambridge MedChem Consulting [ 2 ]. Data were selected and pre-processed by industry experts in drug discovery. All undertaking steps for preparing this experiment are described in Sect. 4.2 .', '10-folds cross-validations were performed on selected data set to compare models. Footnote 15 The obtained results are shown in Table 8 . Results for the developed regression are shown in the ‘ARM’ column and for the commercial software in the ‘ChemAxon’ column accordingly.\\nTable 8 Cross-validation for predicting LogD using developed regression model and ChemAxon software\\nFull size table\\nAn average R2 score for our developed regression model equals to 0.695 \\\\(\\\\pm \\\\) 0.013 , which is nearly twice that obtained by ChemAxon with R2 of 0.338 \\\\(\\\\pm \\\\) 0.034 . It is hard to comment on the underlining ChemAxon algorithm for making prediction without available source code. However, from the description presented on the company website [ 5 ] it possible to assume, that it is a deterministic algorithm which approximates a chemical compound structure into a specific property value. Two scatter plots represented in Figs. 18 and 19 show an alignment of true and predicted values based on our developed regression model and ChemAxon respectively Footnote 16\\nScatter plot of predicted LogD values on in-vivo data using our model\\nFull size image\\nScatter plot of predicted LogD values on in-vivo data using ChemAxon model\\nFull size image\\nThe experiments clearly demonstrate the validity of our proposed model to predict the LogD property of chemical compounds. However a large proportion of tasks required simple classification, for example whether a compound binds to the specified target. An evaluation of the classification model constructed based on variational autoencoder is presented in the next Sect. 5.3 .', '### 5.3 Binding prediction\\n\\nTwo cross validation experiments described in Sect. 4.3 were carried out on 56 ChEMBL data sets. Results of these experiments are presented in Table 9 .\\nTable 9 ChEMBL binding-assays with testing results\\nFull size table\\nConsidering the large volume of obtained results, they were split into five groups based on ROC-AUC metric (see Fig. 20 ). The first group combines 10.7% of assays with least accurate prediction (which ROC-AUC is located in (0.0,\\xa00.6) interval). It is very closely followed by the next group of 12.5% assays, which showed result in [0.6,\\xa00.7) interval. A slightly bigger group of 17.9% of assays demonstrated ROC-AUC in interval [0.7,\\xa00.8). In many cases such accuracy of in-silico prediction on HTS data can be already consider as a very good result. However the largest group, which combines 50% of assays showed ROC-AUC scores in interval [0.8,\\xa00.9). Such accuracy can have a significant impact on planning and execution of HTS experiments, majority of assays filtered by in-silico approach. The remaining group combines 8.9% assays with highest scores located in interval [0.9,\\xa01.0).\\nBinding prediction results split into five groups based on the ROC-AUC metric\\nFull size image\\nDespite competent results obtained on vast majority of tested assays, the authors carried out additional investigation to rank the developed classifier against other ML algorithms. Similar to the regression problem, the main objective here is not a direct comparison of different ML algorithms, since it requires different experimental setup. This study projects prediction accuracy observed for the developed model on results already described by - et al. [ 55 ]. The ROC-AUC characteristics for 17 binding-assays are presented in Table 10 .\\nTable 10 MUV binding-assays with testing results\\nFull size table\\nAn average ROC-AUC of a 10-folds cross-validation experiment was recorded for each assay. The summary field represents an average ROC-AUC across all 17 experiments. It was used for ranking the developed classifier against 6 ML algorithms. A visual representation of this ranking is shown in Fig. 21 .\\nRanking the developed classifier against six ML models using binding prediction\\nFull size image\\nSimilar to the regression problem, GC - a graph based model, scored 0.775 the best result for binding classification. Footnote 17 It closely followed by BYPASS, representing a multitask neural network and LOGREG representing logistic regression model, scoring 0.764 and 0.749 respectively. XBG model showed 0.720 ROC-AUC score which was closely followed by ARM, with 0.696 . Influence Relevance Voting (IRV) systems and Random Forests (RF) are the poorest performing, with ROC-AUC scores of 0.693 and 0.693 respectively. Despite variations in accuracy all techniques showed a very consistent performance. Considering the hugely unbalanced data sets the measured metric can be significantly shifted by producing one extra true positive prediction.\\nWith such variety of different models capable of producing equally accurate results for regression and classification problems, why another approach? This question is addressed in the below.\\n\\n## 6 Discussion and conclusion', 'During the last few years deep neural networks de facto have become an industry standard for creating sophisticated AI models. A significant amount of effort have been devoted to improve classical ML algorithms. Often XBG and RF produce even better results than DNNs. Such a variety of approaches makes it a very difficult task to choose the right technique. This has also made a huge impact on scientific publications. Researchers are forced to show rigorous testing, with comparison of every proposed technique against the recognized leaders. It expected that the developed technique must outperform others, which in practice leads to compromising with experiments design and cherry-picking phenomenon.\\nIt can be seen from our comparison of published models, the majority show very similar performance. However, it is also important to score each model in term of practical application. For example a model can deliver impressive accuracy, but when it needs to be deployed in a production environment, scalability and efficiency diminish all advantages gained in perfecting the quality of predictions.\\nThis work has been inspired by a research effort of using variational autoencoder to generate chemical compounds fingerprints, using these fingerprints for predicting specific properties of chemical compounds. Considering the high complexity of chemical compounds to train quality variational autoencoder requires a large data set of SMILES. A typical size of HTS assays consists of several hundred, maybe thousands of chemical compounds. Such volume of data samples does not deliver a sufficient variety of chemical compounds structures, which makes it impractical to train a variational autoencoder based on assay data.\\nA decision was made to use ChEMBL data to obtain a large representation of chemical compounds structures. It worked very well, with the developed variational autoencoder model reconstructing nearly 90% of SMILES using 1024 latent vector. Taking into the account Hamming and Levenshtein editing distances the final model in average has one misplaced or incorrect atom or bond. Obviously such error is unforgivable in chemistry, but the primary objective of variational autoencoder is to produce latent space where similar structures are crumbled together. Let’s demonstrate this on simple example.\\nAssume that the selected target chemical compound is NC1=NNC(=C1)C1=CC(F)=CC(F)=C1 , which structure is shown Fig. 22 .\\n2D Structure of chemical compound NC1=NNC(=C1)C1=CC(F)=CC(F)=C1\\nFull size image\\nThe search space for similar chemical compounds was reduced to 10,000 samples (out of 674,040 initially allocated for evaluation, for more detail see Sect. 4.1 ) to make computation more efficient. K-nearest neighbour [ 36 ] identified five closest chemical compounds defined in Table 11 , and illustrated in Fig. 23 .\\nTable 11 Definitions of five closest chemical compounds retrieved for the specified\\nNC1=NNC(=C1)C1=CC(F)=CC(F)=C1\\nFull size table\\nStructures of five closest chemical compounds retrieved for the specified NC1=NNC(=C1)C1=CC(F)=CC(F)=C1 target (the structure number corresponds to the compound number in Table 11\\nFull size image\\nThe last column in Table 11 represents Tanimoto similarity score [ 9 ]. It is widely used in the chemistry domain to assess similarity between two chemical compounds. Two compounds can be considered similar if Tanimoto score is greater than 0.85 (for Daylight fingerprints). As it can be seen from the obtained results in Table 11 three compounds retrieved using latent space are also similar according to Tanimoto metric, where the other two results are closely followed.\\nThis example demonstrates that latent vector based fingerprints can be used to define similarity between two chemical compounds. It also clearly shows that selected chemical compounds are closely located in latent space.\\nThe closest approximation of similar compounds in the latent space provides potential capability for the developed model to be applied to generating new chemical compounds and forecasting the desired properties. Such ML models become a hot topic in pharmaceutical domain, which can be witnessed by increasing the number of high-quality research publications in this space [ 25 , 46 ]. In this paper, the main focus was on the transfer learning, to use the trained encoder as a base for classification or regression networks which can predict properties of chemical compounds. However, the decoder can be potentially used for generating novel chemical compounds. By introducing a small modification into the latent representation of a target chemical compound, it is possible to generate a novel structure. Despite this simple idea the implementation of such a model is very complex and outside the scope of this publication.', 'The trained variational autoencoder forms a solid base for creating different classification and regression models. An interesting pattern was observed during a training process. The majority of trained models converge to a stale state (where no longer improvement observed) during 2-3 epochs. However when the same topology of neural network was trained without pretrained variational autoencoder , the training process continue up to 100 epochs. Longer training is not a problem for relatively small data set, where full learning cycle can be complied in the matter of hours. However in case of such collection volume as ChEMBL, training may go on for days. Also, the experiments did not reveal any degradation in accuracy with shortening the training cycle.\\nThe question is why are classification and regression models built based on variational autoencoder so efficient in the training process? To answer this question, let us come back to the methodology described in Sect. 3 . Constructed classification and regression models consists from two parts. The first part is an encoder isolated from a trained variational autoencoder . The second part is MLP, which performs actual predictions. Effectively, the MLP is trained to make predictions based on chemical compound fingerprints. Since the ‘hard work’ has been already done by variational autoencoder , only a few cycles are required to learn differentiation rules (to solve classification or regression problem).\\nA further observation which was called ‘latent space drift’ was also noted. A number of publications using a similar approach do not clearly reveal their mechanisms of using an encoder . It can be used with frozen and unfrozen layers. Using an encoder with frozen layers makes a lot of sense. If it is already trained to encode SMILES, then the attached layers can only be trained to utilize the obtained chemical compounds fingerprints. However, a preliminary study showed that if an encoder is left unfrozen, the training result is generally better. An initial investigation found that fingerprint points in latent space become adjusted according to the target (predicting) property. This process is explained in Fig. 24 .\\nAn example of the latent space drift. The diagrams show a distributions of training points of binary classes before and after the drift\\nFull size image\\nAll this internal analysis of neural network behaviour becomes possible due the specialized AUROMIND software. It provides a set of tools for ‘debugging’ a training process. The authors plan to describe some of the core principles behind developed tools in upcoming publications.\\n- This assessment takes into the account, all aspects of carried out experiments and selects results from those, which correlate with tests performed in the current study.\\n- The variational auto-encoder can be trained to generate chemical compounds fingerprints using a large database of compounds such as ChEMBL. Then, this model can use a small portion of training data (from assays), to predict specific chemical compounds properties of binding characteristics.\\n- The conducted experiments showed that relaxing encoder weights may help to improve prediction accuracy and will be discussed in the following sections.\\n- The reverse process is applied to reconstruct the original sequence from the one-hot matrix representation.\\n- In this publication we are referring to the binary-classification task. This topology requires a slight modification for multi-class classification problems.\\n- A latent vector is a point representing the original input in the latent space. The latent space is a collection of vectors, generated by a complex compression function ( encoder ). It is extremely difficult (and more likely impossible) to demonstrate this on a practical example. This figure is an attempt to demonstrate the variational autoencoder concept in relation to chemical compounds. By introducing this figure we are trying to help our reader to understand how variational autoencoder works within the scope of this paper. If the reader wants to take a deeper dive into variational autoencoder theory, we recommend the following publication [ 33 ].\\n- A number chemical compounds used in the referenced study [ 21 ] is different, due to an outdated filtering algorithm.\\n- The last group handled 100% of data, no random sampling was needed.\\n- The vast majority of the historical data available was determined using octanol/aqueous buffer as the liquid phases, there is a body of data using cyclohexane/aqueous buffer but this was excluded to reduce the number of variables involved in the experiments.', '- Traditionally partition coefficients are determined experimentally using the Shake Flask Method [ 45 ]. Since this test can be time-consuming, alternative methods have been explored; HPLC retention times [ 34 ], however, applying a regression equation derived from one chemical class to a second one may not be reliable. For internal consistency only data from experiments using the shake flask method were used.\\n- The majority of the experiments were carried out at physiological pH (7.4) there is small amount of data from different pH but since this will affect the extent of ionization of the molecules alternative pH data was removed.\\n- Some molecules had been tested many times, to give equal weight to every molecule a single record for each molecule was required. InChiKeys were generated for each molecule [ 3 ] and used to compare records, duplicate structures were removed.\\n- The deviation of predicting LogD values base on the interval \\\\([-20,+20]\\\\) may be seen to be high. However, predicting LogD is a particularly challenging problem since we have to predict lipophilicity (LogP) and the ionization (pKa) of a molecule. The distribution can range considerably but the hope is that by identifying outliers we can identify those structural classes that provide unique challenges to the algorithm. In some cases this will be because the structural class is not well exemplified within the training set, in other cases, it may be that very minor structural changes have very profound effects and the more coarse-grained models don’t give sufficient accuracy.\\n- ChemAxon software was used only to generate predictions for the evaluation fold. Since it is not required any training, 9-folds allocated for this purposes were discarded at each iteration.\\n- Scatter plots based on one of the folds obtained during cross-validation.\\n- It ranked as a second for regression problem, but a gap with the leader (MPNN) is very close.', '## Abbreviations', 'American Standard Code for Information Interchange\\nChemogenomic European Molecular Biology Laboratory\\nDirected Acyclic Graph\\nEuropean Bioinformatics Institute\\nHalf maximal effective concentration\\nExtended-Connectivity Fingerprints\\nEuropean Molecular Biology Laboratory\\nGraph Convolutional\\nHigh-Throughput Screening\\nhalf maximal inhibitory concentration\\nInfluence Relevance Voting\\nDissociation constant\\nInhibition constant\\nk-Nearest Neighbour\\nKernel Ridge Regression\\nLogistic Regression\\nMultilayer Perceptron\\nMultitask Neural Network\\nMessage Passing Neural Network\\nMaximum Unbiased Validation\\npotential of hydrogen\\nRandom Forests\\nReceiver Operating Characteristic Area Under Curve\\nSimplified Molecular-Input Line-Entry System\\neXtreme Gradient Boosting\\n- Acd/labs software. https://www.acdlabs.com . Accessed 16 Jul 2019\\n- Cambridge medchem consulting provides a range of consultancy services in drug discovery and medicinal chemistry. https://www.cambridgemedchemconsulting.com . Accessed 16 Jul 2019\\n- International chemical identifier. https://en.wikipedia.org/wiki/International_Chemical_Identifier . Accessed 30 Jan 2021\\n- MolVS molecule validation and standardization. https://molvs.readthedocs.io/en/latest/ . Accessed 16 Jul 2019\\n- Software solutions and services for chemistry & biology. https://chemaxon.com . Accessed 16 Jul 2019\\n- Aghdam HH, Heravi EJ (2017) Guide to convolutional neural networks: a practical application to traffic-sign detection and classification, 1st edn. Springer Publishing Company Incorporated, Berlin Book Google Scholar\\n- Agrawal A, Choudhary A (2016) Perspective: Materials informatics and big data: realization of the fourth paradigm of science in materials science. APL Materials 4:053208 Article Google Scholar\\n- Agrawal A, Deshpande P, Cecen A, Gautham B, Choudhary A, Kalidindi S (2014) Exploration of data science techniques to predict fatigue strength of steel from composition and processing parameters. Integr Mater Manuf Innov 3:90–128 Article Google Scholar\\n- Bajusz D, Rácz A, Héberger K (2015) Why is tanimoto index an appropriate choice for fingerprint-based similarity calculations? J Cheminform 7(1):20 Article Google Scholar\\n- Bartels R (2015) Re-interpreting r-squared, regression through the origin, and weighted least squares\\n- Bento AP, Gaulton A, Hersey A, Bellis LJ, Chambers J, Davies M, Krüger FA, Light Y, Mak L, McGlinchey S et al (2014) The chembl bioactivity database: an update. Nucleic Acids Res 42(D1):D1083–D1090 Article Google Scholar\\n- Bradley AP (1997) The use of the area under the roc curve in the evaluation of machine learning algorithms. Pattern Recogn 30(7):1145–1159 Article Google Scholar\\n- Butina D (1999) Unsupervised data base clustering based on daylight’s fingerprint and tanimoto similarity: a fast and automated way to cluster small and large data sets. J Chem Inf Comput Sci 39(4):747–750 Article Google Scholar\\n- Chen T, Guestrin C (2016) Xgboost: a scalable tree boosting system. In: Proceedings of the 22Nd ACM SIGKDD international conference on knowledge discovery and data mining, KDD ’16, ACM, New York, NY, USA, pp 785–794\\n- Cox B, Merritt AT, Binnie A, Donnelly MC, Mander TH, Denyer JC, Evans B, Green DV, Lewis JA, Valler MJ, Watson SP (2000) 3-application of high-throughput screening techniques to drug discovery. Elsevier, Amsterdam, pp 83–133 Google Scholar\\n- Davies M, Nowotka M, Papadatos G, Dedman N, Gaulton A, Atkinson F, Bellis L, Overington JP (2015) Chembl web services: streamlining access to drug discovery data and utilities. Nucleic Acids Res 43(W1):W612–W620 Article Google Scholar\\n- Dietterich T (1995) Overfitting and undercomputing in machine learning. ACM Comput Surv 27(3):326–327 Article Google Scholar\\n- Duvenaud DK, Maclaurin D, Aguilera-Iparraguirre J, Gómez-Bombarelli R, Hirzel T, Aspuru-Guzik A, Adams R (2015) Convolutional networks on graphs for learning molecular fingerprints. CoRR arXiv:1509.09292\\n- Evgeniou T, Pontil M (2001) Support vector machines: theory and applications. pp 249–257. https://doi.org/10.1007/3-540-44673-7_12\\n- Gagorik AG, Savoie B, Jackson N, Agrawal A, Choudhary A, Ratner MA, Kohlstedt KL (2016) Improved scaling of molecular network calculations: the emergence of molecular domains. J Phys Chem Lett 8:415–421 Article Google Scholar\\n- Galushka M, Browne F, Mulvenna MD, Bond R, Lightbody G (2018) Toxicity prediction using pre-trained autoencoder. In: IEEE international conference on bioinformatics and biomedicine, BIBM 2018, Madrid, Spain, December 3–6, pp 299–304\\n- Garciarena U, Santana R, Mendiburu A (2018) Expanding variational autoencoders for learning and exploiting latent representations in search distributions. In: Proceedings of the genetic and evolutionary computation conference, GECCO ’18, ACM, New York, NY, pp 849–856\\n- Gilmer J, Schoenholz SS, Riley PF, Vinyals O, Dahl GE (2017) Neural message passing for quantum chemistry. CoRR arXiv:1704.01212 (2017)', '- Gilmer J, Schoenholz SS, Riley PF, Vinyals O, Dahl GE (2017) Neural message passing for quantum chemistry. CoRR arXiv:1704.01212 (2017)\\n- Goodfellow I, Bengio Y, Courville A, Bengio Y (2016) Deep learning, vol 1. MIT Press Cambridge\\n- Gómez-Bombarelli R, Wei JN, Duvenaud D, Hernández-Lobato JM, Sánchez-Lengeling B, Sheberla D, Aguilera-Iparraguirre J, Hirzel TD, Adams RP, Aspuru-Guzik A (2018) Automatic chemical design using a data-driven continuous representation of molecules. ACS Central Sci 4(2):268–276 Article Google Scholar\\n- Heaton J, Polson N, Witte JH (2016) Deep learning in finance. arXiv preprint arXiv:1602.06561\\n- Hoffman MD, Blei DM, Wang C, Paisley J (2013) Stochastic variational inference. J Mach Learn Res 14(1):1303–1347 MathSciNet MATH Google Scholar\\n- Irwin J, Shoichet B (2005) Zinc - a free database of commercially available compounds for virtual screening. J Chem Inf Model 45:177–82 Article Google Scholar\\n- Simplified S (2014) Normal distribution. J Conserv Dent 17(1):96–97 Article Google Scholar\\n- Jiang X, Zhang, Y., Zhang, W., Xiao, X.: A novel sparse auto-encoder for deep unsupervised learning. In: 2013 Sixth international conference on advanced computational intelligence (ICACI) (2013)\\n- Kearnes SM, McCloskey K, Berndl M, Pande VS, Riley P (2016) Molecular graph convolutions: moving beyond fingerprints. J Comput Aided Mol Des 30(8):595–608 Article Google Scholar\\n- Ker J, Wang L, Rao J, Lim T (2018) Deep learning applications in medical image analysis. IEEE Access 6:9375–9389 Article Google Scholar\\n- Kingma DP, Welling M (2019) An introduction to variational autoencoders. CoRR arXiv:1906.02691 (2019)\\n- Klose M, Theiner S, Varbanov H, Hoefer D, Pichler V, Galanski M, Meier-Menches S, Keppler B (2018) Development and validation of liquid chromatography-based methods to assess the lipophilicity of cytotoxic platinum(iv) complexes. Inorganics 6(4):130. https://doi.org/10.3390/inorganics6040130 Article Google Scholar\\n- Koutsoukas A, St Amand J, Mishra M, Huan J (2016) Predictive toxicology: modeling chemical induced toxicological response combining circular fingerprints with random forest and support vector machine. Front Environ Sci 4:11 Article Google Scholar\\n- Kramer O (2013) K-nearest neighbors. Springer, Berlin, p 2013 Google Scholar\\n- Lusci A, Pollastri G, Baldi P (2013) Deep architectures and deep learning in chemoinformatics: the prediction of aqueous solubility for drug-like molecules. J Chem Inf Model 53(7):1563–75 Article Google Scholar\\n- MacKay DJC (1998) Introduction to monte carlo methods. In: Jordan MI (ed) Learning in graphical models, NATO science Series, Kluwer Academic Press, Amsterdam, pp 175–204 (1998)\\n- Manning C, Surdeanu M, Bauer J, Finkel J, Bethard S, McClosky D (2014) The stanford corenlp natural language processing toolkit. In: Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations, pp 55–60\\n- Mayr A, Klambauer G, Unterthiner T, Steijaert M, Wegner JK, Ceulemans H, Clevert DA, Hochreiter S (2018) Large-scale comparison of machine learning methods for drug target prediction on chembl. Chem Sci 9:5441–5451 Article Google Scholar\\n- Menard S (2002) Applied logistic regression analysis. No. v. 106; v. 2002 in Quantitative applications in the social sciences. Sage Publications, New York\\n- Miller FP, Vandome AF, McBrewster J (2009) Levenshtein distance: information theory, computer science, string (computer science), string metric, damerau? Levenshtein distance, spell checker, hamming distance. Alpha Press, Orlando Google Scholar\\n- Mozaffar M, Paul A, Al-Bahrani R, Wolff S, Choudhary A, Agrawal A, Ehmann K, Cao J (2018) Data-driven prediction of the high-dimensional thermal history in directed energy deposition processes via recurrent neural networks. Manuf Lett 18:35–39. https://doi.org/10.1016/j.mfglet.2018.10.002 Article Google Scholar\\n- Norouzi M, Fleet DJ, Salakhutdinov RR (2012) Hamming distance metric learning. In: Pereira F, Burges CJC, Bottou L, Weinberger KQ (eds) Advances in neural information processing systems, vol 25, pp 1061–1069. Curran Associates, Inc\\n- OECD: Test No. 107: Partition Coefficient (n-octanol/water): Shake Flask Method (1995). https://doi.org/10.1787/9789264069626-en\\n- Popova M, Isayev O, Tropsha A (2018) Deep reinforcement learning for de novo drug design. Sci Adv 4(7):eee7855 Article Google Scholar\\n- Raiber F, Kurland O (2017) Kullback-leibler divergence revisited. In: Proceedings of the ACM SIGIR international conference on theory of information retrieval, ICTIR ’17, ACM, New York, NY, pp 117–124\\n- Rohrer SG, Baumann K (2009) Maximum unbiased validation (muv) data sets for virtual screening based on pubchem bioactivity data. J Chem Inf Model 49(2):169–184 Article Google Scholar\\n- Ruder S (2017) An overview of multi-task learning in deep neural networks. CoRR arXiv:1706.05098(2017 )', '- Ruder S (2017) An overview of multi-task learning in deep neural networks. CoRR arXiv:1706.05098(2017 )\\n- Shivanyuk A, Ryabukhin S, Bogolyubsky A, Mykytenko D, Chuprina A, Heilman W, Kostyuk A, Tolmachev A (2007) Enamine real database: making chemical diversity real. Chim Oggi 25:58–59 Google Scholar\\n- Swamidass SJ, Azencott CA, Lin TW, Gramajo H, Tsai SC, Baldi P (2009) Influence relevance voting: an accurate and interpretable virtual high throughput screening method. J Chem Inf Model 49(4):756–766 Article Google Scholar\\n- Weininger D (1988) Smiles, a chemical language and information system. 1. Introduction to methodology and encoding rules. J Chem Inf Comput Sci 28(1):31–36 Article Google Scholar\\n- Wishart D, Knox C, Guo A, Shrivastava S, Hassanali M, Stothard P, Chang Z, Woolsey J (2006) Drugbank: a comprehensive resource for in silico drug discovery and exploration. Database Issue 34:668–672 Google Scholar\\n- Wu Z, Ramsundar B, Feinberg EN, Gomes J, Geniesse C, Pappu AS, Leswing K, Pande V (2018) Moleculenet: a benchmark for molecular machine learning. Chem Sci 9:513–530 Article Google Scholar\\n- Wu Z, Ramsundar B, Feinberg EN, Gomes J, Geniesse C, Pappu AS, Leswing K, Pande VS (2018) Moleculenet: a benchmark for molecular machine learning. Chem Sci 9(2):513–530 Article Google Scholar\\n- Zhang C, Ma Y (2012) Ensemble machine learning: methods and applications. Springer, New York Book Google Scholar\\n- Zhang Y, Duchi J, Wainwright M (2015) Divide and conquer kernel ridge regression: a distributed algorithm with minimax optimal rates. J Mach Learn Res 16(1):3299–3340 MathSciNet MATH Google Scholar\\nDownload references', \"## Acknowledgements\\n\\nWe acknowledge the contribution of Chris Swain the Founded Cambridge MedChem Consulting.\\n\\n## Author information\\n\\n### Authors and Affiliations\\n\\n- AUROMIND Ltd., 126 Eglantine Avenue, Belfast, BT9 6EU, UK Mykola Galushka\\n- Cambridge MedChem Consulting, 8 Mangers Lane, Duxford, Cambs, CB22 4RN, UK Chris Swain\\n- Datactics Ltd., One Lanyon Quay, Belfast, BT1 3LG, UK Fiona Browne\\n- School of Computing, Ulster University, Jordanstown, BT37 0QB, UK Maurice D. Mulvenna\\xa0&\\xa0Raymond Bond\\n- Almac Sciences Ltd., 20 Seagoe Industrial Estate, Craigavon, BT63 5QD, UK Darren Gray\\n- Mykola Galushka View author publications Search author on: PubMed Google Scholar\\n- Chris Swain View author publications Search author on: PubMed Google Scholar\\n- Fiona Browne View author publications Search author on: PubMed Google Scholar\\n- Maurice D. Mulvenna View author publications Search author on: PubMed Google Scholar\\n- Raymond Bond View author publications Search author on: PubMed Google Scholar\\n- Darren Gray View author publications Search author on: PubMed Google Scholar\\n\\n### Corresponding author\\n\\nCorrespondence to Maurice D. Mulvenna .\\n\\n## Ethics declarations\\n\\n### Conflicts of interest\\n\\nThe authors declare that they have no conflict of interest.\\n\\n## Additional information\\n\\n### Publisher's Note\\n\\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\\n\\n## Rights and permissions\\n\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\\nReprints and permissions\\n\\n## About this article\\n\\n### Cite this article\\n\\nGalushka, M., Swain, C., Browne, F. et al. Prediction of chemical compounds properties using a deep learning model. Neural Comput & Applic 33 , 13345–13366 (2021). https://doi.org/10.1007/s00521-021-05961-4\\nDownload citation\\n- Received : 08 September 2020\\n- Accepted : 25 March 2021\\n- Published : 04 June 2021\\n- Issue Date : October 2021\\n- DOI : https://doi.org/10.1007/s00521-021-05961-4\\n\\n### Share this article\\n\\nAnyone you share the following link with will be able to read this content:\\nGet shareable link\\nSorry, a shareable link is not currently available for this article.\\nCopy to clipboard\\nProvided by the Springer Nature SharedIt content-sharing initiative\\n- Machine learning\\n- Deep neural networks\\n- chemical compounds Properties\\n- Maurice D. Mulvenna View author profile\\nUse our pre-submission checklist\\nAvoid common mistakes on your manuscript.\\nAdvertisement\", 'Active Journals\\nFind a Journal\\nJournal Proposal\\nProceedings Series', 'Open Access\\n\\n# Molecular Image-Based Prediction Models of Nuclear Receptor Agonists and Antagonists Using the DeepSnap-Deep Learning Approach with the Tox21 10K Library\\n\\nYasunari Matsuzaka\\nYasunari Matsuzaka\\nSciProfiles\\nPreprints.org\\nGoogle Scholar\\nYoshihiro Uesawa\\nYoshihiro Uesawa\\nSciProfiles\\nPreprints.org\\nGoogle Scholar\\nDepartment of Medical Molecular Informatics, Meiji Pharmaceutical University, Tokyo 204-8588, Japan\\nAuthor to whom correspondence should be addressed.\\n(12), 2764;\\nhttps://doi.org/10.3390/molecules25122764\\nSubmission received: 7 May 2020\\nRevised: 6 June 2020\\nAccepted: 12 June 2020\\nPublished: 15 June 2020\\n(This article belongs to the Special Issue\\nDeep Learning for Molecular Structure Modelling\\nkeyboard_arrow_down\\nDownload PDF\\nDownload PDF with Cover\\nDownload XML\\nDownload Epub\\nDownload Supplementary Material\\nBrowse Figures\\nVersions\\xa0Notes\\nAbstract : The interaction of nuclear receptors (NRs) with chemical compounds can cause dysregulation of endocrine signaling pathways, leading to adverse health outcomes due to the disruption of natural hormones. Thus, identifying possible ligands of NRs is a crucial task for understanding the adverse outcome pathway (AOP) for human toxicity as well as the development of novel drugs. However, the experimental assessment of novel ligands remains expensive and time-consuming. Therefore, an in silico approach with a wide range of applications instead of experimental examination is highly desirable. The recently developed novel molecular image-based deep learning (DL) method, DeepSnap-DL, can produce multiple snapshots from three-dimensional (3D) chemical structures and has achieved high performance in the prediction of chemicals for toxicological evaluation. In this study, we used DeepSnap-DL to construct prediction models of 35 agonist and antagonist allosteric modulators of NRs for chemicals derived from the Tox21 10K library. We demonstrate the high performance of DeepSnap-DL in constructing prediction models. These findings may aid in interpreting the key molecular events of toxicity and support the development of new fields of machine learning to identify environmental chemicals with the potential to interact with NR signaling pathways. Keywords: chemical structure ; DeepSnap ; deep learning ; nuclear receptor ; QSAR ; Tox21 10K library\\n\\n## 1. Introduction', 'Many chemical substances have potential harmful effects, causing the perturbation of endocrine homeostasis by interfering with various nuclear receptors (NRs) of hormones [\\n]. In the disruption of hormone pathways, structurally diverse groups of chemicals are known to interact primarily with ligand–NR bindings, which have the ability to substitute for natural ligands, ultimately resulting in proliferative, reproductive, and metabolic disorders [\\n]. NRs are a superfamily of ligand-dependent transcriptional factors containing n N-terminal transactivation domain, a flexible hinge region, and a C-terminal ligand-binding domain (LBD) [\\n]. NRs are classified mainly into two types according to their subcellular distribution in the absence of a ligand and their mechanisms: Type I steroid receptors, including the estrogen receptor (ER), androgen receptor (AR), progesterone receptor (PR), and glucocorticoid receptor (GR); and Type II nonsteroid receptors, including the thyroid receptor (TR alpha and beta), retinoic acid receptor (RAR alpha, beta, and gamma), retinoid X receptor (RXR), vitamin D receptor (VDR), peroxisome proliferator-activated receptor (PPAR alpha, beta, and gamma), liver X receptor (LXR), farnesoid X receptor (FXR), and pregane X receptor (PXR), [\\n]. In the absence of a ligand, the type I NR forms inactive complexes with chaperone proteins in the cytoplasm, whereas type II NR, regardless of the ligand-binding status, is located in the nucleus and binds to the DNA response elements of its target genes along with corepressors [\\n]. For these types of NRs, a number of allosteric modulators have been identified that can act as either agonist or antagonist by occupying the active pocket of the NR and blocking the recruitment of coactivators or corepressors to the transcriptional complex [\\nThe perturbation of the NR signaling pathway due to the action of agonists or antagonists of chemical compounds is associated with various adverse health outcomes [\\n]. Although chemical hazard assessments have traditionally relied upon toxicity data from animal bioassays and epidemiological studies, there are some drawbacks to this testing method, such as high cost, lengthy test durations, and ethical concerns [\\n]. To resolve these issues, the in vitro high-throughput screening (HTS) assay has been developed as an alternative approach and improved by the Toxicity Forecaster (ToxCast\\n) program run by the U.S. Environmental Protection Agency (EPA) [\\n] and The Toxicology in the 21st Century program (Tox21), an interagency federal collaboration launched by the consortium of the EPA, the U.S. Food and Drug Administration (FDA), the National Institutes of Health (NIH), and the National Toxicology Program (NTP) [\\n]. However, the HTS assay is not sufficient to screen all classes of chemicals, such as those still in molecular development and optimization phase, and thus cannot provide an accurate evaluation of the potential toxicity of chemicals in humans and the environment [\\nRecent technological advances have focused on in silico approaches, such as quantitative structure–activity relationship (QSAR), based on the assumption that similar structures are associated with similar biological activities, taking advantage of their ability to accurately predict the toxicologically discrete values of the chemical or biological properties of molecules [\\n]. However, the QSAR approach has the following disadvantages: (i) required skills and knowledge for feature extraction and selection, (ii) paucity of model interpretability, and (iii) low prediction performance due to the dependence on the choice of molecular descriptors and the prediction modeling algorithms [\\n]. To address these issues, a novel deep learning (DL)-based QSAR method, called DeepSnap-DL [\\n], was developed using molecular image files generated from the steric conformation of three-dimensional (3D) chemical structures, leveraging the increasing evidence of successful classification by convolutional neural networks (CNNs) through DL in toxicological fields [\\n]. This method has the following advantages. First, the feature(s) in the molecular images can be automatically extracted by CNNs. Second, high prediction performance can be expected as more detailed information of the chemical structure can be captured from different viewing directions along the x-, y-, and z-axes [\\n]. Third, determination and visualization of the conformer that is docked in the LBD of the receptor protein may reveal the critical conformation of the chemicals and domain of the receptor protein related to the adverse outcome.', ']. Third, determination and visualization of the conformer that is docked in the LBD of the receptor protein may reveal the critical conformation of the chemicals and domain of the receptor protein related to the adverse outcome.\\nIn this study, using the DeepSnap-DL method, prediction models of 35 agonists and antagonists of NRs were constructed by 3D molecular structure representations using information of chemical compounds from the Tox21 10K library. The results obtained by the DeepSnap-DL method outperformed those of the methods that won the Tox21 data challenge. Therefore, our approach can be practically applied to build prediction models using a CNN for a large number of chemicals to determine their potential toxicity.', '## 2. Results and Discussion', 'To build the prediction models of the agonists and antagonists of NRs, we downloaded the information of 35 NRs for the chemical structures and their activity scores from the Tox21 10K library. The mean number of chemicals was 7262 ± 267, and the highest and lowest numbers of the chemicals were respectively 7671 (progesterone receptor agonist: PR_ago, AID: 1347036), and 6735 (estrogen-related receptor agonist: ERR_ago, AID: 1259404) (\\n). Furthermore, we classified the datasets of these chemical compounds into two groups based on their activity scores—active chemicals were those with an activity score ≥ 40 and inactive chemicals had an activity score < 40. The mean number of active chemicals in the total chemicals was 0.0372 ± 0.0376, and the highest and lowest numbers of active chemicals were, respectively, 0.2052 (pregane X receptor agonist: PXR_ago, AID: 1347033) and 0.0022 (vitamin D receptor agonist: VDR_ago, AID: 743241) (\\n). These results indicate that the datasets are highly class imbalanced.\\nNext, the datasets were divided into Tra:Val:Test groups with a 4:4:1 ratio. The mean numbers of active and inactive chemicals were, respectively, 120.9 ± 124.1 and 3107.1 ± 153.1 in Tra, 120.8 ± 124.3 and 3106.8 ± 153.3 in Val, and 30.1 ± 30.9 and 271.8 ± 279.2 in Test (\\nTable S1, Supplementary Materials\\n). In addition, the highest and lowest numbers of the active chemicals were, respectively, 683 and 2 in Tra, 684 and 6 in Val, and 170 and 2 in Test (\\n). The molecular images derived from the 3D chemical structures were generated using the DeepSnap approach at different angles along the x-, y-, and z-axes, i.e., (176°, 176°, 176°). A total of 27 images for one chemical compound was captured (\\nUsing these molecular images as input data into the DL, the prediction models of 35 NR agonists and antagonists were constructed using Tra, and validated with Val. The values of mean Loss (Val) and Acc (Val) were 0.0748 ± 0.0035 and 97.56 ± 0.09, respectively (\\nFigure S4a, Supplementary Materials\\n). In addition, the highest prediction performance on the Val dataset was observed in the thyroid-stimulating hormone receptor agonist (TSHR2_ago, AID: 1259393), for which the mean Loss (Val) and Acc (Val) were 0.0017 ± 0.0008 and 99.93 ± 0.02, respectively (\\n). The prediction performance of these models was evaluated using Test based on five metrics, namely AUC, BAC, F, Acc (Test), and MCC. The results showed that the mean AUC, BAC, F, Acc (Test), and MCC were 0.8842 ± 0.0165, 0.8471 ± 0.0168, 0.3085 ± 0.0411, 82.73 ± 3.92, and 0.3536 ± 0.0377, respectively (\\nFigure S4a,b\\n). In addition, the highest prediction performance on Test was observed in the thyroid-stimulating hormone receptor agonist (TSHR2_ago, AID: 1259393), with the mean AUC, BAC, F, Acc (Test), and MCC being 0.9994 ± 0.0006, 0.9997 ± 0.0003, 0.9286 ± 0.0714, 99.94 ± 0.06, and 0.9327 ± 0.0673, respectively (\\nFigure S4a,b\\nThe Tox21 Data Challenge 2014 was designed to understand the interference of the chemical compounds derived from the Tox21 10K compound library in the biological pathway via crowdsourced data analysis by independent researchers. It used data generated from seven NR signaling pathway assays to construct prediction models for QSARs [\\n]. The BAC values of the three models constructed by the proposed DeepSnap-DL were 0.8361, 0.8204, and 0.8494, respectively, outperforming the Data Challenge models where the BACs of three models, namely AID:743053 (Arfull_ago), AID:743077 (Erlbd_ago), and AID:743140 (PPARg_ago), were 0.6500, 0.7147, and 0.7852, respectively. However, the best prediction model of AID:743122 (AhR_ago) had a BAC value of 0.8528 in the Data Challenge, whose BAC outperformed that in the DeepSnap-DL method (0.7785). Up to now, conflicting observations have been reported regarding whether DL performs better than conventional shallow machine learning (ML) methods, such as random forest, support vector machine, and gradient boosting decision tree [\\n]. Although some reports suggest that DL outperforms conventional ML methods owing to various improvements, the performance of DL in terms of QSAR may be affected by many factors, such as molecular descriptors, assay targets, chemical space, hyper-parameter optimization, DL architectures, input data size, and quality [', 'Furthermore, the DeepSnap-DL approach has the black box problem, that is, it lacks explainability and interpretability of the prediction models because the convolutional area on the image picture by CNN is not defined. This issue has been extensively studied, especially in the field of image recognition. These studies try to resolve the issue by calculating the gradient of the input image with respect to the output label and highlighting the target pixel as a recognition target when a slight change in a specific input pixel causes a large change in the output label. However, a simple calculation of the gradient generates a noisy highlight, so some improved methods have been proposed for sharpening [\\n]. In addition, in the DeepSnap-DL approach, the performance improves as data size increases, and performance deterioration is observed with insufficient data size or the presence of noise. However, simply increasing the sample size causes problems such as overfitting and increased calculation costs. To resolve the issues of the DeepSnap-DL approach, critical factors include specifying the image area and type required for effective feature extraction to reduce the input data volume, and clarification of the functional relationship of chemical substances with biological activity in vivo. Future applications may include screening of target molecules in specific pathological reactions.\\nTo investigate whether the in vitro bioassays for agonist and antagonist mode in the Tox21 program affect the prediction performance of NRs, we compared prediction performances among four in vitro assays, namely, luciferase, beta-lactamase, cAMP, and intracellular calcium assays, using the results of 35 NR agonist and antagonist prediction models. In the Val dataset, the loss and accuracy values in the luciferase assay were significantly higher and lower, respectively, compared with that of the beta-lactamase assay (\\n< 0.05 for both Loss (Val) and Acc (Val)).\\nIn addition, F and MCC in Test of the cAMP assay significantly increased compared with those of the beta-lactamase assay (\\n< 0.05 for both F-measure and MCC). The BAC value in the Test dataset of the cAMP assay showed a moderate increase compared with that of the beta-lactamase assay (\\nFigure S5c, Supplementary Materials\\n< 0.09). These results indicate that the prediction performance of the NR agonists and antagonists in the Tox21 10K library may be affected by the choice of the in vitro assay method. There are several conflicting reports regarding the in vitro receptor-mediated activity. Chemicals such as bisphenol A (BSA) and its halogenated analogs (tetrabromo-BSA and tetrachloro-BSA) show weak TR antagonist activity but have a potential agonist-like effect at lower concentrations [\\n]. Thus, competitive agonists and antagonists of the steroids have long been known [\\n]. Among them, ligands exhibiting agonist and antagonist activity, called selective steroid receptor modulators (SSRMs), are known to show specificity on tissue or cell type [\\n]. In addition, a competitive antagonist, known as the passive antagonist, hinders the binding but induces the inactive state of NRs by modifying interaction with their corepressor and interfering with their nuclear translocation or DNA binding at saturated concentrations [\\n]. These reports suggest that the ligand of the steroid NRs can serve not only as competitive agonists and antagonists that affect binding to the NRs, but also as a unique allosteric modulator for subsequent molecular interactions. Therefore, classification of the chemicals in the Tox21 10K library may require more detailed insights of the molecular mechanisms of the NRs with chemical compounds and the conditions of in vitro bioassays.', '## 3. Conclusions\\n\\nIn this study, we built prediction models of 35 NR agonists and antagonists using the DeepSnap-DL approach with information of the chemical structure and activity from the Tox21 10K library. Three prediction models outperformed the best performing models in the Tox21 Data Challenge 2014. These results suggest that the 3D chemical structure representation in the DeepSnap-DL approach may be useful for molecular image-based QSAR analysis, and the improvements to the DeepSnap-DL method may aid in achieving high-performing prediction models.\\n\\n## 4. Materials and Methods\\n\\nIn this study, the original datasets related to chemical structures and the corresponding agonist and antagonist scores were downloaded as reported previously [\\n], in the simplified molecular input line entry system (SMILES) format from the PubChem database. We used a keyword of the database search, namely “Tox21 bioassays”, and selected bioassays of the 35 from the NR signaling pathway for the identification of agonists/antagonists (\\n). These bioassay data consisted of quantitative HTS (qHTS) data derived from two cell-based reporter gene assays, including beta-lactamase or luciferase reporter genes. The activity of these reporter genes is controlled by the binding of transcriptional factors induced or suppressed by an agonist/antagonist with response elements (REs) for ARs, ER-alpha, ER-beta, estrogen-related receptors (ERR), FXR, PPAR−gamma, PRs, retinoid-related orphan receptor gamma (ROR−gamma), RXR−alpha, RARs, GRs, TRs, thyroid-stimulating hormone receptors (TSHRs), aryl hydrocarbon receptors (AhRs), VDRs, constitutive androstane receptors (CARs), and PXRs. These receptors are stably integrated into cell lines, including human embryonic kidney 293 cells(HEK293 (AR, ER−alpha, ER−beta, ERR, and TSHR), HEK293H (PPAR−gamma, PPAR−delta, and HEK293T (ER−beta, FXR, PR, RXR−alpha, and VDR)), human breast cancer cells (MDA−MB (AR)), ovarian carcinoma cells (BG1 (ER−alpha)), Chinese hamster ovary cells CHO (ROR−gamma)), human cervical cancer cells (HeLa (GR)), rat pituitary tumor cells (GH3 (TR)), human hepatocellular carcinoma cells (HepG2 (AhR, CAR, PXR)), and C3H mouse embryo cells (C3RL4 (RXR−alpha)). Then, we can measure the ability to induce or inhibit RE-dependent transcription.\\nThe chemicals were derived from the Tox21 10K library, which contains approximately 8900 unique compounds gathered from commercial sources, such as pesticides, industrial and environmental chemicals, natural dietary supplement products, food additives, and drugs, by the NTP, the National Center for Advancing Translational Sciences (NCATS), and the EPA (\\n]. These compounds were dissolved in dimethyl sulfoxide (DMSO) as stock solutions, and compound plates with the different concentrations were prepared in the 1536-well plate format [\\n]. These cell lines of beta-lactamase reporter gene assay constitutively co-express a fusion protein comprised of the LBDs of the human NRs coupled to the DNA-binding domain (DBD) of the yeast transcription factor GAL4 [\\n]. When activated, these fusion proteins stimulate beta-lactamase reporter gene expression.\\nThe cells were dispensed at 1500 to 5000 cells/5 (for antagonist mode) or 6 (for agonist mode) microL/well in 1536-well black wall/clear bottom plates [\\n]. After the cells were incubated at 37 °C for 5 to 6 h depending on the particular NR cell line to allow for cell attachment, 23 nL of the compounds at different concentrations were transferred to the assay plates. For the antagonist mode assay, the known agonist for each NR was added into the assay plates. For the agonist and antagonist mode assays, positive control compounds were dispensed into each other’s wells on the plates (\\n]. The plates were incubated for 16 to 18 h at 37 °C depending on the particular NR cell line. Then, a LiveBLAzer\\nB/G FRET substrate (Invitrogen, Carlsbad, CA, USA) detection mix was added, and the plates were incubated at room temperature for 1.5 to 2 h. The fluorescence intensity (405 nm excitation, 460 and 530 nm emission) was measured using an Envision plate reader (PerkinElmer, Shelton, CT, USA). Data were expressed as the ratio of 460/530 nm emission values. To measure the luciferase reporter gene activity, 4 microL of ONE-Glo\\nLuciferase Assay reagent (Promega, Madison, WI, USA) were added to each plate, and the luminescence intensity was quantified by a ViewLux plate reader (PerkinElmer) after 30 min of incubation at room temperature. Data were expressed as relative luminescence units.\\n\\n#### 4.2. qHTS Data Analysis', '#### 4.2. qHTS Data Analysis\\n\\nThe Tox21 10k library can be grouped into clusters with similar activity that share similar annotated models of action according to PubChem activity scores. In the qHTS of the Tox21 program, to identify the chemical compounds in both potential agonist and antagonist modes, the PubChem activity scores were determined from 0% to 100% by normalizing each titration point relative to the positive control compound (agonist mode: 100%, antagonist mode: 0%) and DMSO-only wells (agonist mode: 0%, antagonist mode: -100%) according to the following equation: % Activity = [(Vcompound − Vdmso)/(Vpos − Vdmso)] × 100, where Vcompound, Vdmso, and Vpos denote the compound well values, the median values of the DMSO-only wells, and the median value of the positive control well, respectively.\\nThe datasets were then corrected using compound-free control plates, i.e., DMSO-only plates, at the beginning and end of the compound plate measurement [\\n]. The half maximum inhibition values (IC\\n) and the maximum response values for each compound were calculated by fitting the concentration–response curves of each compound to a four-parameter Hill equation [\\nThe PubCem activity scores of the agonists and antagonists were grouped into three classes, namely (1) 0, (2) 1–39, and (3) 40–100, which represent inactive, inconclusive, and active compounds, respectively. In this study, compounds with activity scores of 40–100 or 0–39 were defined as active or inactive, respectively. The dataset includes some similar chemical compounds, but with different activity scores for different ID (identification) numbers due to the presence of possible stereoisomers or salts. Therefore, chemical compounds with indefinite activity criteria, nonorganic compounds, and/or inaccurate SMILES were eliminated.\\n\\n#### 4.3. DeepSnap\\n\\nWe then applied a 3D conformational import from the SMILES format using molecular operating environment (MOE) 2018 software (MOLSIS Inc., Tokyo, Japan) to generate the chemical database. Here, the neutralization of the protonation state and the coordinating washed species were used by the external program, CORINA classic software (\\nFigure S1, Supplementary Materials\\n]. The resulting 3D structures were then saved in an SDF file format. Using the SDF files prepared by the MOE application, the 3D chemical structures were depicted as 3D ball-and-stick models with different colors corresponding to different atoms by Jmol, an open-source Java viewer software (version number, manufacturer, city, state abbreviation, country) for 3D molecular modeling of chemical structures [\\n]. These 3D chemical structures produce different images depending on the direction. The 3D chemical models were captured automatically as snapshots with user-defined angle increments with respect to the x-, y-, and z-axes. In this study, one angle increment was used, i.e., (176°, 176°, 176°). Other parameters for the DeepSnap depiction process were set based on previous studies as follows: image pixel size: 256 × 256; molecule number per SDF file to split into: 100; zoom factor (%): 100; atom size for van der Waals radius (%): 23; bond radius (mÅ): 14.5; minimum bond distance: 0.4; and bond tolerance: 0.8 [\\n]. The snapshots saved as 256 × 256 pixel resolution PNG files (RGB) were divided into three types of datasets: training (Tra), validation (Val), and test (Test) (\\n\\n#### 4.4. Preparation of Dataset\\n\\nThree groups of datasets were prepared by dividing the data into Tra, Val, and Test groups. The data were first split into 11 groups, and the two dataset groups (4:4:1_01 and 4:4:1_02) were then built in accordance with the ratio of Tra:Val:Test = 4:4:1. A prediction model was created using the Tra and Val datasets. Then, the prediction performance was evaluated using the Test dataset (4:4:1_01) (\\nFigure S2, Supplementary Materials\\n). For a subsequent analysis, the remaining Test dataset was selected from the group not used in the first analysis. The model was then built, and its probability calculation was examined in the same manner (4:4:1_02). Finally, two tests were performed and the average was calculated (\\n\\n#### 4.5. Deep Learning', '#### 4.5. Deep Learning\\n\\nAll the two-dimensional (2D) PNG images produced by DeepSnap were resized by utilizing the NVIDIA DL GPU Training System (DIGITS) version 4.0.0 software (NVIDIA, Santa Clara, CA, USA), on four-GPU systems, Tesla-V100-PCIE (31.7 GB), with a resolution of 256 × 256 pixels as input data, as previously reported [\\n]. The prediction model was pre-trained as transfer learning [\\n] by the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012 dataset [\\n], which includes 1000 classes, such as animal (40%), device (12%), container (9%), consumer goods (6%), and equipment (4%). The ILSVRC 2012 dataset was divided as 1.2 million Tra, 50,000 Val, and 1 million Test datasets extracted from ImageNet [\\n]. To rapidly train and fine-tune the highly accurate CNNs using the input Tra and Val datasets based on the image classification and building the pre-trained prediction model, we used a pre-trained open-source DL model, Caffe, and the open-source software on the CentOS Linux distribution 7.3.1611. In this study, the deep CNN architecture was GoogLeNet, which is a complex network inspired by LeNet and implemented with a novel module called “Inception”, which facilitates batch normalization, image distortions, and RMSprop; concatenates different filter sizes and dimensions into a single new filter; and introduces sparsity and multiscale information in one block (\\nFigure S3, Supplementary Materials\\n). The network is a 22-layer deep CNN, comprising two convolutional layers, two types of pooling layers (four max pools and one avg pool), and nine Inception modules, each module having six convolution layers and one pooling layer, with 4 million parameters (\\nIn the DeepSnap-DL method, the prediction models were constructed by training datasets using 30 epochs with 1 snapshot interval in each epoch, 1 validation interval in each epoch, 1 random seed, a stochastic gradient descent-type solver, a learning rate of 0.006, and a batch size of 108 in DL. Among the epochs, the lowest Loss value in the Val dataset (Loss (Val)), which is the error rate between the results obtained from the validation data and the corresponding labeled dataset, was selected for subsequent examination of prediction using the Test dataset.\\n\\n#### 4.6. Evaluation of the Predictive Model\\n\\nThrough two tests conducted on the Test datasets for the experiments, with Tra:Val:Test = 4:4:1 in the DL prediction model, we analyzed the probability of the prediction results with the lowest minimum Loss (Val) value among 30 examined epochs. We calculated the probabilities for each image of one molecule captured at different angles with respect to the x-, y-, and z-axes using DeepSnap-DL. The medians of each of these predicted values were used as the representative values for target molecules as previously reported [\\n]. The performance of each model in predicting the NR agonists and antagonists was evaluated in terms of the following metrics: area under the curve of receiver operating characteristic curve (ROC_AUC); balanced accuracy (BAC); accuracy (Acc), which is the percentage of correct answers based on the results obtained from the validation dataset and the corresponding labeled dataset; F-measure; and Matthews correlation coefficient (MCC) calculated using JMP Pro 14, which is a statistical discovery software (SAS Institute Inc., Cary, NC, USA), as previously reported [\\n]. These performance metrics are defined as follows:\\nBAC = (sensitivity + specificity)/2, where\\nSensitivity = ΣTPs / (ΣTPs + ΣFNs),\\nSpecificity = ΣTNs / (ΣTNs + ΣFPs),\\nAccuracy = (TP + TN) / (TP + FP + TN + FN),\\nF-measure = 2 × Recall × Precision / (Recall + Precision), where\\nPrecision = TP / (TP + FP),\\nRecall = TP / (TP + FN),\\nwhere TP, FN, TN, and FP denote true positive, false negative, true negative, and false positive, respectively. To determine the optimal cutoff point for the definition of TP, FN, TN, and FP, the method of maximizing sensitivity (1–specificity), which is called the Youden index [\\n], was adopted using JMP Pro software. The index has a value ranging from 0 to 1, where 1 represents maximum effectiveness and 0 represents minimum effectiveness.\\n\\n#### 4.7. Statistical Analysis\\n\\nDifferences in prediction performance of in vitro assays in terms of loss (Val), Acc (Val), and Acc (Test), were analyzed by Tukey–Kramer’s honestly significant difference test with JMP Pro 14 [\\n]. Results with\\n< 0.05 were considered statistically significant.\\n\\n## Supplementary Materials', '## Supplementary Materials\\n\\nThe following are available online at\\nhttps://www.mdpi.com/1420-3049/25/12/2764/s1\\n, Figure S1. DeepSnap-DL procedure, Figure S2. Schematic illustrating how preparation of Tra/Val/Test datasets. Figure S3. Schematic view of GoogeLeNet archirecture. (a) Total layers used in this study. (b) Inception model within the GoogLeNet. Figure S4a. Average Accuracy values in Val and Test datasets in models of 35 NRs agonist/antagonist by the DeepSnap-DL. N = 2. Figure S4b. Average F and BAC values in Test dataset in models of 35 NRs agonist/antagonist by the DeepSnap-DL. N = 2. Figure S5. Comparison of prediction performances among four in vitro assays. (a) Loss in the Val dataset, (b) accuracy in the Val dataset, (c) accuracy in the Test dataset. Table S1: NRs and chemical compounds used in this study.\\n\\n## Author Contributions\\n\\nY.U. initiated and supervised the work, designed the experiments, collected the information about chemical compounds, and edited the manuscript. Y.M. performed the computer analysis and the statistical analysis, and drafted the manuscript. All authors have read and agreed to the published version of the manuscript.\\nThis study was funded in part by grants from the Ministry of Economy, Trade and Industry, AI-SHIPS (AI-based Substances Hazardous Integrated Prediction System), Japan, project (20180314ZaiSei8).\\n\\n## Acknowledgments\\n\\nThe environmental setting was supported by Shunichi Sasaki and Kota Kurosaki.\\n\\n## Conflicts of Interest\\n\\nThe authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.\\n\\n## Abbreviations', '[TABLE CELL] accuracy in the test dataset\\n[TABLE CELL] aryl hydrocarbon receptor\\n[TABLE CELL] adverse outcome pathway\\n[TABLE CELL] androgen receptor\\n[TABLE CELL] area under the curve\\n[TABLE CELL] accuracy in the validation dataset\\n[TABLE CELL] balanced accuracy\\n[TABLE CELL] bisphenol A\\n[TABLE CELL] constitutive androstane receptor\\n[TABLE CELL] convolutional neural network\\n[TABLE CELL] DNA-binding domain\\n[TABLE CELL] deep learning GPU training system\\n[TABLE CELL] deep learning\\n[TABLE CELL] dimethyl sulfoxide\\n[TABLE CELL] estrogen receptor\\n[TABLE CELL] estrogen-related receptor\\n[TABLE CELL] false negative\\n[TABLE CELL] false positive\\n[TABLE CELL] farnesoid X receptor\\n[TABLE CELL] glucocorticoid receptor\\n[TABLE CELL] ligand-binding domain\\n[TABLE CELL] loss in the validation dataset\\n[TABLE CELL] liver X receptor\\n[TABLE CELL] Matthews correlation coefficient\\n[TABLE CELL] machine learning\\n[TABLE CELL] molecular operating environment\\n[TABLE CELL] nuclear receptor\\n[TABLE CELL] peroxisome proliferator-activated receptor\\n[TABLE CELL] progesterone receptor\\n[TABLE CELL] pregane X receptor\\n[TABLE CELL] quantitative high-throughput screening\\n[TABLE CELL] quantitative structure – activity relationship\\n[TABLE CELL] retinoic acid receptor\\n[TABLE CELL] response element\\n[TABLE CELL] retinoid X receptor\\n[TABLE CELL] receiver operating characteristic\\n[TABLE CELL] standard error\\n[TABLE CELL] simplified molecular input line entry system\\n[TABLE CELL] selective steroid receptor modulator\\n[TABLE CELL] true negative\\n[TABLE CELL] Toxicology in the 21st Century\\n[TABLE CELL] true positive\\n[TABLE CELL] thyroid receptor\\n[TABLE CELL] thyroid-stimulating hormone receptor\\n[TABLE CELL] vitamin D receptor\\n- Hall, J.M.; Greco, C.W. Perturbation of Nuclear Hormone Receptors by Endocrine Disrupting Chemicals: Mechanisms and Pathological Consequences of Exposure. Cells 2019 , 9 , 13. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Leso, V.; Ercolano, M.L.; Cioffi, D.L.; Iavicoli, I. Occupational Chemical Exposure and Breast Cancer Risk According to Hormone Receptor Status: A Systematic Review. Cancers (Basel) 2019 , 11 , 1882. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Tarnow, P.; Tralau, T.; Luch, A. Chemical activation of estrogen and aryl hydrocarbon receptor signaling pathways and their interaction in toxicology and metabolism. Expert Opin. Drug Metab. Toxicol. 2019 , 15 , 219–229. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- McArdle, M.E.; Freeman, E.L.; Staveley, J.P.; Ortego, L.S.; Coady, K.K.; Weltje, L.; Weyers, A.; Wheeler, J.R.; Bone, A.J. Critical Review of Read-Across Potential in Testing for Endocrine-Related Effects in Vertebrate Ecological Receptors. Environ. Toxicol. Chem. 2020 , 39 , 739–753. [ Google Scholar ] [ CrossRef ]\\n- Mansouri, K.; Kleinstreuer, N.; Abdelaziz, A.M.; Alberga, D.; Alves, V.M.; Andersson, P.L.; Andrade, C.H.; Bai, F.; Balabin, I.; Ballabio, D.; et al. CoMPARA: Collaborative Modeling Project for Androgen Receptor Activity. Environ. Health Perspect. 2020 , 128 , 27002. [ Google Scholar ] [ CrossRef ]\\n- Grimaldi, M.; Boulahtouf, A.; Delfosse, V.; Thouennon, E.; Bourguet, W.; Balaguer, P. Reporter Cell Lines for the Characterization of the Interactions between Human Nuclear Receptors and Endocrine Disruptors. Front. Endocrinol. (Lausanne) 2015 , 6 , 62. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Dallel, S.; Tauveron, I.; Brugnon, F.; Baron, S.; Lobaccaro, J.M.A.; Maqdasy, S. Liver X Receptors: A Possible Link between Lipid Disorders and Female Infertility. Int. J. Mol. Sci. 2018 , 19 , 2177. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Mazaira, G.I.; Zgajnar, N.R.; Lotufo, C.M.; Daneri-Becerra, C.; Sivils, J.C.; Soto, O.B.; Cox, M.B.; Galigniana, M.D. The Nuclear Receptor Field: A Historical Overview and Future Challenges. Nucl. Receptor Res. 2018 , 5 , 101320. [ Google Scholar ] [ CrossRef ]\\n- Watanabe, M.; Kakuta, H. Retinoid X Receptor Antagonists. Int. J. Mol. Sci. 2018 , 19 , 2354. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Jackson, E.N.; Thatcher, S.E.; Larian, N.; English, V.; Soman, S.; Morris, A.J.; Weng, J.; Stromberg, A.; Swanson, H.I.; Pearson, K.; et al. Effects of Aryl Hydrocarbon Receptor Deficiency on PCB-77-Induced Impairment of Glucose Homeostasis during Weight Loss in Male and Female Obese Mice. Environ. Health Perspect. 2019 , 127 , 77004. [ Google Scholar ] [ CrossRef ]\\n- Meijer, F.A.; Leijten-van de Gevel, I.A.; de Vries, R.M.J.M.; Brunsveld, L. Allosteric small molecule modulators of nuclear receptors. Mol. Cell Endocrinol. 2019 , 485 , 20–34. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Saha, T.; Makar, S.; Swetha, R.; Gutti, G.; Singh, S.K. Estrogen signaling: An emanating therapeutic target for breast cancer treatment. Eur. J. Med. Chem. 2019 , 177 , 116–143. [ Google Scholar ] [ CrossRef ] [ PubMed ]', '- Saha, T.; Makar, S.; Swetha, R.; Gutti, G.; Singh, S.K. Estrogen signaling: An emanating therapeutic target for breast cancer treatment. Eur. J. Med. Chem. 2019 , 177 , 116–143. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Fischer, A.; Smieško, M. Ligand Pathways in Nuclear Receptors. J. Chem. Inf. Model. 2019 , 59 , 3100–3109. [ Google Scholar ] [ CrossRef ]\\n- Weikum, E.R.; Liu, X.; Ortlund, E.A. The nuclear receptor superfamily: A structural perspective. Protein Sci. 2018 , 27 , 1876–1892. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Veras Ribeiro Filho, H.; Tambones, I.L.; Mariano Gonçalves Dias, M.; Bernardi Videira, N.; Bruder, M.; Amorim Amato, A.; Migliorini Figueira, A.C. Modulation of nuclear receptor function: Targeting the protein-DNA interface. Mol. Cell Endocrinol. 2019 , 484 , 1–14. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Tecalco-Cruz, A.C. Molecular pathways involved in the transport of nuclear receptors from the nucleus to cytoplasm. J. Steroid Biochem. Mol. Biol. 2018 , 178 , 36–44. [ Google Scholar ] [ CrossRef ]\\n- Baker, J.D.; Ozsan, I.; Rodriguez Ospina, S.; Gulick, D.; Blair, L.J. Hsp90 Heterocomplexes Regulate Steroid Hormone Receptors: From Stress Response to Psychiatric Disease. Int. J. Mol. Sci. 2018 , 20 , 79. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Gabler, M.; Kramer, J.; Schmidt, J.; Pollinger, J.; Weber, J.; Kaiser, A.; Löhr, F.; Proschak, E.; Schubert-Zsilavecz, M.; Merk, D. Allosteric modulation of the farnesoid X receptor by a small molecule. Sci. Rep. 2018 , 8 , 6846. [ Google Scholar ] [ CrossRef ]\\n- Azhagiya Singam, E.R.; Tachachartvanich, P.; La Merrill, M.A.; Smith, M.T.; Durkin, K.A. Structural Dynamics of Agonist and Antagonist Binding to the Androgen Receptor. J. Phys. Chem. B 2019 , 123 , 7657–7666. [ Google Scholar ] [ CrossRef ]\\n- D’Aniello, E.; Iannotti, F.A.; Falkenberg, L.G.; Martella, A.; Gentile, A.; De Maio, F.; Ciavatta, M.L.; Gavagnin, M.; Waxman, J.S.; Di Marzo, V.; et al. In Silico Identification and Experimental Validation of (-)-Muqubilin A, a Marine Norterpene Peroxide, as PPARα/γ-RXRα Agonist and RARα Positive Allosteric Modulator. Mar. Drugs 2019 , 17 , 110. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Fay, K.A.; Villeneuve, D.L.; Swintek, J.; Edwards, S.W.; Nelms, M.D.; Blackwell, B.R.; Ankley, G.T. Differentiating Pathway-Specific From Nonspecific Effects in High-Throughput Toxicity Data: A Foundation for Prioritizing Adverse Outcome Pathway Development. Toxicol. Sci. 2018 , 163 , 500–515. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Clippinger, A.J.; Allen, D.; Behrsing, H.; BéruBé, K.A.; Bolger, M.B.; Casey, W.; DeLorme, M.; Gaça, M.; Gehen, S.C.; Glover, K.; et al. Pathway-based predictive approaches for non-animal assessment of acute inhalation toxicity. Toxicol. In Vitr. 2018 , 52 , 131–145. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Dal Negro, G.; Eskes, C.; Belz, S.; Bertein, C.; Chlebus, M.; Corvaro, M.; Corvi, R.; Dhalluin, S.; Halder, M.; Harvey, J.; et al. One science-driven approach for the regulatory implementation of alternative methods: A multi-sector perspective. Regul. Toxicol. Pharmacol. 2018 , 99 , 33–49. [ Google Scholar ] [ CrossRef ]\\n- Sewell, F.; Gellatly, N.; Beaumont, M.; Burden, N.; Currie, R.; de Haan, L.; Hutchinson, T.H.; Jacobs, M.; Mahony, C.; Malcomber, I.; et al. The future trajectory of adverse outcome pathways: A commentary. Arch. Toxicol. 2018 , 92 , 1657–1661. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Terron, A.; Bennekou, S.H. Towards a regulatory use of alternative developmental neurotoxicity testing (DNT). Toxicol. Appl. Pharmacol. 2018 , 354 , 19–23. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Prior, H.; Casey, W.; Kimber, I.; Whelan, M.; Sewell, F. Reflections on the progress towards non-animal methods for acute toxicity testing of chemicals. Regul. Toxicol. Pharmacol. 2019 , 102 , 30–33. [ Google Scholar ] [ CrossRef ]\\n- Thomas, R.S.; Bahadori, T.; Buckley, T.J.; Cowden, J.; Deisenroth, C.; Dionisio, K.L.; Frithsen, J.B.; Grulke, C.M.; Gwinn, M.R.; Harrill, J.A.; et al. The Next Generation Blueprint of Computational Toxicology at the U.S. Environmental Protection Agency. Toxicol. Sci. 2019 , 169 , 317–332. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Kavlock, R.; Chandler, K.; Houck, K.; Hunter, S.; Judson, R.; Kleinstreuer, N.; Knudsen, T.; Martin, M.; Padilla, S.; Reif, D.; et al. Update on EPA’s ToxCast program: Providing high throughput decision support tools for chemical risk management. Chem. Res. Toxicol. 2012 , 25 , 1287–1302. [ Google Scholar ] [ CrossRef ]\\n- Judson, R.; Houck, K.; Martin, M.; Knudsen, T.; Thomas, R.S.; Sipes, N.; Shah, I.; Wambaugh, J.; Crofton, K. In vitro and modelling approaches to risk assessment from the U.S. Environmental Protection Agency ToxCast programme. Basic Clin. Pharmacol. Toxicol. 2014 , 115 , 69–76. [ Google Scholar ] [ CrossRef ] [ Green Version ]', '- Kleinstreuer, N.C.; Yang, J.; Berg, E.L.; Knudsen, T.B.; Richard, A.M.; Martin, M.T.; Reif, D.M.; Judson, R.S.; Polokoff, M.; Dix, D.J.; et al. Phenotypic screening of the ToxCast chemical library to classify toxic and therapeutic mechanisms. Nat. Biotechnol. 2014 , 32 , 583–591. [ Google Scholar ] [ CrossRef ]\\n- Tice, R.R.; Austin, C.P.; Kavlock, R.J.; Bucher, J.R. Improving the human hazard characterization of chemicals: A Tox21 update. Environ. Health Perspect. 2013 , 121 , 756–765. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Judson, R.S.; Magpantay, F.M.; Chickarmane, V.; Haskell, C.; Tania, N.; Taylor, J.; Xia, M.; Huang, R.; Rotroff, D.M.; Filer, D.L.; et al. Integrated Model of Chemical Perturbations of a Biological Pathway Using 18 In Vitro High-Throughput Screening Assays for the Estrogen Receptor. Toxicol. Sci. 2015 , 148 , 137–154. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Fourches, D.; Ash, J. 4D-quantitative structure-activity relationship modeling: Making a comeback. Expert Opin. Drug Discov. 2019 , 14 , 1227–1235. [ Google Scholar ] [ CrossRef ]\\n- Hisaki, T.; Kaneko, M.A.N.; Hirota, M.; Matsuoka, M.; Kouzuki, H. Integration of read-across and artificial neural network-based QSAR models for predicting systemic toxicity: A case study for valproic acid. J. Toxicol. Sci. 2020 , 45 , 95–108. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Li, X.; Kleinstreuer, N.C.; Fourches, D. Hierarchical Quantitative Structure-Activity Relationship Modeling Approach for Integrating Binary, Multiclass, and Regression Models of Acute Oral Systemic Toxicity. Chem. Res. Toxicol. 2020 , 33 , 353–366. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Ruiz, I.L.; Gómez-Nieto, M.Á. Building Highly Reliable Quantitative Structure-Activity Relationship Classification Models Using the Rivality Index Neighborhood Algorithm with Feature Selection. J. Chem. Inf. Model. 2020 , 60 , 133–151. [ Google Scholar ] [ CrossRef ]\\n- Santos, K.L.B.D.; Cruz, J.N.; Silva, L.B.; Ramos, R.S.; Neto, M.F.A.; Lobato, C.C.; Ota, S.S.B.; Leite, F.H.A.; Borges, R.S.; Silva, C.H.T.P.D.; et al. Identification of Novel Chemical Entities for Adenosine Receptor Type 2A Using Molecular Modeling Approaches. Molecules 2020 , 25 , 1245. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Danishuddin Khan, A.U. Descriptors and their selection methods in QSAR analysis: Paradigm for drug design. Drug Discov. Today 2016 , 21 , 1291–1302. [ Google Scholar ] [ CrossRef ]\\n- Dutt, R.; Madan, A.K. Development and application of novel molecular descriptors for predicting biological activity. Med. Chem. Res. 2017 , 26 , 1988–2006. [ Google Scholar ] [ CrossRef ]\\n- Idakwo, G.; Thangapandian, S.; Luttrell, J., 4th; Zhou, Z.; Zhang, C.; Gong, P. Deep Learning-Based Structure-Activity Relationship Modeling for Multi-Category Toxicity Classification: A Case Study of 10K Tox21 Chemicals With High-Throughput Cell-Based Androgen Receptor Bioassay Data. Front. Physiol. 2019 , 10 , 1044. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Uesawa, Y. Quantitative structure-activity relationship analysis using deep learning based on a novel molecular image input technique. Bioorg. Med. Chem. Lett. 2018 , 28 , 3400–3403. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Mayr, A.; Klambauer, G.; Unterthiner, T.; Hochreiter, S. DeepTox: Toxicity Prediction using Deep Learning. Front. Environ. Sci. 2016 , 3 , 80. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Wu, K.; Wei, G.W. Quantitative Toxicity Prediction Using Topology Based Multitask Deep Neural Networks. J. Chem. Inf. Model. 2018 , 58 , 520–531. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Matsuzaka, Y.; Uesawa, Y. Optimization of a Deep-Learning Method Based on the Classification of Images Generated by Parameterized Deep Snap a Novel Molecular-Image-Input Technique for Quantitative Structure-Activity Relationship (QSAR) Analysis. Front. Bioeng. Biotechnol. 2019 , 7 , 65. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Matsuzaka, Y.; Uesawa, Y. Prediction Model with High-Performance Constitutive Androstane Receptor (CAR) Using DeepSnap-Deep Learning Approach from the Tox21 10K Compound Library. Int. J. Mol. Sci. 2019 , 20 , 4855. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Matsuzaka, Y.; Uesawa, Y. DeepSnap-Deep Learning Approach Predicts Progesterone Receptor Antagonist Activity with High Performance. Front. Bioeng. Biotechnol. 2020 , 7 , 485. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Matsuzaka, Y.; Hosaka, T.; Ogaito, A.; Yoshinari, K.; Uesawa, Y. Prediction Model of Aryl Hydrocarbon Receptor Activation by a Novel QSAR Approach, DeepSnap-Deep Learning. Deep Learning. Molecules 2020 , 25 , 1317. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Available online: https://tripod.nih.gov/tox21/challenge/index.jsp (accessed on 12 June 2020).', '- Available online: https://tripod.nih.gov/tox21/challenge/index.jsp (accessed on 12 June 2020).\\n- Mayr, A.; Klambauer, G.; Unterthiner, T.; Steijaert, M.; Wegner, J.K.; Ceulemans, H.; Clevert, D.A.; Hochreiter, S. Large-scale comparison of machine learning methods for drug target prediction on ChEMBL. Chem. Sci. 2018 , 9 , 5441–5451. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Xu, Y.; Pei, J.; Lai, L. Deep Learning Based Regression and Multiclass Models for Acute Oral Toxicity Prediction with Automatic Chemical Feature Extraction. J. Chem. Inf. Model. 2017 , 57 , 2672–2685. [ Google Scholar ] [ CrossRef ]\\n- Ambe, K.; Ishihara, K.; Ochibe, T.; Ohya, K.; Tamura, S.; Inoue, K.; Yoshida, M.; Tohkin, M. In Silico Prediction of Chemical-Induced Hepatocellular Hypertrophy Using Molecular Descriptors. Toxicol. Sci. 2018 , 162 , 667–675. [ Google Scholar ] [ CrossRef ]\\n- Fernandez, M.; Ban, F.; Woo, G.; Hsing, M.; Yamazaki, T.; LeBlanc, E.; Rennie, P.S.; Welch, W.J.; Cherkasov, A. Toxic Colors: The Use of Deep Learning for Predicting Toxicity of Compounds Merely from Their Graphic Images. J. Chem. Inf. Model. 2018 , 58 , 1533–1543. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Liu, R.; Madore, M.; Glover, K.P.; Feasel, M.G.; Wallqvist, A. Assessing Deep and Shallow Learning Methods for Quantitative Prediction of Acute Chemical Toxicity. Toxicol. Sci. 2018 , 164 , 512–526. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Huo, Y.; Terry, J.G.; Wang, J.; Nath, V.; Bermudez, C.; Bao, S.; Parvathaneni, P.; Carr, J.J.; Landman, B.A. Coronary Calcium Detection using 3D Attention Identical Dual Deep Network Based on Weakly Supervised Learning. Proc. SPIE Int. Soc. Opt. Eng. 2019 , 10949 , 1094917. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Adebayo, J.; Gilmer, J.; Muelly, M.; Goodfellow, I.; Hardt, M.; Kim, B. Sanity Checks for Saliency Maps. arXiv 2018 , arXiv:1810.03292v2. Available online: https://arxiv.org/abs/1810.03292 (accessed on 12 June 2020).\\n- Fiosina, J.; Fiosins, M.; Bonn, S. Explainable Deep Learning for Augmentation of Small RNA Expression Profiles. J. Comput. Biol. 2019 , 27 , 234–247. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Böhle, M.; Eitel, F.; Weygandt, M.; Ritter, K. Layer-Wise Relevance Propagation for Explaining Deep Neural Network Decisions in MRI-Based Alzheimer’s Disease Classification. Front. Aging Neurosci. 2019 , 11 , 194. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Smilkov, D.; Thorat, N.; Kim, B.; Viégas, F.; Wattenberg, M. SmoothGrad: Removing noise by adding noise. arXiv 2017 , arXiv:1706.03825v1. Available online: https://arxiv.org/abs/1706.03825 (accessed on 12 June 2020).\\n- Goh, G.S.W.; Lapuschkin, S.; Weber, L.; Samek, W.; Binder, A. Understanding Integrated Gradients with SmoothTaylor for Deep Neural Network Attribution. arXiv 2020 , arXiv:2004.10484v1. Available online: https://arxiv.org/abs/2004.10484 (accessed on 12 June 2020).\\n- Paul-Friedman, K.; Martin, M.; Crofton, K.M.; Hsu, C.W.; Sakamuru, S.; Zhao, J.; Xia, M.; Huang, R.; Stavreva, D.A.; Soni, V.; et al. Limited Chemical Structural Diversity Found to Modulate Thyroid Hormone Receptor in the Tox21 Chemical Library. Environ. Health Perspect. 2019 , 127 , 97009. [ Google Scholar ] [ CrossRef ]\\n- Zhang, J.; Li, T.; Wang, T.; Yuan, C.; Zhong, S.; Guan, T.; Li, Z.; Wang, Y.; Yu, H.; Luo, Q.; et al. Estrogenicity of halogenated bisphenol A: In vitro and in silico investigations. Arch. Toxicol. 2018 , 92 , 1215–1223. [ Google Scholar ] [ CrossRef ]\\n- Smirnova, O.V. Competitive Agonists and Antagonists of Steroid Nuclear Receptors: Evolution of the Concept or Its Reversal. Biochemistry (Moscow) 2015 , 80 , 1227–1234. [ Google Scholar ] [ CrossRef ]\\n- Liu, S.; Han, S.J.; Smith, C.L. Cooperative activation of gene expression by agonists and antagonists mediated by estrogen receptor heteroligand dimer complexes. Mol. Pharmacol. 2013 , 83 , 1066–1077. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Dotzlaw, H.; Papaioannou, M.; Moehren, U.; Claessens, F.; Baniahmad, A. Agonist-antagonist induced coactivator and corepressor interplay on the human androgen receptor. Mol. Cell Endocrinol. 2003 , 213 , 79–85. [ Google Scholar ] [ CrossRef ]\\n- Arnal, J.F.; Lenfant, F.; Metivier, R.; Flouriot, G.; Henrion, D.; Adlanmerini, M.; Fontaine, C.; Gourdy, P.; Chambon, P.; Katzenellenbogen, B.; et al. Membrane and Nuclear Estrogen Receptor Alpha Actions: From Tissue Specificity to Medical Implications. Physiol. Rev. 2017 , 97 , 1045–1087. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Gustafsson, K.L.; Farman, H.; Henning, P.; Lionikaite, V.; Movérare-Skrtic, S.; Wu, J.; Ryberg, H.; Koskela, A.; Gustafsson, J.Å.; Tuukkanen, J.; et al. The role of membrane ERα signaling in bone and other major estrogen responsive tissues. Sci. Rep. 2016 , 6 , 29473. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]', '- Furuya, K.; Yamamoto, N.; Ohyabu, Y.; Morikyu, T.; Ishige, H.; Albers, M.; Endo, Y. Mechanism of the tissue-specific action of the selective androgen receptor modulator S-101479. Biol. Pharm. Bull. 2013 , 36 , 442–451. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Arao, Y.; Hamilton, K.J.; Ray, M.K.; Scott, G.; Mishina, Y.; Korach, K.S. Estrogen receptor α AF-2 mutation results in antagonist reversal and reveals tissue selective function of estrogen receptor modulators. Proc. Natl. Acad. Sci. USA 2011 , 108 , 14986–14991. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Spillman, M.A.; Manning, N.G.; Dye, W.W.; Sartorius, C.A.; Post, M.D.; Harrell, J.C.; Jacobsen, B.M.; Horwitz, K.B. Tissue-specific pathways for estrogen regulation of ovarian cancer growth and metastasis. Cancer Res. 2010 , 70 , 8927–8936. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Schoch, G.A.; D’Arcy, B.; Stihle, M.; Burger, D.; Bär, D.; Benz, J.; Thoma, R.; Ruf, A. Molecular switch in the glucocorticoid receptor: Active and passive antagonist conformations. J. Mol. Biol. 2010 , 395 , 568–577. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Titus, S.; Neumann, S.; Zheng, W.; Southall, N.; Michael, S.; Klumpp, C.; Yasgar, A.; Shinn, P.; Thomas, C.J.; Inglese, J.; et al. Quantitative high-throughput screening using a live-cell cAMP assay identifies small-molecule agonists of the TSH receptor. J. Biomol. Screen 2008 , 13 , 120–127. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Huang, R.; Sakamuru, S.; Martin, M.T.; Reif, D.M.; Judson, R.S.; Houck, K.A.; Casey, W.; Hsieh, J.H.; Shockley, K.R.; Ceger, P.; et al. Profiling of the Tox21 10K compound library for agonists and antagonists of the estrogen receptor alpha signaling pathway. Sci. Rep. 2014 , 4 , 5664. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Huang, R.; Xia, M.; Cho, M.H.; Sakamuru, S.; Shinn, P.; Houck, K.A.; Dix, D.J.; Judson, R.S.; Witt, K.L.; Kavlock, R.J.; et al. Chemical genomics profiling of environmental chemical modulation of human nuclear receptors. Environ. Health Perspect. 2011 , 119 , 1142–1148. [ Google Scholar ] [ CrossRef ]\\n- Chen, Y.; Sakamuru, S.; Huang, R.; Reese, D.H.; Xia, M. Identification of compounds that modulate retinol signaling using a cell-based qHTS assay. Toxicol. In Vitro 2016 , 32 , 287–296. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Huang, R.; Xia, M.; Sakamuru, S.; Zhao, J.; Shahane, S.A.; Attene-Ramos, M.; Zhao, T.; Austin, C.P.; Simeonov, A. Modelling the Tox21 10 K chemical profiles for in vivo toxicity prediction and mechanism characterization. Nat. Commun. 2016 , 7 , 10425. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Lynch, C.; Zhao, J.; Wang, H.; Xia, M. Quantitative High-Throughput Luciferase Screening in Identifying CAR Modulators. Methods Mol. Biol. 2016 , 1473 , 33–42. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Teng, C.T.; Hsieh, J.H.; Zhao, J.; Huang, R.; Xia, M.; Martin, N.; Gao, X.; Dixon, D.; Auerbach, S.S.; Witt, K.L.; et al. Development of Novel Cell Lines for High-Throughput Screening to Detect Estrogen-Related Receptor Alpha Modulators. SLAS Discov. 2017 , 22 , 720–731. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Huang, R.; Xia, M.; Sakamuru, S.; Zhao, J.; Lynch, C.; Zhao, T.; Zhu, H.; Austin, C.P.; Simeonov, A. Expanding biological space coverage enhances the prediction of drug adverse effects in human using in vitro activity profiles. Sci. Rep. 2018 , 8 , 3783. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Lynch, C.; Zhao, J.; Huang, R.; Kanaya, N.; Bernal, L.; Hsieh, J.H.; Auerbach, S.S.; Witt, K.L.; Merrick, B.A.; Chen, S.; et al. Identification of Estrogen-Related Receptor α Agonists in the Tox21 Compound Library. Endocrinology 2018 , 159 , 744–753. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Lynch, C.; Zhao, J.; Sakamuru, S.; Zhang, L.; Huang, R.; Witt, K.L.; Merrick, B.A.; Teng, C.T.; Xia, M. Identification of Compounds That Inhibit Estrogen-Related Receptor Alpha Signaling Using High-Throughput Screening Assays. Molecules 2019 , 24 , 841. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Lynch, C.; Mackowiak, B.; Huang, R.; Li, L.; Heyward, S.; Sakamuru, S.; Wang, H.; Xia, M. Identification of Modulators That Activate the Constitutive Androstane Receptor From the Tox21 10K Compound Library. Toxicol. Sci. 2019 , 167 , 282–292. [ Google Scholar ] [ CrossRef ]\\n- Wei, Z.; Sakamuru, S.; Zhang, L.; Zhao, J.; Huang, R.; Kleinstreuer, N.C.; Chen, Y.; Shu, Y.; Knudsen, T.B.; Xia, M. Identification and Profiling of Environmental Chemicals That Inhibit the TGFβ/SMAD Signaling Pathway. Chem. Res. Toxicol. 2019 , 32 , 2433–2444. [ Google Scholar ] [ CrossRef ]', '- Xia, M.; Huang, R.; Guo, V.; Southall, N.; Cho, M.H.; Inglese, J.; Austin, C.P.; Nirenberg, M. Identification of compounds that potentiate CREB signaling as possible enhancers of long-term memory. Proc. Natl. Acad. Sci. USA 2009 , 106 , 2412–2417. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Inglese, J.; Auld, D.S.; Jadhav, A.; Johnson, R.L.; Simeonov, A.; Yasgar, A.; Zheng, W.; Austin, C.P. Quantitative high-throughput screening: A titration-based approach that efficiently identifies biological activities in large chemical libraries. Proc. Natl. Acad. Sci. USA 2006 , 103 , 11473–11478. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Wang, Y.; Jadhav, A.; Southal, N.; Huang, R.; Nguyen, D.T. A grid algorithm for high throughput fitting of dose-response curve data. Curr. Chem. Genomics 2010 , 4 , 57–66. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Molecular Networks GmbH, Nürnberg, Germany. Available online: https://www.mn-am.com/products/corina (accessed on 12 June 2020).\\n- Available online: http://image-net.org/challenges/LSVRC/2012/browse-synsets (accessed on 12 June 2020).\\n- Available online: http://www.image-net.org/index (accessed on 12 June 2020).\\n- Szegedy, C.; Liu, W.; Jia, Y.; Sermanet, Y.; Reed, S.; Anguelov, D.; Erhan, D.; Vanhoucke, V.; Rabinovich, A. Going Deeper with Convolutions. arXiv 2014 , arXiv:1409.4842v1. [ Google Scholar ]\\n- Yang, Y.; Yan, L.F.; Zhang, X.; Han, Y.; Nan, H.Y.; Hu, Y.C.; Hu, B.; Yan, S.L.; Zhang, J.; Cheng, D.L.; et al. Glioma Grading on Conventional MR Images: A Deep Learning Study With Transfer Learning. Front. Neurosci. 2018 , 12 , 804. [ Google Scholar ] [ CrossRef ] [ PubMed ] [ Green Version ]\\n- Kim, J.Y.; Lee, H.E.; Choi, Y.H.; Lee, S.J.; Jeon, J.S. CNN-based diagnosis models for canine ulcerative keratitis. Sci. Rep. 2019 , 9 , 14209. [ Google Scholar ] [ CrossRef ] [ PubMed ]\\n- Yun, J.H.; Chun, S.M.; Kim, J.C.; Shin, H.I. Obesity cutoff values in Korean men with motor complete spinal cord injury: Body mass index and waist circumference. Spinal Cord 2019 , 57 , 110–116. [ Google Scholar ] [ CrossRef ]\\n- Liang, K.; Wang, C.; Yan, F.; Wang, L.; He, T.; Zhang, X.; Li, C.; Yang, W.; Ma, Z.; Ma, A.; et al. HbA1c Cutoff Point of 5.9% Better Identifies High Risk of Progression to Diabetes among Chinese Adults: Results from a Retrospective Cohort Study. J. Diabetes Res. 2018 , 2018 , 7486493. [ Google Scholar ] [ CrossRef ] [ Green Version ]\\n- Available online: https://www.jmp.com/support/help/en/15.1/index.shtml#page/jmp/example-of-all-pairs-tukey-hsd-test.shtml (accessed on 12 June 2020).\\n[TABLE CELL] Sample Availability: Samples of the compounds are available from the authors.\\nActivity distribution of the Tox21 10K library against 35 NR agonists and antagonists used in the DeepSnap-deep learning (DL) approach. (\\n) Number of chemical compounds used in the modeling by the DeepSnap-DL, where orange and blue indicate active and inactive chemicals, respectively. (\\n) Percentage of active chemicals against total chemicals.\\n<p><a class=\"html-figpopup\" href=\"#fig_body_display_molecules-25-02764-f001\">\\n Click here to enlarge figure\\n </a></p>\\nActivity distribution of the Tox21 10K library against 35 NR agonists and antagonists used in the DeepSnap-deep learning (DL) approach. (\\n) Number of chemical compounds used in the modeling by the DeepSnap-DL, where orange and blue indicate active and inactive chemicals, respectively. (\\n) Percentage of active chemicals against total chemicals.\\nRepresentative molecular images used in the DeepSnap-DL method. The parentheses below each image indicates the angles of depiction in DeepSnap.\\n<p><a class=\"html-figpopup\" href=\"#fig_body_display_molecules-25-02764-f002\">\\n Click here to enlarge figure\\n </a></p>\\nRepresentative molecular images used in the DeepSnap-DL method. The parentheses below each image indicates the angles of depiction in DeepSnap.\\nAverage Loss values in the validation (Val) dataset in models of 35 NR agonists and antagonists constructed by DeepSnap-DL.\\n= 2. Each bar indicates average of Loss (Val) ± standard error.\\n<p><a class=\"html-figpopup\" href=\"#fig_body_display_molecules-25-02764-f003\">\\n Click here to enlarge figure\\n </a></p>\\nAverage Loss values in the validation (Val) dataset in models of 35 NR agonists and antagonists constructed by DeepSnap-DL.\\n= 2. Each bar indicates average of Loss (Val) ± standard error.\\nAverage Matthews correlation coefficient (MCC) (\\n) and area under the curve (AUC) (\\n) values in the Test dataset in the models of 35 NR agonists and antagonists constructed by DeepSnap-DL.\\n= 2. Each bar indicates average MCC and AUC ± standard error.\\n<p><a class=\"html-figpopup\" href=\"#fig_body_display_molecules-25-02764-f004\">\\n Click here to enlarge figure\\n </a></p>\\nAverage Matthews correlation coefficient (MCC) (\\n) and area under the curve (AUC) (\\n) values in the Test dataset in the models of 35 NR agonists and antagonists constructed by DeepSnap-DL.', 'Click here to enlarge figure\\n </a></p>\\nAverage Matthews correlation coefficient (MCC) (\\n) and area under the curve (AUC) (\\n) values in the Test dataset in the models of 35 NR agonists and antagonists constructed by DeepSnap-DL.\\n= 2. Each bar indicates average MCC and AUC ± standard error.\\nRepresentative area under the curve of receiver operating characteristic curve (ROC_AUC) in the models of 35 NR agonists and antagonists constructed by DeepSnap-DL.\\n<p><a class=\"html-figpopup\" href=\"#fig_body_display_molecules-25-02764-f005\">\\n Click here to enlarge figure\\n </a></p>\\nRepresentative area under the curve of receiver operating characteristic curve (ROC_AUC) in the models of 35 NR agonists and antagonists constructed by DeepSnap-DL.\\nComparison of prediction performance among four in vitro assays. (\\n) Loss in the Val dataset, (\\n) accuracy in the Val dataset, (\\n) accuracy in the Test dataset.\\n= 14, 17, 3, and 1 for luciferase, beta-lactamase, cAMP, and intracellular calcium assays, respectively. Each bar indicates the average of the performance metric of the four in vitro assays with standard error. *\\n< 0.05 by Tukey–Kramer’s honestly significant difference test.\\n<p><a class=\"html-figpopup\" href=\"#fig_body_display_molecules-25-02764-f006\">\\n Click here to enlarge figure\\n </a></p>\\nComparison of prediction performance among four in vitro assays. (\\n) Loss in the Val dataset, (\\n) accuracy in the Val dataset, (\\n) accuracy in the Test dataset.\\n= 14, 17, 3, and 1 for luciferase, beta-lactamase, cAMP, and intracellular calcium assays, respectively. Each bar indicates the average of the performance metric of the four in vitro assays with standard error. *\\n< 0.05 by Tukey–Kramer’s honestly significant difference test.\\nNuclear receptors (NRs) and their bioassays used in this study.\\nNuclear receptors (NRs) and their bioassays used in this study.\\n[TABLE CELL] PubChem AID\\n[TABLE CELL] Model Names\\n[TABLE CELL] Reporter Gene Assay\\n[TABLE CELL] Agonist/Antagonist\\n[TABLE CELL] Positive Control\\n[TABLE CELL] glucocorticoid receptor\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] Dexamethasone\\n[TABLE CELL] glucocorticoid receptor\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] Dexamethasone\\n[TABLE CELL] Mifeprostone\\n[TABLE CELL] androgen receptor\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] androgen receptor\\n[TABLE CELL] androgen receptor\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] Cyproterone acetate\\n[TABLE CELL] thyroid receptor\\n[TABLE CELL] estrogen receptor alpha\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] 17beta-estradiol\\n[TABLE CELL] estrogen receptor alpha\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] 17beta-estradiol\\n[TABLE CELL] 4-Hydroxy tamoxifen\\n[TABLE CELL] estrogen receptor alpha\\n[TABLE CELL] 17beta-estradiol\\n[TABLE CELL] 4-Hydroxy tamoxifen\\n[TABLE CELL] aryl hydrocarbon receptor\\n[TABLE CELL] peroxisome proliferator-activated receptor gamma\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] Rosiglitazone\\n[TABLE CELL] peroxisome proliferator-activated receptor delta\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] peroxisome proliferator-activated receptor delta\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] farnesoid-X-receptor\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] Chenodeoxycholic acid\\n[TABLE CELL] farnesoid-X-receptor\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] Chenodeoxycholic acid\\n[TABLE CELL] Guggulsterone\\n[TABLE CELL] vitamin D receptor\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] 1alpha, 25-Dihydroxy Vitamin D3\\n[TABLE CELL] vitamin D receptor\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] 1alpha, 25-Dihydroxy Vitamin D3\\n[TABLE CELL] retinoid-related orphan receptor gamma\\n[TABLE CELL] Doxycycline Hyclate\\n[TABLE CELL] retinoid X nuclear receptor alpha\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] 9-cis retinoic acid\\n[TABLE CELL] retinoic acid receptor\\n[TABLE CELL] constitutive androstane receptor\\n[TABLE CELL] thyroid stimulating hormone receptor\\n[TABLE CELL] thyroid stimulating hormone\\n[TABLE CELL] ARfull2_ant\\n[TABLE CELL] androgen receptor\\n[TABLE CELL] ERfull_estra_ant\\n[TABLE CELL] estrogen receptor alpha\\n[TABLE CELL] 17beta-estradiol\\n[TABLE CELL] 4-Hydroxy tamoxifen\\n[TABLE CELL] androgen receptor\\n[TABLE CELL] estrogen receptor alpha\\n[TABLE CELL] ICI-182,780\\n[TABLE CELL] 17beta-Estradiol\\n[TABLE CELL] thyroid stimulating hormone receptor\\n[TABLE CELL] thyroid stimulating hormone\\n[TABLE CELL] estrogen receptor beta\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] 17beta-Estradiol\\n[TABLE CELL] thyroid stimulating hormone receptor\\n[TABLE CELL] thyroid stimulating hormone\\n[TABLE CELL] estrogen receptor beta\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] 17beta-Estradiol\\n[TABLE CELL] 4-Hydroxy tamoxifen\\n[TABLE CELL] estrogen related receptor\\n[TABLE CELL] estrogen related receptor\\n[TABLE CELL] pregnane X receptor\\n[TABLE CELL] progesterone receptor\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] thyrotropin-releasing hormone receptor\\n[TABLE CELL] intracellular calcium assay\\n[TABLE CELL] thyrotropin-releasing hormone\\nNA: not analyzed.', '[TABLE CELL] pregnane X receptor\\n[TABLE CELL] progesterone receptor\\n[TABLE CELL] beta-lactamase\\n[TABLE CELL] thyrotropin-releasing hormone receptor\\n[TABLE CELL] intracellular calcium assay\\n[TABLE CELL] thyrotropin-releasing hormone\\nNA: not analyzed.\\n© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY) license (\\nhttp://creativecommons.org/licenses/by/4.0/', '## Share and Cite\\n\\nMDPI and ACS Style\\nMatsuzaka, Y.; Uesawa, Y. \\n Molecular Image-Based Prediction Models of Nuclear Receptor Agonists and Antagonists Using the DeepSnap-Deep Learning Approach with the Tox21 10K Library. Molecules 2020 , 25 , 2764.\\n https://doi.org/10.3390/molecules25122764\\nMatsuzaka Y, Uesawa Y. \\n Molecular Image-Based Prediction Models of Nuclear Receptor Agonists and Antagonists Using the DeepSnap-Deep Learning Approach with the Tox21 10K Library. Molecules . 2020; 25(12):2764.\\n https://doi.org/10.3390/molecules25122764\\nChicago/Turabian Style\\nMatsuzaka, Yasunari, and Yoshihiro Uesawa. \\n 2020. \"Molecular Image-Based Prediction Models of Nuclear Receptor Agonists and Antagonists Using the DeepSnap-Deep Learning Approach with the Tox21 10K Library\" Molecules 25, no. 12: 2764.\\n https://doi.org/10.3390/molecules25122764\\nMatsuzaka, Y., & Uesawa, Y. \\n \\n (2020). Molecular Image-Based Prediction Models of Nuclear Receptor Agonists and Antagonists Using the DeepSnap-Deep Learning Approach with the Tox21 10K Library. Molecules , 25 (12), 2764.\\n https://doi.org/10.3390/molecules25122764\\n\\n## Article Metrics\\n\\n### Article Access Statistics\\n\\nFor more information on the journal statistics, click\\nMultiple requests from the same IP address are counted as one view.', '- Open access\\n- Published: 12 May 2020\\n\\n# kGCN: a graph-based deep learning framework for chemical structures\\n\\n- Ryosuke Kojima ORCID: orcid.org/0000-0003-1095-8864 1 ,\\n- Shoichi Ishida 2 ,\\n- Masateru Ohta 3 ,\\n- Hiroaki Iwata 1 ,\\n- Teruki Honma 3 , 4 &\\n- Yasushi Okuno 1 , 3\\nShow authors\\nJournal of Cheminformatics volume 12 , Article\\xa0number: 32 ( 2020 ) Cite this article\\n- 19k Accesses\\n- 83 Citations\\n- 10 Altmetric\\n- Metrics details\\nDeep learning is developing as an important technology to perform various tasks in cheminformatics. In particular, graph convolutional neural networks (GCNs) have been reported to perform well in many types of prediction tasks related to molecules. Although GCN exhibits considerable potential in various applications, appropriate utilization of this resource for obtaining reasonable and reliable prediction results requires thorough understanding of GCN and programming. To leverage the power of GCN to benefit various users from chemists to cheminformaticians, an open-source GCN tool, kGCN, is introduced. To support the users with various levels of programming skills, kGCN includes three interfaces: a graphical user interface (GUI) employing KNIME for users with limited programming skills such as chemists, as well as command-line and Python library interfaces for users with advanced programming skills such as cheminformaticians. To support the three steps required for building a prediction model, i.e., pre-processing, model tuning, and interpretation of results, kGCN includes functions of typical pre-processing, Bayesian optimization for automatic model tuning, and visualization of the atomic contribution to prediction for interpretation of results. kGCN supports three types of approaches, single-task, multi-task, and multi-modal predictions. The prediction of compound-protein interaction for four matrixmetalloproteases, MMP-3, -9, -12 and -13, in the inhibition assays is performed as a representative case study using kGCN. Additionally, kGCN provides the visualization of atomic contributions to the prediction. Such visualization is useful for the validation of the prediction models and the design of molecules based on the prediction model, realizing “explainable AI” for understanding the factors affecting AI prediction. kGCN is available at https://github.com/clinfo .\\n\\n## Introduction', 'Deep learning is emerging as an important technology to perform various tasks in cheminformatics [ 1 , 2 , 3 ]. With the recent development of artificial intelligence (AI) and deep learning, the application of deep learning approaches has been practically demonstrated for various predictions such as virtual screening [ 4 ], quantitative structure-activity relationship (QSAR) studies [ 5 ], and ADMET (absorption, distribution, metabolism elimination, and toxicology) prediction [ 6 , 7 ]. In particular, with the democratization of AI, it is expected that these prediction tools should be readily used by the non-experts. The accessibility of deep learning to non-experts is an important issue in the field of cheminformatics. For example, as deep learning can be applied to a wide range of research areas in drug discovery such as ADMET predictions for lead optimization and virtual screening for lead identification, the chemists should be able to solve these research problems by using the latest technologies and analyze the results, availing the benefits of deep learning. However, as chemists are typically not proficient in deep learning, the development of easy-to-use, multi-functional deep learning software is necessary.\\nIn the predictions based on molecular structures, graph neural networks (GNNs), where a chemical structure is represented as a graph, have been reported to perform well [ 8 , 9 ]. In particular, graph convolutional networks (GCNs), a type of GNN, exhibited excellent performances in many applications [ 10 , 11 ]. Despite these results, an appropriate application of GCN to real-world research problems requires practical programming skills and comprehensive understanding of deep learning and GCN.\\nTo address this issue, a new open-source software, kGCN Footnote 1 . is introduced for various users to employ deep learning including GCNs. kGCN is developed for the following functions:\\n- Providing interfaces for the various levels of users including users with limited programming skills\\n- Handling different types of data for cheminformatics tasks\\n- Easy, intuitive, and convincing interpretation of results\\n- Hyper-parameter optimization\\nArchitecture of kGCN\\nFull size image\\nAs mentioned, one function of kGCN is to afford interfaces to assist various users such as chemists, cheminformaticians, and data scientists. Considering the expertise levels of these users, a software should provide multiple interfaces suitable for each user. To satisfy these requirements, kGCN provides three types of user interfaces. Figure 1 shows the architecture of the kGCN system. The kGCN system supports both GUI-based and command-line interfaces. To intuitively access a machine-learning procedure, the kGCN system provides a GUI interface on the GUI platform, KNIME (Konstanz Information Miner) [ 12 ]. The command-line interface supports typical machine-learning procedures such as training, evaluation, and cross-validation. Additionally, the kGCN modules can be used as a Python library to allow flexibility and processing through programming languages.\\nThe second function is to support different types of data. In cheminformatics, various types of data including chemical structures represented by graphs shoulded be considered. For example, the protein sequence data is often represented as a symbol sequence or vector descriptor. In deep learning, various architectures for neural networks have been proposed [ 13 ]. The simplest GCN is based on the single-graph-input single-label-output architecture. The kGCN system supports 1) multi-input (multi-modal GCN) and 2) multi-output (multi-task GCN) architectures. A multi-modal GCN is a neural network that can accept multiple modalities of inputs [ 14 , 15 ]. kGCN can accommodate a neural network with two inputs: chemical structure as a graph and a protein sequence as a series of characters. This type of neural network can be used to predict interactions between the compounds and proteins for virtual screening and/or drug-repurposing [ 4 , 16 ]. However, multiple related tasks are needed to be simultaneously handled in cheminformatics [ 17 ], for example, tasks to predict multiple different properties of a compound. To tackle these, a multi-task neural network is applied, which affords better results than those for an individual prediction [ 18 , 19 ].', 'The third function is the interpretation and understanding of the cause of prediction results via deep learning by visualizing contributions of input data to the prediction. This process is important because the validity of the prediction model can be examined through a visual inspection of the good and bad features. The refinement or re-construction of the prediction model can be performed if the causes of prediction do not appear to be reasonable or are contrary to common sense. Notably, designing new molecules with improved properties is possible if the reasons for good and/or bad predictions are identified by visualization. In recent years, several methods to calculate the different contributions to the prediction results of deep learning have been proposed [ 20 , 21 ]. The kGCN system uses the integrated gradient method [ 22 ], which can be applied to any type of neural network architectures including multi-task and multi-modal neural networks.\\nThe last function is hyper-parameter optimization. In analysis using deep neural networks, hyper-parameters of deep learning such as the number of network layers, number of layer nodes, learning rate, and batch size should be appropriately set. However, setting these parameters is not easy for users without deep learning knowledge and experience. To assist such users and automatically determine the optimal hyper-parameters, the kGCN system employs Bayesian optimization and metaheuristics for hyper-parameter optimization [ 23 ].\\nIn addition to this information, the kGCN system also provides tools for improving the usability. The kGCN back-end implementation uses Tensorflow [ 24 ] and supports GPUs (graphics processing units). To setup the execution conditions, kGCN-installed Docker images are also provided Footnote 2 . Additional unique tools to enhance the usability are provided for each interface. These will be described in the Implementation section.\\nSimilar types of software have been reported in prior studies, e.g., DeepChem [ 25 ], Chainer chemistry [ 26 ], and OpenChem [ 27 ]. DeepChem is a Python library for neural networks, including GCNs. A notable feature of DeepChem is to support various machine learning methods as well as deep learning methods. Because deep learning usually requires large amounts of data, this feature can help users handle relatively small amounts of data. Chainer chemistry provides GCNs as an extended Python library of Chainer [ 28 ]. Both libraries can be used with Python and were developed for professional programmers of machine learning and Python. Although OpenChem supports both command-line and Python interfaces, good programming skills are still required to use OpenChem. The kGCN system is a framework containing the GUI, command-line, and Python interfaces. The GUI interface of kGCN is expected to engage users with limited programming skills in GCN and deep learning. To our knowledge, kGCN is the first open-source and multi-functional GCN software to support all three interfaces.', \"## Implementation\\n\\nBefore describing the details of the kGCN system, basic implementation techniques for the graph representation of molecules and graph convolution are discussed.\\n\\n### Graph representation of molecules for GCN\\n\\nThis section first describes the formalization of a molecule to apply the GCNs. A molecule is formalized as a tuple \\\\(\\\\mathcal{M} \\\\equiv (V,E,F)\\\\) , where V is a set of nodes. A node represents an atom in a molecule. A node has features \\\\(\\\\mathbf {f}_i \\\\in F (i \\\\in V)\\\\) , and F is a set of feature vectors representing the atom properties such as atom type, formal charge, and hybridization. These features should be appropriately designed by users. E is a set of edges, and an edge \\\\(e \\\\in E\\\\) represents a bond between the atoms, i.e., \\\\(e \\\\in V \\\\times V \\\\times T\\\\) , where T is a set of bond types. An adjacency matrix \\\\(\\\\mathbf {A}^{(t)}\\\\) is used, which is defined as follows:\\n$$\\\\begin{aligned} (\\\\mathbf {A}^{(t)})_{i,j} = {\\\\left\\\\{ \\\\begin{array}{ll} 1 &{} (v_i,v_j,t) \\\\in E \\\\\\\\ 0 &{} (v_i,v_j,t) \\\\notin E \\\\end{array}\\\\right. }, \\\\end{aligned}$$\\nwhere \\\\((\\\\cdot )_{i,j}\\\\) represents the j -th element of i -th row. Similarly, the feature matrix is defined as:\\n$$\\\\begin{aligned} (\\\\mathbf {F})_{j,k}=(\\\\mathbf {f}_j)_k \\\\end{aligned}$$\\nwhere \\\\((\\\\cdot )_{k}\\\\) represents the k -th element of a vector.\\nUsing this matrix, a molecule is represented by \\\\(\\\\mathcal{M'} = (\\\\mathbf {A},\\\\mathbf {F})\\\\) , where \\\\(\\\\mathbf {A} = \\\\{\\\\mathbf {A}^{(t)} | t \\\\in T\\\\}\\\\) . The framework in the present system uses RDKit [ 29 ] to create adjacency and feature matrices and employs \\\\(\\\\mathcal M'\\\\) as the input for GCN.\\n\\n### Graph convolutional network\\n\\nkGCN supports GCNs in addition to the standard feed-forward neural networks. Therefore, GCNs for molecules are described first. Graph convolution layer, graph dense layer, and graph gather layer are defined as described below.\\n\\n#### Graph convolution layer\\n\\nThe graph convolution is calculated from the input \\\\(\\\\mathbf {X^{(\\\\ell )}}\\\\) of the \\\\(\\\\ell\\\\) -th layer as follows:\\n$$\\\\begin{aligned} \\\\mathbf {X}^{(\\\\ell +1)} = \\\\sigma \\\\left( \\\\sum _t \\\\tilde{\\\\mathbf {A}}^{(t)} \\\\mathbf {X}^{(\\\\ell )} \\\\mathbf {W}^{(\\\\ell )}_t \\\\right) , \\\\end{aligned}$$\\nwhere \\\\(\\\\mathbf {X}^{(\\\\ell )}\\\\) is the \\\\(N \\\\times D^{(\\\\ell )}\\\\) matrix and \\\\(\\\\mathbf {W}^{(\\\\ell )}_t\\\\) is the parameter matrix ( \\\\(D^{(\\\\ell )} \\\\times D^{(\\\\ell +1)}\\\\) ) for a bond type t , \\\\(\\\\sigma\\\\) is the activation function, and \\\\(\\\\tilde{\\\\mathbf {A}}^{(t)}\\\\) is the normalized adjacency matrix ( \\\\(N \\\\times N\\\\) ). This normalization and implementation of the layers follows Kipf’s model [ 30 ] as a default. There are various choices for implementing the setting of graph convolution layers. In the kGCN system, the operation of the first layer input can be easily switched by changing the initial setting file for building the model.\\nThe GCN is based on this graph convolution operation. The input of the first layer \\\\(\\\\mathbf {X}^{(1)}\\\\) often corresponds to the feature matrix, \\\\(\\\\mathbf {F}\\\\)\\n\\n#### Graph dense layer\\n\\n\\\\(\\\\mathbf {X^{(\\\\ell )}}\\\\) is an input for graph dense layer. \\\\(\\\\mathbf {X^{(\\\\ell +1)}}\\\\) is calculated as follows:\\n$$\\\\begin{aligned} \\\\mathbf {X}^{\\\\ell +1} = \\\\mathbf {X}^{(\\\\ell )} \\\\mathbf {W}^{(\\\\ell )}, \\\\end{aligned}$$\\nwhere \\\\(\\\\mathbf {X}^{(\\\\ell )}\\\\) is an \\\\(N \\\\times D^{(\\\\ell )}\\\\) matrix and \\\\(\\\\mathbf {W}^{(\\\\ell )}\\\\) is a parameter matrix ( \\\\(D^{(\\\\ell )} \\\\times D^{(\\\\ell +1)}\\\\) ).\\n\\n#### Graph gather layer\", '#### Graph gather layer\\n\\nThis layer converts a graph into a vector [ 31 ], i.e., the input \\\\(\\\\mathbf {X}^{(\\\\ell )}\\\\) is an \\\\(N \\\\times D^{(\\\\ell )}\\\\) matrix and \\\\(\\\\mathbf {X}^{(\\\\ell )}\\\\) , i.e.,\\n$$\\\\begin{aligned} (\\\\mathbf {X}^{(\\\\ell +1)})_{j}=\\\\sum _j (\\\\mathbf {X}^{(\\\\ell )})_{ij}, \\\\end{aligned}$$\\nwhere \\\\((\\\\cdot )_{i}\\\\) represents an i -th element of a vector. This operation converts a matrix into a vector.\\nFigure 2 shows an example of GCN for a prediction task. The GCN model is a neural network consisting of a graph convolutional layer (GraphConv) with batch normalization (BN) [ 32 ] and rectified linear unit (ReLU) activation, graph dense layer with the ReLU activation, graph gather layer, and dense layer with the softmax activation. By assigning the label that is suitable for each task to the compounds, this model can be applied to many types of tasks, e.g., ADMET prediction based on the chemical structures.\\nFigure 3 shows an example of a multi-task GCN for a prediction task. The only difference is that multiple labels are predicted as an output. In this type of neural networks, multiple labels associated with a molecule such as several types of ADMET properties can be predicted simultaneously. It is well-known that multi-task prediction affords more improvement in the performance compared to that of individual single-task prediction [ 33 ].\\nFigure 4 shows an example of a multi-modal neural network employing a graph representing a compound and sequence of a protein. In addition to the information derived from the molecular structure, information from other modalities can also be used for the input. An example of the prediction of activity using compound and protein related information is described in detail in the Experiment section.\\nThe kGCN system supports operations described above and some other additional operations to build a neural network. These operations are implemented using TensorFlow [ 34 ] and are compatible with Keras [ 35 ], allowing the users to construct neural networks such as convolutional neural networks and recurrent neural networks [ 13 ] with Keras operations.\\nThese neural networks include hyper-parameters such as the number of layers in a model and number of dimensions for each layer. To determine these hyper-parameters, the kGCN system includes Bayesian optimization.\\nGraph convolutional network for a prediction task with a compound input\\nFull size image\\nMulti-task graph convolutional network with a compound input\\nFull size image\\nMulti-modal graph convolutional network with compound and sequence inputs\\nFull size image\\n\\n### Visualization of graph convolutional network\\n\\nTo confirm the features of the molecules that influence prediction result, a visualization system using the integrated gradient (IG) method [ 22 ] is developed. After the construction of the prediction model, the visualization of the atom importance in the molecular structure, based on the IG value \\\\(\\\\mathcal{I}(x)\\\\) derived from the prediction model, is possible.\\nIG value \\\\(\\\\mathcal{I}(x)\\\\) is defined as follows:\\n$$\\\\begin{aligned} \\\\mathcal{I}(x) = \\\\frac{x}{M}\\\\sum _{k=1}^M \\\\nabla S\\\\left(\\\\frac{k}{M}x\\\\right), \\\\end{aligned}$$\\nwhere x is the input of an atom of a molecule, M is the number of divisions of the input, S ( x ) is the prediction score, i.e., the neural network output with input x , and \\\\(\\\\nabla S(x)\\\\) is the gradient of S ( x ) related to input x . In the default setting, M is set to 100. The atom importance is defined as the sum of the IG values of features in each atom. The calculation of the atom importance is performed on compound-by-compound basis.\\nThe evaluation of the visualization results depends on each case. Although methods for the visualization of deep learning results are still developing, their effectiveness in solving common problems has not been reported; however, a quantitative evaluation of the IG values related to the molecules was previously reported for the prediction of a reaction [ 36 ].\\n\\n### Hyper-parameter optimization\\n\\nTo optimize the neural network models, hyper-parameters such as the number of graph convolution layers, the number of dense layers, dropout rate, and learning rate should be determined. As it is difficult to manually determine all these hyper-parameters, kGCN allows automatic hyper-parameter optimization with Gaussian-process-based Bayesian optimization using a Python library, GPyOpt [ 37 ].\\nThis section describes three interfaces in the kGCN system.\\n\\n#### Command-line interface', '#### Command-line interface\\n\\nThe kGCN system provides the command-line interface suitable for batch execution. Data processing is designed according to the aim, but there is a standard process common to many data processing designs, e.g., a series of processes for cross-validation. The kGCN commands include these common processes, i.e., the kGCN system allows preprocessing, learning, prediction, cross-validation, and Bayesian optimization using the following commands:\\nkgcn-chem command\\nallows preprocessing of molecule data, e.g., structure-data file (SDF) and SMILES.\\nkgcn command\\nallows batch execution related to prediction tasks: supervised training, prediction, cross-validation, and visualization.\\nkgcn-opt command\\nallows batch execution related to hyper-parameter optimization.\\nThese commands can be used with Linux commands and enable users to construct automatic scripts, e.g., Bash scripts. Because such batch execution is suitable for large-scale experiments using workstation and reproducible experiments, this interface is useful for the evaluation of neural network models.\\n\\n#### KNIME interface\\n\\nSingle-task workflow for the hold-out procedure using the KNIME interface (Upper). Multi-task workflow for the hold-out procedure (Lower)\\nFull size image\\nMulti-modal workflow for the hold-out procedure\\nFull size image\\nThe kGCN system supports KNIME modules as a GUI. KNIME is a platform to prepare the workflow, which consists of KNIME nodes for data processing, and is particularly useful in the field of data science. The kGCN KNIME nodes described below are useful for the execution of various kGCN functions in combination with existing KNIME nodes. The command-line interface allows batch execution, whereas the KNIME interface is suitable for early steps in the machine learning process such as prototyping and data preparation.\\nTo train and evaluate the model, kGCN provides the following two nodes.\\ntrains the model from a given dataset. This node receives the training dataset and provides the trained model as an output. Detailed settings such as batch size and learning rate can be set as the node properties.\\nGCNPredictor\\npredicts the label from a given trained model and new dataset.\\nUsing the kGCN nodes mentioned above, Fig. 5 shows an example of the workflow. This data flow can be separated into that before and after GCNLearner. The former part is for data preparation, for which kGCN includes the following KNIME nodes:\\nCSVLabelExtractor\\nreads labels from a CSV file for training and evaluation\\nreads the molecular information from an SDF.\\nGraphExtractor\\nextracts the graph from each molecule.\\nAtomFeatureExtractor\\nextracts the features from each molecule.\\nGCNDatasetBuilder\\nconstructs the complete dataset by combining input and label data.\\nGCNDatasetSplitter\\nsplits the dataset into training and test datasets.\\nThe test dataset is used for the evaluation and interpretation of results. kGCN also provides the modules to display the output of the results.\\nprovides the scores of the prediction model such as accuracy.\\nGCNScoreViewer\\ndisplays the graph of ROC scores in the image file.\\nGCNVisualizer\\ncomputes the IG values and atom importance.\\nGCNGraphViewer\\ndisplays the atom importance in the image file.\\nAnother example of the workflow is shown in Fig. 6 , which includes an example of multi-modal neural networks. To design multi-modal neural networks, the kGCN system provides the following modules:\\nAdditionalModalityPreprocessor\\nreads the data of another modality from a given file.\\nAddModality\\nadds the data of another modality to the dataset.\\nTo change from single-task to multi-modal, AddModality node should be added next to the GCNDatasetBuilder node.\\nThe visualization process shown at the bottom-right of Fig. 6 requires a specific computation time depending on the number of molecules to be visualized, as the computation time for the integrated gradient method for each molecule is 1–5 s during GPU execution. To reduce the size of the dataset, GCNDatasetSplitter can be used for selecting a part of the dataset.\\n\\n#### Python interface', '#### Python interface\\n\\nThe kGCN system also provides a Python library for programmers to more precisely tune the setting of the analysis. The kGCN system can be used in a manner similar to any standard library and supports pip, a Python standard package manager. Furthermore, the kGCN system can be used in the Jupyter notebook, which is an interactive interface. Therefore, the users can easily explore this library using google collaboratory, a cloud environment for the execution of Python programs.\\nThe kGCN system adopts an interface similar to scikit-learn, a defacto standard machine learning library in Python. Therefore, the process employing the kGCN library includes preprocessing, training by fit methods, and evaluation by pred method, in this order. The users can easily access the kGCN library in a similar manner to that of scikit-learn. Furthermore, designing a neural network, which is necessary for using kGCN, is easy if users are familiar with Keras because kGCN is compatible with the Keras library, and the users can easily design a neural network such as Keras.\\nTo demonstrate a wide applicability of the present framework, three sample programs comprising the datasets and scripts using the standard functions of kGCN are available in the framework web pages. In addition to these examples, the application of kGCN for a reaction prediction has been reported in a prior study [ 36 ], where the visualized reaction centers predicted by GCNs were consistent with reaction centers reported in the literature. This literature report used GCNs for reaction prediction on the kGCN system.\\n\\n#### Flexible user interfaces', 'As described in the introduction and implementation sections, kGCN provides KNIME GUI, a command-line interface, and a programming interface to support various types of users with various skill levels. For example, an easy-to-use high-layer GUI can assist the chemists with limited programming knowledge in using kGCN and understand SAR at a molecular level. Contrarily, for machine learning professionals with good programming skills, it is expected that they will focus on the improvement of algorithms using a low-layer python interface. By using a Python interface, the users can make machine learning procedures more flexible and incorporate the kGCN functions into the user specific programs such as web services. The users with good programming skills can also use the command-line interface to automate data-analysis procedures using the kGCN functions because it is easy to construct a pipeline combined with other commands such as Linux commands.\\nFor applications of kGCN, this section describes the prediction of the assay results of a protein based on the molecular structure. The prediction of compound-protein interactions (CPIs) has played an important role in drug discovery [ 38 ], and CPI prediction methods using deep learning have achieved excellent results [ 4 , 14 , 15 , 16 ]. In this study, the applicability of kGCN to CPI prediction is demonstrated as an example of single-task/multi-task/multi-modal GCNs. The single-task GCN predicts the activity against a protein based on the chemical structure represented as a graph. The multi-task GCN predicts the activities against multiple proteins from a chemical structure. Although single-task and multi-task GCNs do not use the information related to proteins, multi-modal neural networks predict the activity from information of both the protein sequence and chemical structure.\\nFor this examination, a dataset was prepared from the ChEMBL ver.20 database. The threshold for active/inactive was defined as 30uM. This dataset consists of four types of matrix metalloprotease inhibition assays, MMP-3, MMP-9, MMP-12, and MMP-13. The number of compounds for each assay are listed in Table 1 . These MMPs were selected because relatively large amounts of data were available for these in the ChEMBL dataset [ 39 ].\\nTable 1 Number of compounds in our dataset\\nFull size table\\nkGCN provides many types of descriptors for a compound and protein. For example, kGCN allows graph representation for GCN and vector representation, such as ECFP [ 40 ] and DRAGON [ 41 ], for standard neural networks. Additionally, to represent a protein, kGCN uses an amino-acid sequence and vector representation such as PROFEAT descriptors [ 42 ]. This application uses graph representation for a compound and sequence representation for a sequence.\\nTo simplify the experiment, the molecules with greater than 50 atoms were removed. As the dataset was unbalanced, negative data corresponding to inactivity were selected in the same manner [ 14 ]. Negative data was generated to equalize the number of negative and positive data for each assay.\\nSuch preprocessing can be realized using the kgcn-chem command included in the section describing the command-line interface.\\nAUCs obtained from five-fold cross-validation\\nFull size image\\na Chemical structure. b Atomic contributions to the predicted MMP-9 activity. Red color represents the positive contribution to the prediction (MMP-9 active in this case). Blue color represents the negative contribution (not active)\\nFull size image\\nFigure 7 shows the area under the curve in the receiver operator characteristic curve (ROC-AUC) of five-fold cross-validation. This result shows that the multi-modal approach outperforms the other approaches. The reason for a better ROC-AUC of the prediction with multi-modal approach is speculated to be the use of sequence-related information of the target proteins in addition to the graph representation of the compounds. This result is consistent with the reported results which indicate that the sequence descriptor contributes to improved accuracy [ 4 , 14 , 15 , 16 ].', 'kGCN allows the visualization of the atomic contributions to the prediction result, as shown in Fig. 8 b. The compound, N-hydroxy-2-[N-(propan-2-yloxy)[1,1’-biphenyl]-4-sulfonamido]acetamide (Fig. 8 a), is used for this prediction and its reported activity 200 nM (IC50) against MMP-9 [ 43 ]. The label of this compound for MMP-9 in the dataset is active, and the activity predicted for this compound in single-task mode is correct (probability of active label is 0.964). This compound possesses a hydroxamic acid group (-C(=O)NHOH), and it is well-known that many MMP inhibitors have a hydroxamic group. The crystallographic structure of a complex of MMP-9 and this compound has been previously reported [ 44 ]. MMP-9 is a zinc protease, and the hydroxamic acid group of the above compound is coordinated to the zinc ion of MMP-9. The positive contributions of OH, NH, and carbonyl oxygen of the hydroxamic acid group shown in Fig. 8 b are consistent with the interaction of the hydroxamic group with zinc of MMP-9.\\nSuch visualization can be used to confirm the validity of the prediction by comparing the atomic contributions toward the prediction with structure-activity and/or -property relationships. Additionally, this visualization can be useful for drug designing to improve the activity, physicochemical properties and/or ADMET properties by modifying the chemical moieties that contribute negatively to the prediction.\\nFor assisting various users including chemists and cheminformaticians, an open-source GCN tool, kGCN, is described. To support the users with various levels of programming skills, kGCN provides three interfaces: a GUI using the KNIME platform for users with limited programming skills such as chemists, as well as command-line and Python library interfaces for the advanced users such as cheminformaticians and data scientists. Three steps including preprocessing, model tuning, and interpretation of results, required for building a prediction model and utilization of prediction results. kGCN supports these three steps by including functions such as the automatic preparation of graph representation based on the chemical structures for pre-processing, Bayesian optimization for automatic optimization of the hyper-parameters of the neural networks for model tuning, the integrated gradient method to visualize the atomic contribution toward the prediction result for interpretation. In terms of the approaches used for prediction, kGCN supports single-task, multi-task, and multi-modal predictions. The CPI prediction for four assays of matrixmetalloprotease inhibition, MMP-3, -9, -12, and -13, is performed as a representative case study using kGCN. Multi-modal prediction shows higher accuracy than those of the single-task and multi-task predictions. Additionally, the visualization of atomic contribution to the prediction indicated that hydroxamate group of the compound exhibits a positive contribution to the activity and this is consistent with the known structure-activity relationships. Such visualization is useful for the validation of the models and designing new molecules based on the model. This also allows the realization of “explainable AI” for understanding the factors influencing the AI prediction which are typically a black-box.\\nkGCN is available at https://github.com/clinfo/kGCN . Various examples such as Jupyter notebooks are also provided. Future works will include supporting new methods of graph neural networks because graph neural networks are a hot topic at present and new methods, e.g., graph attention and pooling, are being actively developed. We will proactively adopt these new methods and continue to develop kGCN so that various users can easily apply such latest methods to appropriately analyze the data in their hands and understand the reasons for the predictions. Also, we are going to gather the user feedback and improve kGCN for better usability.', '## Availability of data and materials', 'Project name: kGCN. Project home page: https://github.com/clinfo/kGCN . Operating system(s): Platform independent(Ubuntu 18.04, and CentOS 7 are mainly supported). Programming language: Python. Other requirements: python3 (> 3.6), tensorflow. License: https://github.com/clinfo/kGCN/blob/master/LICENSE . Any restrictions to use by non-academics: licence needed.\\n- Kyoto-university graph convolutional network framework.\\n- https://hub.docker.com/r/clinfo/kgcn.\\n- Gawehn E, Hiss JA, Schneider G (2016) Deep learning in drug discovery. Mol Inform 35(1):3–14. https://doi.org/10.1002/minf.201501008 Article CAS PubMed Google Scholar\\n- Goh GB, Hodas NO, Vishnu A (2017) Deep learning for computational chemistry. J Comput Chem 38(16):1291–1307 Article CAS PubMed Google Scholar\\n- Elton DC, Boukouvalas Z, Fuge MD, Chung PW (2019) Deep learning for molecular design - a review of the state of the art. Mol Syst Design Eng 4(4):828–849 Article CAS Google Scholar\\n- Torng W, Altman RB (2019) Graph convolutional neural networks for predicting drug-target interactions. J Chem Inform Model 59(10):4131–4149 Article CAS Google Scholar\\n- Ma J, Sheridan RP, Liaw A, Dahl GE, Svetnik V (2015) Deep neural nets as a method for quantitative structure-activity relationships. J Chem Inform Model 55(2):263–274 Article CAS Google Scholar\\n- Schneckener S, Grimbs S, Hey J, Menz S, Osmers M, Schaper S, Hillisch A, Göller AH (2019) Prediction of oral bioavailability in rats: transferring insights from in vitro correlations to (deep) machine learning models using in silico model outputs and chemical structure parameters. J Chem Inform Model 59(11):4893–4905 Article CAS Google Scholar\\n- Wegner JK, Sterling A, Guha R, Bender A, Faulon J-L, Hastings J, O’Boyle N, Overington J, Van Vlijmen H, Willighagen E (2012) Cheminformatics. Commun ACM 55(11):65–75 Article Google Scholar\\n- Kearnes S, McCloskey K, Berndl M, Pande V, Riley P (2016) Molecular graph convolutions: moving beyond fingerprints. J Comput Aided Mol Des 30(8):595–608 Article CAS PubMed PubMed Central Google Scholar\\n- Gilmer J, Schoenholz SS, Riley PF, Vinyals O, Dahl GE (2017) Neural message passing for quantum chemistry. In: Proceedings of the 34th International Conference on Machine Learning, vol. 70, pp 1263–1272\\n- Duvenaud DK, Maclaurin D, Iparraguirre J, Bombarell R, Hirzel T, Aspuru-Guzik A, Adams RP (2015) Convolutional networks on graphs for learning molecular fingerprints. Adv Neural Inform Process Syst 28:2224–2232 Google Scholar\\n- Jin W, Coley CW, Barzilay R, Jaakkola T (2017) Predicting organic reaction outcomes with weisfeiler-lehman network. In: Proceedings of the 31st International Conference on Neural Information Processing Systems, pp 2604–2613\\n- Berthold MR, Cebron N, Dill F, Gabriel TR, Kotter T, Meinl T, Ohl P, Thiel K, Wiswedel B (2009) Knime - the konstanz information miner: version 20 and beyond. ACM SIGKDD Explorat Newslett 11(1):26–31 Article Google Scholar\\n- Goodfellow I, Bengio Y, Courville A (2016) Deep Learning. MIT Press\\n- Hamanaka M, Taneishi K, Iwata H, Ye J, Pei J, Hou J, Okuno Y (2017) Cgbvs-dnn: prediction of compound-protein interactions based on deep learning. Mol Inform 36(1–2):1600045 Article Google Scholar\\n- Nguyen TT, Nguyen T, Le DH, Quinn H, Venkatesh S (2020) Predicting drug–target binding affinity with graph neural networks. bioRxiv. https://doi.org/10.1101/684662 . https://www.biorxiv.org/content/early/2020/01/22/684662.full.pdf\\n- Tsubaki M, Tomii K, Sese J (2019) Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences. Bioinformatics 35(2):309–318 Article CAS PubMed Google Scholar\\n- Ramsundar B, Liu B, Wu Z, Verras A, Tudor M, Sheridan RP, Pande V (2017) Is multitask deep learning practical for pharma? J Chem Inform Model 57(8):2068–2076 Article CAS Google Scholar\\n- Sanyal S, Balachandran J, Yadati N, Kumar A, Rajagopalan P, Sanyal S, Talukdar P (2018) MT-CGCNN: Integrating crystal graph convolutional neural network with multitask learning for material property prediction. arXiv preprint arXiv:1811.05660\\n- Liu K, Sun X, Jia L, Ma J, Xing H, Wu J, Gao H, Sun Y, Boulnois F, Fan J (2019) Chemi-net: a molecular graph convolutional network for accurate drug property prediction. Int J Mol Sci 20(14):3389 Article CAS PubMed Central Google Scholar\\n- Selvaraju RR, Cogswell M, Das Vedantam AR, Parikh D, Batra D (2017) Grad-cam: visual explanations from deep networks via gradient-based localization. In: Proceedings of the IEEE International Conference on Computer Vision, pp 618–626\\n- Smilkov D, Thorat N, Kim B, Viegas F, Wattenberg M (2017) Smoothgrad: removing noise by adding noise. arXiv preprint arXiv:1706.03825\\n- Sundararajan M, Taly A, Yan Q (2017) Axiomatic attribution for deep networks. In: Proceedings of the 34th International Conference on Machine Learning, vol 70, pp 3319–3328. JMLR.org', '- Sundararajan M, Taly A, Yan Q (2017) Axiomatic attribution for deep networks. In: Proceedings of the 34th International Conference on Machine Learning, vol 70, pp 3319–3328. JMLR.org\\n- Snoek J, Larochelle H, Adams RP (2012) Practical bayesian optimization of machine learning algorithms. In: Proceedings of the 25th International Conference on Neural Information Processing Systems, vol 2, pp 2951–2959\\n- Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, Devin M, Ghemawat S, Irving G, Isard M, et al. (2016) Tensorflow: a system for large-scale machine learning. In: 12th USENIX Symposium on Operating Systems Design and Implementation, pp 265–283\\n- Ramsundar B, Eastman P, Walters P, Pande V (2019) Deep learning for the life sciences. O’Reilly Media inc.,\\n- pfnet research: chainer-chemistry. https://github.com/pfnet-research/chainer-chemistry\\n- Popova M Openchem: deep learning toolkit for computational chemistry and drug design. https://github.com/Mariewelt/OpenChem\\n- Tokui S, Oono K, Hido S, Clayton J Chainer (2015) A next-generation open source framework for deep learning. In: Proceedings of Workshop on Machine Learning Systems (LearningSys) in the Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), vol 5, pp 1–6\\n- Landrum G (2018) RDKit: open-source cheminformatics. http://www.rdkit.org (Accessed August 21 2019)\\n- Kipf TN, Welling M (2017) Semi-supervised classification with graph convolutional networks. In: International Conference on Learning Representations\\n- Altae-Tran H, Ramsundar B, Pappu AS, Pande V (2017) Low data drug discovery with one-shot learning. ACS Cent Sci 3:283–293 Article CAS PubMed PubMed Central Google Scholar\\n- Ioffe S, Szegedy C (2015) Batch normalization: accelerating deep network training by reducing internal covariate shift. arXiv preprint. arXiv:1502.03167\\n- Montanari F, Kuhnke L, Laak A Ter, Clevert D-A (2020) Modeling physico-chemical admet endpoints with multitask graph convolutional networks. Molecules 25(1):44 Article CAS Google Scholar\\n- Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS, Davis A, Dean J, Zheng X (2015) TensorFlow: large-scale machine learning on heterogeneous systems. https://www.tensorflow.org/ (Accessed 21 August 2019)\\n- Chollet F, et al (2015) Keras. https://github.com/fchollet/keras\\n- Ishida S, Terayama K, Kojima R, Takasu K, Okuno Y (2019) Prediction and interpretable visualization of retrosynthetic reactions using graph convolutional networks. J Chem Inform Model 59(12):5026–5033 Article CAS Google Scholar\\n- The GPyOpt authors: GPyOpt (2016) A bayesian optimization framework in Python. http://github.com/SheffieldML/GPyOpt\\n- Keiser MJ, Setola V, Irwin JJ, Laggner C, Abbas AI, Hufeisen SJ, Jensen NH, Kuijer MB, Matos RC, Tran TB et al (2009) Predicting new molecular targets for known drugs. Nature 462(7270):175–181 Article CAS PubMed PubMed Central Google Scholar\\n- Gimeno A, Beltrán-Debón R, Mulero M, Pujadas G, Garcia-Vallvé S (2020) Understanding the variability of the S1’ pocket to improve matrix metalloproteinase inhibitor selectivity profiles. Drug Discov Today 25(1):38–57 Article CAS PubMed Google Scholar\\n- Rogers D, Hahn M (2010) Extended-connectivity fingerprints. J Chem Informat Model 50(5):742–754 Article CAS Google Scholar\\n- Mauri A, Consonni V, Pavan M, Todeschini R (2006) Dragon software: an easy approach to molecular descriptor calculations. MATCH Commun Math Comput Chem 56(2):237–248 CAS Google Scholar\\n- Zhang P, Tao L, Zeng X, Qin C, Chen S, Zhu F, Li Z, Jiang Y, Chen W, Chen Y-Z (2016) A protein network descriptor server and its use in studying protein, disease, metabolic and drug targeted networks. Brief Bioinform 18(6):1057–1070 PubMed Central Google Scholar\\n- Rossello A, Nuti E, Carelli P, Orlandini E, Macchia M, Nencetti S, Zandomeneghi M, Balzano F, Barretta GU, Albini A, Benelli R, Cercignani G, Murphy G, Balsamo A (2005) Ni-propoxy-n-biphenylsulfonylaminobutylhydroxamic acids as potent and selective inhibitors of mmp-2 and mt1-mmp. Bioorg Med Chem Lett 15(5):1321–1326 Article CAS PubMed Google Scholar\\n- Antoni C, Vera L, Devel L, Catalani MP, Czarny B, Cassar-Lajeunesse E, Nuti E, Rossello A, Dive V, Stura EA (2013) Crystallization of bi-functional ligand protein complexes. J Struct Biol 182(3):246–254 Article CAS PubMed Google Scholar\\nDownload references\\nThis paper is based on a part of results obtained from a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO).', \"## Author information\\n\\n### Authors and Affiliations\\n\\n- Graduate School of Medicine, Kyoto University, Shogoin-kawaharacho, Sakyo-ku, Kyoto, 606-8507, Japan Ryosuke Kojima,\\xa0Hiroaki Iwata\\xa0&\\xa0Yasushi Okuno\\n- Graduate School of Pharmaceutical Sciences, Kyoto University, Yoshida, Sakyo-ku, Kyoto, 606-8501, Japan Shoichi Ishida\\n- Medical Sciences Innovation Hub Program, RIKEN Cluster for Science, Technology and Innovation Hub, Tsurumi-ku, Kanagawa, Kanagawa, 230-0045, Japan Masateru Ohta,\\xa0Teruki Honma\\xa0&\\xa0Yasushi Okuno\\n- RIKEN Center for Biosystems Dynamics Research, Tsurumi-ku, Kanagawa, Kanagawa, 230-0045, Japan Teruki Honma\\n- Ryosuke Kojima View author publications Search author on: PubMed Google Scholar\\n- Shoichi Ishida View author publications Search author on: PubMed Google Scholar\\n- Masateru Ohta View author publications Search author on: PubMed Google Scholar\\n- Hiroaki Iwata View author publications Search author on: PubMed Google Scholar\\n- Teruki Honma View author publications Search author on: PubMed Google Scholar\\n- Yasushi Okuno View author publications Search author on: PubMed Google Scholar\\n\\n### Contributions\\n\\nRK; Designed and implemented the software, analysed data, and co-wrote the paper. SI; Designed and implemented the software, analysed data, and co-wrote the paper. MO; analysed data and co-wrote the paper. HI; analysed data and co-wrote the paper. TH and YO; supervised the research. All authors provided critical feedback and helped shape the research, analysis and manuscript. All authors read and approved the final manuscript.\\n\\n### Corresponding author\\n\\nCorrespondence to Ryosuke Kojima .\\n\\n## Ethics declarations\\n\\n### Competing interests\\n\\nThe authors declare that they have no competing interests.\\n\\n## Additional information\\n\\n### Publisher's Note\\n\\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\\n\\n## Rights and permissions\\n\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ . The Creative Commons Public Domain Dedication waiver ( http://creativecommons.org/publicdomain/zero/1.0/ ) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\\nReprints and permissions\\n\\n## About this article\\n\\n### Cite this article\\n\\nKojima, R., Ishida, S., Ohta, M. et al. kGCN: a graph-based deep learning framework for chemical structures. J Cheminform 12 , 32 (2020). https://doi.org/10.1186/s13321-020-00435-6\\nDownload citation\\n- Received : 17 February 2020\\n- Accepted : 28 April 2020\\n- Published : 12 May 2020\\n- DOI : https://doi.org/10.1186/s13321-020-00435-6\\n\\n### Share this article\\n\\nAnyone you share the following link with will be able to read this content:\\nGet shareable link\\nSorry, a shareable link is not currently available for this article.\\nCopy to clipboard\\nProvided by the Springer Nature SharedIt content-sharing initiative\\n- Graph convolutional network\\n- Graph neural network\\n- Open source software\", 'Download PDF\\n- Open access\\n- Published: 03 June 2025\\n\\n# Multimodal deep learning for chemical toxicity prediction and management\\n\\n- Jiwon Hong 1 &\\n- Hyun Kwon 2\\nScientific Reports volume 15 , Article\\xa0number: 19491 ( 2025 ) Cite this article\\n- 1391 Accesses\\n- 2 Altmetric\\n- Metrics details\\n- Biochemistry\\n- Chemical engineering\\nThe accurate prediction of chemical toxicity is a crucial research focus in chemistry, biotechnology, and national defense. The development of comprehensive datasets for chemical toxicity prediction remains limited due to security constraints and the structural complexity of chemical data. Existing studies are often confined to specific domains, such as genotoxicity or acute oral toxicity. To address these gaps, this study introduces an integrated research dataset that combines chemical property data and molecular structure images. The dataset is curated from diverse sources, preprocessed, and normalized to optimize it for deep learning applications. The proposed deep learning model enhances the precision of multi-toxicity predictions by integrating Vision Transformer (ViT) architecture for image-based data and a Multilayer Perceptron (MLP) for numerical data. A joint fusion mechanism is employed to effectively combine image and numerical features, significantly improving predictive performance. The model is also designed for multi-label toxicity prediction, enabling simultaneous evaluation of diverse toxicological endpoints. Experimental results show that ViT model demonstrate an accuracy of 0.872, an F1-score of 0.86, and a Pearson Correlation Coefficient (PCC) of 0.9192.\\n\\n### Similar content being viewed by others\\n\\n### Accurate clinical toxicity prediction using multi-task deep neural nets and contrastive molecular explanations\\n\\nOpen access\\n25 March 2023\\n\\n### Prediction of reproductive and developmental toxicity using an attention and gate augmented graph convolutional network\\n\\nOpen access\\n25 May 2025\\n\\n### Quantitative structure–activity relationship models for genotoxicity prediction based on combination evaluation strategies for toxicological alternative experiments\\n\\nOpen access\\n13 April 2021\\n\\n## Introduction', 'Open access\\n25 May 2025\\n\\n### Quantitative structure–activity relationship models for genotoxicity prediction based on combination evaluation strategies for toxicological alternative experiments\\n\\nOpen access\\n13 April 2021\\n\\n## Introduction\\n\\nAccurate prediction of chemical toxicity 1 , 2 , 3 , 4 , 5 has emerged as a pivotal research area in chemistry, biotechnology, and national defense. As global conflicts and the threats of asymmetric warfare become increasingly prominent, the potential deployment of chemical weapons 6 and the occurrence of chemical terrorism pose significant challenges to public safety and security. This highlights the urgent need for sophisticated systems capable of predicting, detecting, and mitigating chemical toxicity to protect both military personnel and civilians.\\nIn addition to these pressing security concerns, chemical toxicity prediction 7 is essential for safeguarding the environment and public health. The widespread use of industrial chemicals 8 , pesticides 9 , and pharmaceuticals necessitates precise toxicological assessments to ensure regulatory compliance and minimize harm. However, the inherent complexity of chemical substances and the scarcity of comprehensive datasets have hindered progress in this field. Current prediction models often rely on narrow datasets focused on specific toxic endpoints, such as genotoxicity or acute oral toxicity, which limits their generalizability and practical application.\\nWhile traditional machine learning techniques 1 , 3 , 10 , 11 have been employed in toxicity prediction, they frequently fall short due to their reliance on manually engineered features and their inability to effectively model the non-linear relationships inherent in chemical data. Deep learning models, on the other hand, offer a transformative potential by leveraging advanced architectures to extract and integrate complex patterns from diverse data sources. Despite these advancements, existing deep learning approaches are often restricted to single-modality inputs, such as either numerical data or molecular structure images, failing to capitalize on the synergistic benefits of multi-modal data fusion.\\nTo address these limitations, we introduces an innovative framework for chemical toxicity prediction that integrates chemical property data with molecular structure images into a unified multi-modal deep learning model. Our approach leverages a Vision Transformer (ViT) for processing image-based features and a Multilayer Perceptron (MLP) for handling numerical data, enabling a joint fusion mechanism that significantly enhances predictive accuracy. Unlike existing methods that classify using a single type of dataset, the proposed method introduces a multi-label classification model that integrates multiple dataset types to determine whether a chemical is toxic or non-toxic.\\nCompared to existing studies, our approach provides several contributions: (1) the development of a comprehensive dataset that integrates chemical property data with molecular structure images, (2) the implementation of a multi-modal deep learning model that achieves superior accuracy through effective data fusion, and (3) the incorporation of wearable sensors to extend the framework’s applicability to dynamic and high-risk environments. These innovations collectively address the critical gaps in existing research, enabling more precise toxicity predictions and facilitating the development of robust chemical safety management systems.\\nThe remainder of this paper is organized as follows: Section “ Related work ” reviews the related work in the field. Section “ Proposed method ” presents the proposed methodology in detail. Section “ Experimental setup and results ” outlines the experimental setup and provides an evaluation of the results. Section “ Discussion ” offers an in-depth discussion of the proposed approach. Finally, Section “ Conclusion ” summarizes the findings and concludes the paper.\\n\\n## Related work\\n\\nThe prediction of chemical toxicity has been extensively studied, owing to its critical implications in environmental safety, drug development, and defense against chemical threats. Traditional methods primarily rely on in vivo and in vitro testing, which, despite being highly reliable, are time-consuming, expensive, and ethically challenging. Consequently, computational approaches, particularly those leveraging machine learning and deep learning, have gained significant attention as efficient alternatives.\\n\\n### Traditional machine learning approaches', '### Traditional machine learning approaches\\n\\nEarly computational efforts in toxicity prediction focused on Quantitative Structure-Activity Relationship (QSAR) 12 models, which correlate chemical structures with biological activities or toxic effects. Algorithms such as Support Vector Machines (SVM) 13 , Random Forests (RF) 14 , and k-Nearest Neighbors (k-NN) 15 have been widely utilized. For instance, Matthews et al. (2016) developed a QSAR model using Random Forests to predict acute toxicity endpoints. Similarly, Trinh et al. 16 demonstrated the effectiveness of machine learning in predicting specific endpoints like carcinogenicity and genotoxicity. While these methods achieved moderate success, their reliance on manually engineered features limited their capacity to model complex, non-linear relationships in chemical data.\\n\\n### Deep learning for toxicity prediction\\n\\nDeep learning models have emerged as powerful tools for addressing the limitations of traditional machine learning. These models, capable of learning hierarchical feature representations, have shown promise in processing diverse data types, such as numerical descriptors and molecular graphs. For instance, Sharma et al. 17 proposed a multi-task deep neural network for predicting various toxicity endpoints, demonstrating improved accuracy over traditional QSAR models. Similarly, Sun et al. 18 utilized graph convolutional networks (GCNs) to capture structural information from molecular graphs, achieving state-of-the-art performance in predicting mutagenicity and other toxic endpoints. However, most existing deep learning models are designed for single-modality inputs. For example, Schwartz et al. 19 processes molecular fingerprints or SMILES strings, while Hirohara et al. 20 employs convolutional neural networks (CNNs) to analyze chemical images. These approaches often fail to exploit the synergistic benefits of integrating multiple data modalities, such as combining chemical property descriptors with structural images.\\n\\n### Multi-modal toxicity prediction models\\n\\nMulti-modal learning has recently gained traction as a means of integrating heterogeneous data sources for more robust toxicity prediction. For example, Schneider et al. 21 introduced a hybrid model combining molecular descriptors and molecular dynamics simulation data, showing enhanced predictive accuracy for endocrine disruption. Similarly, Liu et al. 22 proposed a multi-modal framework that fuses text-based chemical descriptions with image data for broader applicability across diverse datasets. These studies highlight the potential of multi-modal approaches but also reveal challenges, including increased computational complexity and the need for sophisticated data fusion mechanisms.\\nBuilding upon these prior works, our study addresses these gaps by introducing a novel multi-modal deep learning framework that combines chemical property data with molecular structure images for binaray-label toxicity prediction. Additionally, by integrating wearable sensor technology, our approach bridges the gap between predictive modeling and real-world applicability, offering a scalable solution for dynamic and high-risk environments.\\n\\n## Proposed method\\n\\nIn this study, we propose a multi-modal deep learning model aimed at predicting the toxicity of chemical compounds. In Fig. 1 , the model leverages both image-based and tabular data inputs to improve the prediction accuracy. Specifically, we adopt the Joint or Intermediate Fusion strategy, which combines information from different modalities at an intermediate stage of the model. This approach allows the model to learn the interactions between different data types while preserving the unique characteristics of each modality.\\nOverview of the proposed method. A fully connected layer is also referred to as an Multi-Layer Perceptron (MLP).\\nFull size image\\n\\n### Model architecture\\n\\nThe proposed architecture consists of two primary components: image processing and tabular data processing. These two components are fused at an intermediate stage, leading to a final prediction of toxicity. The architecture can be outlined as follows:\\n\\n#### Image processing backbone: vision transformer (ViT)', '#### Image processing backbone: vision transformer (ViT)\\n\\nThe first input modality consists of 2D structural images of chemical compounds, such as molecular structures. These images are processed by a pre-trained Vision Transformer (ViT) model, which has been fine-tuned to handle chemical structure images. The ViT model employed in this study follows the ViT-Base/16 architecture introduced by Dosovitskiy et al. 23 , which was pre-trained on the ImageNet-21k dataset and processes input images as 16 × 16-pixel patches at a resolution of 224 × 224 pixels. To adapt this model for chemical structure recognition, we fine-tuned it using a custom dataset of 4179 molecular structure images. These images were collected programmatically using a Python-based web crawler, which systematically extracted publicly available molecular structure images from chemical databases such as PubChem and eChemPortal based on CAS (Chemical Abstracts Service) numbers. Each image was annotated with its corresponding CAS number to ensure alignment with chemical property data. The chemical diversity of the dataset was carefully curated to include a broad spectrum of organic and inorganic compounds. Specifically, the selected CAS numbers encompassed pharmaceuticals, agrochemicals, and industrial chemicals, with deliberate inclusion of compounds featuring diverse functional groups, stereochemical configurations, and molecular sizes. This approach ensures that the model is exposed to a representative subset of chemical space, enhancing its generalizability. The ViT model extracts features from these images and converts them into a 128-dimensional feature vector. Let \\\\(I \\\\in {\\\\mathbb {R}}^{H \\\\times W \\\\times C}\\\\) represent the input image of height H , width W , and C channels. The Vision Transformer processes the image and generates a feature vector \\\\({\\\\textbf{f}}_{\\\\text {img}} \\\\in {\\\\mathbb {R}}^{128}\\\\) as follows:\\n$$\\\\begin{aligned} {\\\\textbf{f}}_{\\\\text {img}} = \\\\text {ViT}(I) \\\\end{aligned}$$\\nThe number of trainable parameters in the MLP layer used for dimensionality reduction is:\\n$$\\\\begin{aligned} N_{\\\\text {img}} = (768 + 1) \\\\times 128 = 98,688. \\\\end{aligned}$$\\n\\n#### Tabular data processing: multi-layer perceptron (MLP)\\n\\nThe second input modality consists of tabular data representing the chemical properties of the compounds. This data includes both numerical and categorical features as show in Table 8 . The tabular data is processed by a multi-layer perceptron (MLP) that transforms it into a 128-dimensional feature vector. Let \\\\(X \\\\in {\\\\mathbb {R}}^{n_{\\\\text {features}}}\\\\) represent the tabular data, where \\\\(n_{\\\\text {features}}\\\\) denotes the number of features in the dataset. The MLP processes the tabular data and generates a feature vector \\\\({\\\\textbf{f}}_{\\\\text {tab}} \\\\in {\\\\mathbb {R}}^{128}\\\\) , which can be expressed as:\\n$$\\\\begin{aligned} {\\\\textbf{f}}_{\\\\text {tab}} = \\\\text {MLP}(X) \\\\end{aligned}$$\\nThe number of trainable parameters in this MLP layer is given by:\\n$$\\\\begin{aligned} N_{\\\\text {tab}} = (D_{\\\\text {tab}} + 1) \\\\times 128. \\\\end{aligned}$$\\n\\n#### Fusion layer: joint fusion of features\\n\\nThe features extracted from both modalities are then fused at an intermediate stage. The image feature vector \\\\({\\\\textbf{f}}_{\\\\text {img}}\\\\) and the tabular data feature vector \\\\({\\\\textbf{f}}_{\\\\text {tab}}\\\\) are concatenated to form a fused feature vector \\\\({\\\\textbf{f}}_{\\\\text {fused}} \\\\in {\\\\mathbb {R}}^{256}\\\\) :\\n$$\\\\begin{aligned} {\\\\textbf{f}}_{\\\\text {fused}} = \\\\text {concat}({\\\\textbf{f}}_{\\\\text {img}}, {\\\\textbf{f}}_{\\\\text {tab}}) \\\\end{aligned}$$\\nThis fused feature vector \\\\({\\\\textbf{f}}_{\\\\text {fused}}\\\\) is then passed to the toxicity prediction module for further processing.\\n\\n#### Toxicity prediction module: fully connected layer', '#### Toxicity prediction module: fully connected layer\\n\\nThe fused feature vector \\\\({\\\\textbf{f}}_{\\\\text {fused}}\\\\) is passed through a fully connected layer (MLP) to generate the final toxicity prediction. The output of this module consists of independent probability values \\\\({\\\\hat{y}}_i\\\\) for each toxicity label. Let \\\\({\\\\hat{y}}_i \\\\in [0, 1]\\\\) represent the predicted probability of toxicity for label i , which can be calculated as:\\n$$\\\\begin{aligned} {\\\\hat{y}}_i = \\\\sigma ({\\\\textbf{w}}_i^T {\\\\textbf{f}}_{\\\\text {fused}} + b_i) \\\\end{aligned}$$\\nwhere \\\\(\\\\sigma (\\\\cdot )\\\\) denotes the sigmoid activation function, \\\\({\\\\textbf{w}}_i\\\\) is the weight vector associated with label i , and \\\\(b_i\\\\) is the bias term. The sigmoid function ensures that the output is a probability value in the range [0, 1]. The total number of trainable parameters in this layer is:\\n$$\\\\begin{aligned} N_{\\\\text {out}} = (128 + 128 + 1) \\\\times C. \\\\end{aligned}$$\\nwhere C represents the number of toxicity labels.\\nDuring training, all three MLP layers (image_fc, feature_fc, output_fc) are optimized jointly, ensuring that the ViT-based image representation and tabular MLP representation are updated simultaneously rather than freezing the early layers. The optimization follows Adam optimizer, as the output consists of multiple independent probability scores.\\n\\n#### Thresholding for prediction\\n\\nOnce the probabilities for each toxicity label have been computed, a threshold of 0.5 is applied to determine whether a compound is toxic for each label. If \\\\({\\\\hat{y}}_i \\\\ge 0.5,\\\\) the compound is predicted to be toxic for label \\\\(i\\\\) (i.e., the label is classified as positive). If \\\\({\\\\hat{y}}_i < 0.5,\\\\) the label is classified as negative. This decision rule can be expressed as:\\n$$\\\\begin{aligned} y_i = {\\\\left\\\\{ \\\\begin{array}{ll} 1 & \\\\text {if } {\\\\hat{y}}_i \\\\ge 0.5 \\\\\\\\ 0 & \\\\text {if } {\\\\hat{y}}_i < 0.5 \\\\end{array}\\\\right. } \\\\end{aligned}$$\\nAlgorithm 1\\nMulti-modal deep learning for toxicity prediction\\nFull size image\\n\\n### Training objective\\n\\nThe model is trained using multi-label binary cross-entropy loss for each toxicity label. The multi-label cross-entropy loss for label i is defined as:\\n$$\\\\begin{aligned} L_i = -y_i \\\\log ({\\\\hat{y}}_i) - (1 - y_i) \\\\log (1 - {\\\\hat{y}}_i) \\\\end{aligned}$$\\nThe total loss for the model is the sum of the individual losses for all labels:\\n$$\\\\begin{aligned} L_{\\\\text {total}} = \\\\sum _{i=1}^{k} L_i \\\\end{aligned}$$\\nwhere \\\\(k\\\\) is the number of toxicity labels. The model is optimized to minimize the total loss function, thereby improving the accuracy of the toxicity predictions. Details of the procedure for the proposed scheme are provided in Algorithm\\xa01.\\nThe proposed method has several advantages. First, the joint fusion strategy allows for the effective integration of diverse data modalities, enabling the model to leverage complementary information from both image and tabular data sources. This enhances the model’s ability to capture complex relationships between a chemical compound’s structure and its toxicity profile.\\nSecond, the use of deep learning models such as Vision Transformer (ViT) and MLP ensures that the model can handle large-scale datasets with high-dimensional inputs, making it suitable for real-world chemical toxicity prediction tasks. Moreover, the model’s outputs-probabilities for each toxicity label-can be thresholded to provide interpretable binary predictions, offering valuable insights for decision-making.\\nIn conclusion, the proposed multi-modal deep learning model represents a robust framework for predicting the toxicity of chemical compounds. By effectively integrating image and tabular data, the model provides a more accurate and efficient prediction compared to traditional methods that rely on single-modal inputs.\\n\\n## Experimental setup and results\\n\\n### Experimental setup', 'The experimental environment was designed to facilitate the effective integration of chemical property data and molecular structure image data for predicting multi-label toxicity across multiple categories, including general symptoms, inhalation, skin, eye, oral, and others. The entire pipeline was implemented using Python 3.10.12 24 on a Linux system equipped with an NVIDIA A100-SXM4-40GB GPU, which supports CUDA version 12.1. The deep learning frameworks PyTorch 2.1.0 and TensorFlow 2.17.1 25 were utilized for model development, training, and evaluation. The GPU availability was verified through the frameworks, ensuring optimized computational performance during experimentation.\\nTable 1 Chemical property and toxicity data were curated from eight publicly available sources on Korea’s Ministry of Environment.\\nFull size table\\nAn example of molecular structure images.\\nFull size image\\nTable 2 The merged dataset in Table\\nFull size table\\nTable 3 The final constructed dataset.\\nFull size table\\nThe dataset was constructed by fusing molecular structure image data with chemical property and toxicity datasets. To construct the molecular structure image data, 4179 molecular structure images were collected using a Python-based web crawler as shown in Fig. 2 . This crawler scraped publicly available chemical information websites, including PubChem and eChemPortal, to download molecular structure images based on CAS (Chemical Abstracts Service) 34 numbers. Each image was annotated with its corresponding CAS number and stored with its file path for seamless integration with chemical property data. Additionally, no image rotations or flipping were applied in the preprocessing, while zero normalization was performed.\\nChemical property and toxicity data were curated from eight publicly available sources 26 , 27 , 28 , 29 , 30 , 31 , 32 , 33 , including the chemical safety information provided by Korea’s Ministry of Environment as shown in Tables 1 and 2 . Relevant columns were extracted and merged into a single dataset using the merge() function from the pandas library, with CAS numbers serving as the primary key. Toxicity data, comprising textual descriptions of symptoms for various exposure types (e.g., general symptoms, inhalation, skin, eye, oral, others), were processed using natural language processing (NLP) 35 techniques. Tokenization, stopword removal, and stemming were applied to convert textual data into a structured keyword list suitable for machine learning tasks. For instance, textual descriptions such as “causes permanent damage to the digestive tract and may result in nausea, vomiting, and diarrhea” were transformed into keywords such as “digestive tract damage, nausea, vomiting, diarrhea.”\\nThe merged dataset comprised 4179 chemical records with molecular structure images and detailed property data, as shown in Table 2 . Missing values in numerical features such as boiling points and melting points were imputed using correlated features, and range values were replaced with their midpoint. Categorical variables such as color and state were encoded using one-hot encoding, ensuring that all features were normalized for training. Toxicity data were converted into multi-label binary vectors using MultiLabelBinarizer , enabling the representation of symptoms as binary vectors (e.g., \\\\([1, 0, 1, 0, 0]\\\\) ). The final constructed dataset is shown in Table 3 .\\nTo ensure robust evaluation, the dataset was split into training, validation, and testing sets in a 7:1:2 ratio using the train_test_split function from the scikit-learn library 36 , with the random seed set to 0. During model training, a batch size of 16 was utilized to handle data efficiently. The optimization algorithm employed in our study is Adam, with a learning rate set to 0.0001 37 . The dropout rate was configured at 0.3 to mitigate overfitting. For early stopping, training was halted if validation accuracy did not improve for three consecutive epochs. Additionally, the pre-trained model utilized was the Vision Transformer (ViT) model trained on the ImageNet-21k dataset, specifically the variant with a patch size of 16. The multi-label binary cross-entropy loss ( BCELoss ) function was chosen to handle the multi-label nature of the predictions. Training was conducted over 30 epochs, and early stopping was implemented to prevent overfitting if the validation accuracy did not improve for three consecutive epochs. Model performance was evaluated using metrics such as accuracy, F1-score 38 , and Pearson correlation coefficient (PCC) 39 .\\nThe loss values of ViT, DeiT, and BEiT models over 20 epochs, highlighting the optimization progress of each model.\\nFull size image\\nThe accuracy trends of ViT, DeiT, and BEiT models over 20 epochs, demonstrating their performance improvement during training in validation dataset.\\nFull size image', 'Full size image\\nThe accuracy trends of ViT, DeiT, and BEiT models over 20 epochs, demonstrating their performance improvement during training in validation dataset.\\nFull size image\\nF1 score progression of ViT, DeiT, and BEiT models across 20 epochs, reflecting their balance between precision and recall in validation dataset.\\nFull size image\\nPearson Correlation Coefficient (PCC) of ViT, DeiT, and BEiT models over 20 epochs, indicating their alignment with ground truth in validation dataset.\\nFull size image', '### Experimental results', 'In this section, we conducted a performance analysis of the BEiT and DeiT models to compare their performance with the ViT model used in the proposed method. Bidirectional Encoder for Image Transformers (BEiT) applies the principles of BERT, originally developed for Natural Language Processing (NLP), to image data. It leverages self-supervised learning and bidirectional context understanding, enabling the model to capture complex image representations. However, BEiT requires intricate tokenization processes, which can add complexity to the model. Data-efficient Image Transformer (DeiT) improves the efficiency of ViT by using knowledge distillation, allowing the model to learn effectively with a smaller amount of data. It can achieve comparable performance to ViT with only about 1/10th of the training data, though its performance is heavily dependent on the quality of the teacher model. Additionally, DeiT has limitations when handling more complex tasks.\\nFigure 3 illustrates the loss values of the three models (ViT 40 , BEiT 41 , and DeiT 42 ) over 22 epochs. ViT exhibits the most efficient loss minimization, rapidly decreasing in the initial epochs and converging to almost zero by the 20th epoch. This demonstrates its robust learning capability and effective optimization during training. DEiT follows a similar trend, with a steady decline in loss, although it converges slightly slower than ViT. In contrast, BEiT shows a slower reduction in loss and stabilizes at a higher value, indicating its relative inefficiency in adapting to the dataset. This behavior may be attributed to the model’s reliance on pretraining strategies that are less suited to the domain-specific characteristics of this dataset.\\nFigure 4 depicts the accuracy of the models for each epoch in the validation dataset. ViT achieves the highest accuracy, reaching approximately 95.39% at the 21th epoch. This result highlights its superior capacity to generalize and correctly classify samples. DeiT also demonstrates commendable performance, attaining an accuracy of 79.57%, which reflects the effectiveness of its teacher-student distillation approach. However, BEiT underperforms significantly, with its accuracy plateauing at 19.67%. This limitation suggests that BEiT’s self-supervised pretraining and tokenization mechanisms might not be optimally aligned with the dataset’s characteristics.\\nFigure 5 shows the F1 score, a metric that balances precision and recall in the validation dataset. Here, ViT once again leads, achieving a final F1 score close to 0.8845, signifying its strength in handling imbalanced data distributions. DeiT performs competitively, reaching an F1 score of 0.8706, which underscores its reliability in maintaining a balance between precision and recall. Conversely, BEiT shows slower growth in its F1 score, culminating at a much lower value of 0.6801. This underperformance indicates challenges in adapting its masked patch prediction mechanism to this particular task.\\nFigure 6 illustrates the Pearson Correlation Coefficient (PCC), which measures the linear correlation between predicted and actual values in the validation dataset. ViT stands out with the highest PCC value of 0.8971, reflecting its ability to accurately capture the underlying data patterns. DeiT follows with a PCC value of 0.5770, showing its effectiveness in preserving predictive accuracy. BEiT, however, achieves a lower PCC value of 0.2178, highlighting its limitations in modeling the linear relationships between inputs and outputs. This could be due to suboptimal pretraining or insufficient alignment with the dataset’s structure.\\nThe evaluation metrics reported in Figs. 4 , 5 , and 6 were computed as follows: accuracy and F1-score were calculated for each toxic endpoint individually and then averaged across all endpoints to provide a single representative value for each method. This approach ensures a comprehensive evaluation of model performance across the validation dataset. For the Pearson Correlation Coefficient (PCC), the computation was performed by comparing the predicted toxicity values with the ground truth values across all samples and endpoints. The reported PCC values represent the overall correlation between predicted and actual toxicity levels, providing a measure of the model’s predictive consistency.', 'Table 4 presents a detailed performance comparison of three models-ViT, BEiT, and DeiT-evaluated across three key metrics in the test dataset. The accuracy, F1 score, and Pearson Correlation Coefficient (PCC) indicate the performance of the proposed method on the test dataset. Regarding accuracy, ViT achieves an impressive 87.2%, clearly outperforming both DeiT (86.84%) and BEiT (65.77%). This demonstrates the robustness of ViT in correctly classifying data. The F1 score, which provides a balance between precision and recall, is also highest for ViT at 0.860. DeiT closely follows with a score of 0.8007, showing its ability to maintain a good balance, while BEiT scores the lowest at 0.8408. Finally, the Pearson Correlation Coefficient (PCC), which measures the linear correlation between predictions and actual values, is highest for ViT at 0.9192 DeiT achieves a moderate PCC of 0.8311, while BEiT records the lowest value of 0.5660, indicating its limited ability to align predictions with ground truth.\\nOverall, the results demonstrate that ViT outperforms both BEiT and DeiT across all metrics, excelling in efficiency, accuracy, and correlation with true values. DeiT shows reasonable performance, particularly in the F1 score, but remains behind ViT in other metrics. BEiT, despite leveraging unique pretraining strategies, underperforms significantly, especially in accuracy and PCC, likely due to its reliance on domain-specific data.\\nTable 4 Performance comparison by multi-modal models in the test dataset.\\nFull size table\\nTo evaluate the effectiveness of the proposed multimodal approach, a comparative analysis was conducted against two widely used machine learning models, LightGBM 43 and XGBoost 44 . LightGBM is a gradient boosting framework that uses histogram-based learning to enhance training efficiency and reduce memory consumption, making it well-suited for large-scale datasets. XGBoost, another gradient boosting framework, employs a sparsity-aware algorithm and weighted quantile sketching to improve computational speed and model performance, particularly in handling missing values and imbalanced data. The proposed method integrates 2D chemical structure images and tabular (CSV) data using a Vision Transformer (ViT)-based architecture, whereas LightGBM and XGBoost rely solely on tabular data. These models were selected due to their extensive use in chemical property prediction tasks based on structured numerical features. For a fair comparison, hyperparameters for LightGBM and XGBoost were optimized through grid search. The performance of each model on the test dataset is summarized in Table 5 .\\nTable 5 Performance comparison by the proposed method (ViT), LightGBM, XGBoost in the test dataset.\\nFull size table\\nAs shown in Table 5 , the proposed multimodal ViT-based approach demonstrates superior performance compared to LightGBM and XGBoost across all evaluation metrics. The multimodal model achieves an accuracy of 0.872, significantly exceeding that of LightGBM (0.661) and XGBoost (0.676). Additionally, the F1 Score of 0.860 indicates a well-balanced classification performance, while the Pearson Correlation Coefficient (PCC) of 0.9192 reflects a strong correlation between predictions and ground truth, highlighting the model’s robustness.\\nThe observed performance improvements can be attributed to the multimodal nature of the proposed approach, which effectively integrates structural and numerical information to enhance predictive accuracy. Unlike LightGBM and XGBoost, which rely solely on tabular data, the ViT-based model leverages chemical structure images, enabling it to capture complex spatial and relational patterns that are otherwise difficult to model using traditional feature-based methods.', '### Contribution\\n\\nTo address the limitations of traditional machine learning techniques in predicting specific toxicity endpoints, this study integrates both numerical and image-based chemical data. The proposed deep learning framework combines the Vision Transformer (ViT) for molecular structure images and a Multilayer Perceptron (MLP) for numerical properties. This fusion-based approach enhances feature representation and predictive accuracy, mitigating the constraints posed by insufficient toxicity data. Furthermore, by employing a multi-label learning strategy, the model effectively generalizes across diverse toxicological endpoints, which is particularly beneficial given the fragmented nature of available datasets. While ensemble models are often employed to enhance generalization, they may not entirely resolve the challenges associated with data scarcity.\\n\\n### Averaging methods in multi-label classification', '### Averaging methods in multi-label classification\\n\\nIn our research, we opted for the samples averaging method because it aligns with our focus on evaluating the toxicity label prediction performance of each chemical compound sample individually. Specifically, we aimed to assess how accurately the model predicted multiple toxicity labels for each sample. Our experimental results indicated that the samples averaging method yielded superior performance compared to other averaging methods. This suggests that our model effectively predicts the toxicity labels for each chemical compound sample individually.\\nThe datasets used in this work were sourced from the Chemical Safety Information provided by Korea’s Ministry of Environment, which focuses on chemicals of emerging regulatory concern within the region. These substances have not been systematically studied in prior research, particularly in the context of deep learning for toxicity prediction. By prioritizing these datasets, our goal was to contribute novel insights into localized chemical risks, which global databases may not fully represent due to differences in regulatory frameworks or regional exposure patterns.\\nImportantly, the toxicity data was structured into six predefined symptom categories (General, Inhalation, Dermal, Ocular, Oral, Other) to align with Korea’s standardized chemical safety reporting guidelines. This intentional limitation allowed us to train models on symptom-specific toxicity profiles that reflect real-world regulatory needs, rather than aggregating heterogeneous data from diverse sources. While this approach narrows the scope, it ensures practical relevance for regional risk assessment and management.\\nRegarding molecular images, we complemented the Korea-specific toxicity labels with structural data crawled from PubChem and eChemPortal. This hybrid methodology ensured the accuracy of molecular representations while maintaining our focus on locally relevant toxicity profiles. We believe this integration of regionally curated toxicity data with globally validated structural information represents a unique strength of our dataset, balancing specificity and scientific rigor.\\nWhile it is true that larger datasets generally facilitate more robust model training, we employed several strategies to mitigate the limitations posed by the dataset’s size and achieve satisfactory performance. Firstly, we leveraged transfer learning by utilizing a pre-trained Vision Transformer (ViT) model. This approach allowed us to benefit from features learned on a large-scale image dataset, effectively compensating for the limited size of our own dataset. The pre-trained ViT model provided a strong foundation for feature extraction, which was then fine-tuned on our specific task. Secondly, we employed regularization methods, including dropout and weight decay, to prevent overfitting. These techniques helped to constrain the model’s complexity and improve its generalization ability. Thirdly, we utilized early stopping during the training process. By monitoring the model’s performance on a validation set, we were able to halt training when performance began to plateau or decline, preventing the model from overfitting to the training data. Fourthly, given the multi-label nature of our toxicity prediction task, we implemented multi-label learning strategies. This allowed us to train the model to predict multiple toxicity labels simultaneously, capturing the interdependencies between different toxicity categories. Lastly, we integrated numerical and categorical features alongside the image data. This multimodal approach provided the model with additional contextual information, further enhancing its predictive capabilities. Despite the dataset’s size, our approach effectively addressed the challenges of deep learning on limited data. We recognize that increasing the dataset size would likely lead to further improvements in model performance.\\nWe applied zero normalization to the entire dataset. To ensure a more appropriate evaluation for imbalanced data, we additionally assessed model performance using balanced accuracy. The results demonstrated minimal variation (0.001–0.002) compared to accuracy, indicating a negligible impact on overall model evaluation. Furthermore, the dataset consists exclusively of toxic compounds, with no inclusion of non-toxic compounds. Consequently, classification between toxic and non-toxic compounds was not applicable in this study. A comprehensive list of toxic compounds has been provided in Table 8 of the “Appendix” for reference.\\n\\n### Sensitivity of multi-modal models to fusion order', '### Sensitivity of multi-modal models to fusion order\\n\\nTo investigate the impact of fusion order when integrating molecular structure images and properties into 1D vectors, additional experiments were conducted by altering the sequence from “Image + Property” to “Property + Image.”\\nTable 6 Performance comparison by multi-modal models with different fusion orders in the test dataset.\\nFull size table\\nTable 6 presents the performance comparison of multi-modal models on the test dataset. The results indicate that BEiT exhibits an improved F1 score (0.8741) compared to its previous configuration, suggesting that it benefits from the modified fusion order, potentially due to its robust representation learning mechanism. In contrast, ViT and DeiT maintain relatively stable performance across different fusion orders, with only minor variations in accuracy, F1 score, and Pearson Correlation Coefficient (PCC). These findings highlight that the influence of fusion order on model performance varies depending on the architecture. BEiT demonstrates greater sensitivity to the fusion sequence, while ViT and DeiT appear to be less affected. This observation provides valuable insights for optimizing fusion strategies in multi-modal molecular property prediction tasks.\\n\\n### Predefined rule for using molecular structure images\\n\\nAll images were first converted to the PNG format to preserve visual fidelity and maintain format uniformity. They were then resized to a fixed resolution of 224 × 224 pixels using bilinear interpolation, a choice driven by the input requirements of the Vision Transformer (ViT) architecture adopted in this work. To align with the preprocessing conventions of pre-trained vision models, images were converted to RGB color mode, and pixel values were normalized using the mean ([0.485, 0.456, 0.406]) and standard deviation ([0.229, 0.224, 0.225]) parameters widely employed in the computer vision community.\\n\\n### Robustness to image transformations in molecular representations\\n\\nThe Vision Transformer (ViT) processes images by dividing them into small patches and converting them into 1D vectors. Since this tokenization approach can be sensitive to transformations, ensuring consistency in molecular image orientations is crucial for stable representation learning.\\nTo address this, we adopted standardized molecular depiction rules that generate 2D molecular structure images with fixed orientations. This standardization minimizes variations caused by arbitrary rotations and maintains structural consistency across samples. Additionally, the multimodal nature of the proposed method integrates molecular structure images with textual and numerical chemical features, enhancing overall robustness. By leveraging information from multiple modalities, the model reduces the impact of perturbations in any single input representation.\\n\\n### Impact of image rotation on model performance\\n\\nTable 7 presents a comparison between the proposed method with and without image rotation applied. Interestingly, our experiments revealed that incorporating image rotation led to a decline in performance. While image augmentation techniques generally enhance performance in standard image datasets, we hypothesize that in the case of chemical molecular structure images, rotation alters the perception of molecular bonds, leading to misinterpretations and performance degradation.\\nTable 7 Performance comparison of the proposed method with and without image rotation on the test dataset.\\nFull size table\\n\\n### Application domain', '### Application domain\\n\\nThe accurate prediction of chemical toxicity is a critical challenge in interdisciplinary research, particularly in contexts requiring robust decision-making under constraints such as limited data availability and structural complexity. While the proposed multimodal deep learning framework demonstrates promising performance in multi-label toxicity prediction, it is essential to address the applicability domain (AD) of the model to ensure its reliability and interpretability in real-world scenarios.\\nThe concept of an applicability domain, as emphasized in OECD guidelines 45 and related literature 46 , 47 , defines the boundaries within which a predictive model operates with validated confidence. These boundaries are determined by the chemical space covered by the training data, the representativeness of molecular features, and the mechanistic relevance of the model to the endpoints being predicted. In this study, the integrated dataset-curated from diverse sources and normalized for structural, numerical, and toxicological consistency-aims to expand the chemical space coverage compared to domain-specific datasets (e.g., genotoxicity-only or acute toxicity-focused studies). However, the structural diversity of chemicals and the inherent biases in data sources (e.g., security-related restrictions) necessitate a rigorous definition of the AD to avoid extrapolation beyond the model’s validated scope.\\nTo address this, the proposed framework incorporates two key AD-related considerations. First, the feature fusion mechanism combining Vision Transformer (ViT)-extracted molecular image embeddings and MLP-processed numerical descriptors inherently encodes chemical similarity metrics. This dual-stream architecture ensures that predictions are anchored in both structural and physicochemical property spaces, aligning with methodologies 48 for AD definition. Second, the model’s training protocol emphasizes chemical diversity through stratified sampling and data augmentation, reducing overfitting to narrow subspaces.\\nFurthermore, the multi-label prediction capability necessitates a toxicity endpoint-specific AD analysis. For instance, predictions for acute oral toxicity may rely more heavily on specific physicochemical descriptors (e.g., logP, molecular weight), while structural alerts identified via ViT-based image analysis could dominate predictions for genotoxicity. This aligns with the ICCVAM 49 recommendation for defining context-specific validity boundaries. Future work will involve implementing quantitative AD metrics, such as leverage-based approaches or distance-to-model measures, to provide explicit confidence intervals for individual predictions. In conclusion, while the proposed model achieves high accuracy (0.872) and strong correlation (PCC: 0.9192) in toxicity prediction, its real-world utility depends on transparently communicating the AD. Adherence to OECD and ICCVAM guidelines ensures that stakeholders can evaluate the model’s suitability for specific chemicals or regulatory purposes, ultimately enhancing trust in AI-driven toxicological assessments.\\n\\n### Limitation and future research', '### Limitation and future research\\n\\nOne major limitation of the proposed multimodal model is its applicability domain. Since the model integrates both image and structured data for classification, its prediction accuracy may be restricted when input data deviates significantly from the training distribution. This issue is inherent in deep learning models, as they rely on learned representations from a finite dataset and may struggle with out-of-distribution (OOD) samples.\\nIn our study, the dataset used for training the classification model only includes labeled toxicity data, meaning that the model is constrained to making predictions within the provided dataset range. When a compound lacks a recorded toxicity endpoint, it is not explicitly assigned a 0 label; rather, the classification model predicts the most probable toxicity class based on learned patterns. This approach aligns with the fundamental nature of deep learning models, which do not inherently handle missing labels but instead infer the most probable category given the available information. While this method enables the model to generalize within the training distribution, it also highlights the need for strategies to handle missing or uncertain labels effectively.\\nTo address this limitation, future research will focus on defining the applicability domain of the model and implementing mechanisms to detect and handle OOD inputs. Possible approaches include uncertainty estimation techniques such as Monte Carlo dropout and Bayesian neural networks, as well as distance-based methods like Mahalanobis distance or outlier detection algorithms. By incorporating these strategies, we aim to enhance the model’s robustness and reliability when dealing with unseen or atypical data.\\nAdditionally, expanding the dataset to include a more diverse range of chemical substances will help improve the generalizability of the model. We also plan to explore domain adaptation techniques to mitigate performance degradation when applied to new chemical categories. Despite these limitations, our study presents a novel contribution by introducing a multimodal classification approach in the field of chemical toxicity prediction. We believe that further research in applicability domain estimation will complement our proposed method and contribute to its practical deployment in real-world scenarios.\\nIn this paper, we proposed a multi-modal deep learning framework for chemical toxicity prediction, combining molecular structure images with chemical property data to improve predictive accuracy. The experimental results validated the superiority of our approach, particularly the Vision Transformer (ViT) model. Among the three models tested-ViT, BEiT, and DeiT-ViT consistently outperformed the others across all key metrics. In terms of accuracy, ViT achieved an impressive 87.2%, surpassing DeiT (86.84%) and BEiT (65.77%). The model also excelled in F1 score (0.86) and Pearson Correlation Coefficient (PCC) (0.9192), further confirming its superior performance. These findings highlight the ability of ViT to effectively handle multi-modal data, combining both image-based features and numerical data for more accurate chemical toxicity predictions. The ability to predict multiple toxicity endpoints, including general toxicity, dermal toxicity, ocular toxicity, and oral toxicity, further strengthens the model’s applicability in various real-world contexts, such as environmental safety and public health.\\nLooking forward, this research opens several avenues for future studies. One potential direction is the exploration of additional data modalities, such as toxicological data from clinical studies or environmental exposure data, which could further enhance the model’s performance. Incorporating these data sources might allow for more nuanced predictions and a deeper understanding of the relationships between chemical structures and their toxicological impacts. Another area for improvement lies in expanding the model’s generalizability. While our framework demonstrated exceptional results on the dataset used, its applicability to other chemical domains or novel substances requires further testing. Future work may involve fine-tuning the model through transfer learning techniques, allowing it to adapt to new datasets with limited data. Furthermore, integrating real-time sensor data from wearable devices for continuous toxicity monitoring could pave the way for dynamic and on-site toxicity assessments in both civilian and military applications. Overall, while our study demonstrates a significant step forward in toxicity prediction, there remains much room for growth in refining and scaling the framework to address broader challenges in toxicological research and safety management.\\n\\n## Data availability', 'The data used to support the findings of this study will be available from the corresponding author upon request after acceptance.\\n- Pérez Santín, E. et al. Toxicity prediction based on artificial intelligence: A multidisciplinary overview. Wiley Interdisc. Rev. Comput. Mole. Sci. 11 (5), 1516 (2021). Article Google Scholar\\n- Tran, T. T. V., Surya Wibowo, A., Tayara, H. & Chong, K. T. Artificial intelligence in drug toxicity prediction: Recent advances, challenges, and future perspectives. J. Chem. Inf. Model. 63 (9), 2628–2643 (2023). Article CAS PubMed Google Scholar\\n- Cavasotto, C. N. & Scardino, V. Machine learning toxicity prediction: Latest advances by toxicity end point. ACS Omega 7 (51), 47536–47546 (2022). Article CAS PubMed PubMed Central Google Scholar\\n- Wu, Z. et al. Mining toxicity information from large amounts of toxicity data. J. Med. Chem. 64 (10), 6924–6936 (2021). Article CAS PubMed Google Scholar\\n- Sharma, N., Naorem, L. D., Jain, S. & Raghava, G. P. Toxinpred2: An improved method for predicting toxicity of proteins. Brief. Bioinform. 23 (5), bbac174 (2022). Article PubMed Google Scholar\\n- F.\\xa0Berg and S.\\xa0Kappler, Future biological and chemical weapons. In Ciottone’s Disaster Medicine , pp.\\xa0520–530 (Elsevier, 2024).\\n- Gustavsson, M. et al. Transformers enable accurate prediction of acute and chronic chemical toxicity in aquatic organisms. Sci. Adv. 10 (10), 6669 (2024). Article Google Scholar\\n- Stucki, A. O. et al. Use of new approach methodologies (NAMS) to meet regulatory requirements for the assessment of industrial chemicals and pesticides for effects on human health. Front. Toxicol. 4 , 964553 (2022). Article PubMed PubMed Central Google Scholar\\n- Syafrudin, M. et al. Pesticides in drinking water: A review. Int. J. Environ. Res. Public Health 18 (2), 468 (2021). Article CAS PubMed PubMed Central Google Scholar\\n- Lin, Z. & Chou, W.-C. Machine learning and artificial intelligence in toxicological sciences. Toxicol. Sci. 189 (1), 7–19 (2022). Article MathSciNet CAS PubMed PubMed Central Google Scholar\\n- Badwan, B.\\xa0A., Liaropoulos, G., Kyrodimos, E., Skaltsas, D., Tsirigos, A., Gorgoulis, V.G.: Machine learning approaches to predict drug efficacy and toxicity in oncology. Cell Rep. Methods , 3(2) (2023).\\n- Huang, T. et al. Quantitative structure-activity relationship (QSAR) studies on the toxic effects of nitroaromatic compounds (NACS): A systematic review. Int. J. Mol. Sci. 22 (16), 8557 (2021). Article CAS PubMed PubMed Central Google Scholar\\n- Kurani, A., Doshi, P., Vakharia, A. & Shah, M. A comprehensive comparative study of artificial neural network (ANN) and support vector machines (SVM) on stock forecasting. Ann. Data Sci. 10 (1), 183–208 (2023). Article Google Scholar\\n- Khajavi, H. & Rastgoo, A. Predicting the carbon dioxide emission caused by road transport using a random forest (rf) model combined by meta-heuristic algorithms. Sustain. Cities Soc. 93 , 104503 (2023). Article Google Scholar\\n- Isnain, A. R., Supriyanto, J. & Kharisma, M. P. Implementation of k-nearest neighbor (k-nn) algorithm for public sentiment analysis of online learning. Indones. J. Comput. Cybern. Syst. 15 (2), 121–130 (2021). Article Google Scholar\\n- Trinh, T. X., Seo, M., Yoon, T. H. & Kim, J. Developing random forest based QSAR models for predicting the mixture toxicity of tio2 based nano-mixtures to daphnia magna. NanoImpact 25 , 100383 (2022). Article CAS PubMed Google Scholar\\n- Sharma, B. et al. Accurate clinical toxicity prediction using multi-task deep neural nets and contrastive molecular explanations. Sci. Rep. 13 (1), 4908 (2023). Article CAS PubMed PubMed Central ADS Google Scholar\\n- Sun, M. et al. Graph convolutional networks for computational drug development and discovery. Brief. Bioinform. 21 (3), 919–935 (2020). Article PubMed Google Scholar\\n- Schwartz, J., Awale, M. & Reymond, J.-L. Smifp (smiles fingerprint) chemical space for virtual screening and visualization of large databases of organic molecules. J. Chem. Inf. Model. 53 (8), 1979–1989 (2013). Article CAS PubMed Google Scholar\\n- Hirohara, M., Saito, Y., Koda, Y., Sato, K. & Sakakibara, Y. Convolutional neural network based on smiles representation of compounds for detecting chemical motif. BMC Bioinform. 19 , 83–94 (2018). Article Google Scholar\\n- Schneider, M., Pons, J.-L., Labesse, G. & Bourguet, W. In silico predictions of endocrine disruptors properties. Endocrinology 160 (11), 2709–2716 (2019). Article CAS PubMed PubMed Central Google Scholar\\n- Liu, P., Ren, Y., Tao, J. & Ren, Z. Git-mol: A multi-modal large language model for molecular science with graph, image, and text. Comput. Biol. Med. 171 , 108073 (2024). Article CAS PubMed Google Scholar\\n- D.\\xa0Alexey, An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010.11929 (2020)\\n- Imambi, S., Prakash, K.B., Kanagachidambaresan, G. Pytorch, Programming with TensorFlow: Solution for Edge Computing Applications , pp.\\xa087–104 (2021).', '- Imambi, S., Prakash, K.B., Kanagachidambaresan, G. Pytorch, Programming with TensorFlow: Solution for Edge Computing Applications , pp.\\xa087–104 (2021).\\n- Pang, B., Nijkamp, E. & Wu, Y. N. Deep learning with tensorflow: A review. J. Educ. Behav. Stat. 45 (2), 227–248 (2020). Article Google Scholar\\n- Parker, N. A. Rapid and Spatially Explicit Assessment of Contaminants of Emerging Concern in Data Limited Watersheds (University of California, Santa Barbara, 2023). Google Scholar\\n- Ma, H. et al. Unveiling the structure-surface energy relationship of zeolites through machine learning. J. Phys. Chem. C 128 (36), 14927–14936 (2024). Article CAS Google Scholar\\n- Guo, J., Woo, V., Andersson, D.A., Hoyt, N., Williamson, M., Foster, I., Benmore, C., Jackson, N.E., Sivaraman, G.: Al4gap: Active learning workflow for generating dft-scan accurate machine-learning potentials for combinatorial molten salt mixtures. J. Chem. Phys. , 159 (2) (2023).\\n- Rostamkhani, N. et al. Enhanced anti-tumor and anti-metastatic activity of quercetin using ph-sensitive alginate@ zif-8 nanocomposites: in vitro and in vivo study. Nanotechnology 35 (47), 475102 (2024). Article CAS Google Scholar\\n- Galatro, D., Dawe, S.: Data-based modelling for prediction. In Data Analytics for Process Engineers: Prediction, Control and Optimization , pp.\\xa059–105 (Springer, 2023).\\n- Jaafar, S. M. & Sukri, R. S. Data on the physicochemical characteristics and texture classification of soil in Bornean tropical heath forests affected by exotic acacia mangium. Data Brief 51 , 109670 (2023). Article CAS PubMed PubMed Central Google Scholar\\n- Tetley, M.J. The Distribution, Ecological Niche Modelling and Habitat Suitability Mapping of the Minke Whale (Balaenoptera acutorostrata) within the North Atlantic . Bangor University (United Kingdom) (2010).\\n- Nitharshni, J., Nilasruthy, R., Shakthi Akshaiya, K. & Rajavel, M. Quality check of water for human consumption using machine learning. Adv. Sci. Technol. 124 , 574–589 (2023). Article Google Scholar\\n- Baum, Z. J. et al. Artificial intelligence in chemistry: Current trends and future directions. J. Chem. Inf. Model. 61 (7), 3197–3212 (2021). Article CAS PubMed Google Scholar\\n- Kang, Y., Cai, Z., Tan, C.-W., Huang, Q. & Liu, H. Natural language processing (NLP) in management research: A literature review. J. Manage. Anal. 7 (2), 139–172 (2020). Google Scholar\\n- Bisong, E.: Introduction to scikit-learn. In Building Machine Learning and Deep Learning Models on Google Cloud Platform: A Comprehensive Guide for Beginners , pp.\\xa0215–229 (Springer, 2019).\\n- Guan, L.: Weight prediction boosts the convergence of adamw. In Pacific-Asia Conference on Knowledge Discovery and Data Mining , pp.\\xa0329–340 (Springer, 2023).\\n- Yacouby, R., Axman, D.: Probabilistic extension of precision, recall, and f1 score for more thorough evaluation of classification models. In Proceedings of the First Workshop on Evaluation and Comparison of NLP Systems , pp.\\xa079–91 (2020).\\n- Benesty, J., Chen, J. & Huang, Y. On the importance of the Pearson correlation coefficient in noise reduction. IEEE Trans. Audio Speech Lang. Process. 16 (4), 757–765 (2008). Article Google Scholar\\n- Yue, X., Sun, S., Kuang, Z., Wei, M., Torr, P.H., Zhang, W., Lin, D.: Vision transformer with progressive sampling. In Proceedings of the IEEE/CVF International Conference on Computer Vision , pp.\\xa0387–396 (2021).\\n- Bao, H., Dong, L., Piao, S., Wei, F.: Beit: Bert pre-training of image transformers, arXiv preprint arXiv:2106.08254 (2021).\\n- Touvron, H., Cord, M., Jégou, H.: Deit III: Revenge of the vit. In European Conference on Computer Vision , pp.\\xa0516–533 (Springer, 2022).\\n- Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q., Liu, T.-Y.: Lightgbm: A highly efficient gradient boosting decision tree. Adv. Neural Inf. Process. Syst. 30 (2017).\\n- Chen, T., Guestrin, C.: Xgboost: A scalable tree boosting system. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , pp.\\xa0785–794 (2016).\\n- O.\\xa0for Economic Co-operation and Development, Guidance document on the validation of (quantitative) structure-activity relationship [(Q) SAR] models . Organisation for Economic Co-operation and Development (2014).\\n- Hanser, T., Barber, C., Marchaland, J. & Werner, S. Applicability domain: Towards a more formal definition. SAR QSAR Environ. Res. 27 (11), 865–881 (2016). Article CAS Google Scholar\\n- Kar, S., Roy, K., Leszczynski, J.: Applicability domain: A step toward confident predictions and decidability for qsar modeling. Comput. Toxicol. Methods Protocols , pp.\\xa0141–169 (2018).\\n- Sahigara, F. et al. Comparison of different approaches to define the applicability domain of QSAR models. Molecules 17 (5), 4791–4810 (2012). Article CAS PubMed PubMed Central Google Scholar', '- Sahigara, F. et al. Comparison of different approaches to define the applicability domain of QSAR models. Molecules 17 (5), 4791–4810 (2012). Article CAS PubMed PubMed Central Google Scholar\\n- Validation, Q.: Regulatory acceptance of new approach methodologies. In A Report of the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM) Validation Workgroup (2023).\\nDownload references\\nThis study was supported by Future Strategy and Technology Research Institute of the Korea Military Academy (2025-NWMD-05) and the National Research Foundation of Korea (NRF) grant, funded by the Korean government (MSIT) (RS-2025-00516065).', '## Author information\\n\\n### Authors and Affiliations\\n\\n- ROK Army Signal School, Daejeon, 34059, South Korea Jiwon Hong\\n- Department of Artificial Intelligence and Data Science, Korea Military Academy, Seoul, 01805, South Korea Hyun Kwon\\n- Jiwon Hong View author publications Search author on: PubMed Google Scholar\\n- Hyun Kwon View author publications Search author on: PubMed Google Scholar\\n\\n### Corresponding author\\n\\nCorrespondence to Hyun Kwon .\\n\\n## Ethics declarations\\n\\n### Competing interests\\n\\nThe authors declare that there are no conflicts of interest regarding the publication of this article.\\n\\n### Ethical approval\\n\\nAll authors give ethical and informed consent.\\n\\n## Additional information\\n\\n### Publisher’s note\\n\\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\\nIn Table 8 , each of the 4179 data entries contains information on six symptom-related columns: General Symptoms, Inhalation, Dermal, Ocular, Oral, and Other. Each symptom column may include multiple symptoms per data entry.\\nTo clarify, the positive count refers to the number of times a particular symptom appears within its respective category, while the negative count is obtained by subtracting the positive occurrences from the total number of data points (4179).\\nFor further illustration, consider a single data entry (Sample A). This sample may exhibit the following symptoms across different categories:\\n- General Symptoms : Irritation, Headache, Vomiting\\n- Inhalation : Death, Coma, Edema, Dizziness\\n- Dermal : Blisters, Edema\\n- Ocular : Irritation, Congestion, Corneal Damage, Tearing\\n- Oral : Vomiting\\n- Other : Coma\\nTable 8 provides a summary of the toxicity endpoints across these six symptom categories, detailing the number of unique symptom labels and the top 10 most frequently occurring symptoms within each category. For instance, in the General Symptoms category, there are 50 unique symptom labels, with the most frequently occurring symptoms including Irritation (844 occurrences), Burns (381 occurrences), Dizziness (212 occurrences), etc.\\nTable 8 Summary of toxicity endpoints across symptom categories, including the number of unique symptom labels and the top 10 most frequently occurring symptoms within each category.\\nFull size table\\n\\n## Rights and permissions\\n\\nOpen Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ .\\nReprints and permissions\\n\\n## About this article\\n\\n### Cite this article\\n\\nHong, J., Kwon, H. Multimodal deep learning for chemical toxicity prediction and management. Sci Rep 15 , 19491 (2025). https://doi.org/10.1038/s41598-025-95720-5\\nDownload citation\\n- Received : 02 January 2025\\n- Accepted : 24 March 2025\\n- Published : 03 June 2025\\n- DOI : https://doi.org/10.1038/s41598-025-95720-5\\n\\n### Share this article\\n\\nAnyone you share the following link with will be able to read this content:\\nGet shareable link\\nSorry, a shareable link is not currently available for this article.\\nCopy to clipboard\\nProvided by the Springer Nature SharedIt content-sharing initiative\\n- Chemical substances\\n- Chemical safety management\\n- Biochemistry\\n- Vision Transformer (ViT)\\n- Wearable sensors', 'J Cheminform\\n. 2024 Nov 18;16:129. doi:\\n10.1186/s13321-024-00916-y\\n\\n# A systematic review of deep learning chemical language models in recent era\\n\\nHector Flores-Hernandez\\n\\n### Hector Flores-Hernandez\\n\\nTecnológico de Monterrey, School of Engineering and Sciences, Monterrey, 64710 Nuevo León México\\nFind articles by\\nHector Flores-Hernandez\\nEmmanuel Martinez-Ledesma\\n\\n### Emmanuel Martinez-Ledesma\\n\\nTecnológico de Monterrey, School of Medicine and Health Sciences, Monterrey, 64710 Nuevo León México\\nInstitute for Obesity Research, Tecnológico de Monterrey, Monterrey, 64710 Nuevo León México\\nFind articles by\\nEmmanuel Martinez-Ledesma\\n- Author information\\n- Article notes\\n- Copyright and License information\\nTecnológico de Monterrey, School of Engineering and Sciences, Monterrey, 64710 Nuevo León México\\nTecnológico de Monterrey, School of Medicine and Health Sciences, Monterrey, 64710 Nuevo León México\\nInstitute for Obesity Research, Tecnológico de Monterrey, Monterrey, 64710 Nuevo León México\\nCorresponding author.\\nReceived 2024 Jul 26; Accepted 2024 Oct 17; Collection date 2024.\\n© The Author(s) 2024\\nOpen Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/ .\\nPMC Copyright notice\\nPMCID: PMC11571686\\xa0\\xa0PMID:\\nDiscovering new chemical compounds with specific properties can provide advantages for fields that rely on materials for their development, although this task comes at a high cost in terms of complexity and resources. Since the beginning of the data age, deep learning techniques have revolutionized the process of designing molecules by analyzing and learning from representations of molecular data, greatly reducing the resources and time involved. Various deep learning approaches have been developed to date, using a variety of architectures and strategies, in order to explore the extensive and discontinuous chemical space, providing benefits for generating compounds with specific properties. In this study, we present a systematic review that offers a statistical description and comparison of the strategies utilized to generate molecules through deep learning techniques, utilizing the metrics proposed in Molecular Sets (MOSES) or Guacamol. The study included 48 articles retrieved from a query-based search of Scopus and Web of Science and 25 articles retrieved from citation search, yielding a total of 72 retrieved articles, of which 62 correspond to chemical language models approaches to molecule generation and other 10 retrieved articles correspond to molecular graph representations. Transformers, recurrent neural networks (RNNs), generative adversarial networks (GANs), Structured Space State Sequence (S4) models, and variational autoencoders (VAEs) are considered the main deep learning architectures used for molecule generation in the set of retrieved articles. In addition, transfer learning, reinforcement learning, and conditional learning are the most employed techniques for biased model generation and exploration of specific chemical space regions. Finally, this analysis focuses on the central themes of molecular representation, databases, training dataset size, validity-novelty trade-off, and performance of unbiased and biased chemical language models. These themes were selected to conduct a statistical analysis utilizing graphical representation and statistical tests. The resulting analysis reveals the main challenges, advantages, and opportunities in the field of chemical language models over the past four years.\\nKeywords: Chemical language models (CLMs), Recurrent neural networks (RNNs), Transformers, Variational autoencoders (VAEs), Generative adversarial networks (GANs), Transfer learning, Reinforcement learning and conditional learning\\n\\n## Introduction\\n\\n### Chemical space and de novo molecule design', 'Molecule design aims to discover chemical entities that are distributed within a vast, intricate, and discontinuous space known as chemical space, which encompasses all possible atomic configurations that can produce molecules [ 1 ]. The search for molecules with specific properties in chemical space is a time-consuming and expensive task due to the irregular distribution of molecules, where even slight changes in a molecule can result in significant changes in physicochemical properties [ 2 ]. For years, researchers considered molecule design as a process based on trial and error to explore various arrangements of functional groups or atoms, yielding molecules with diverse structures that can map regions of chemical space. This approach relied heavily on human knowledge and was limited to the human ability to identify complex chemical patterns from structures [ 3 ]. However, since the introduction of computational methods in the field of chemical sciences and supported by the exponential growth of available molecular data (e.g. chemical structure, physicochemical properties, bioactivity, toxicity, and others), from the last decade, molecular design experienced one of the most important advances in its history, driven mainly by methods capable of learning from data to generate novel chemical entities with specific properties, such as deep learning [ 4 ].\\nDeep learning is a subset of machine learning that performs predictive or generative tasks, specifically involving learning representation methods that enable computers to understand how to represent data from its raw form by performing multiple nonlinear matrix multiplications to learn multiple levels of abstraction [ 5 ]. As such, deep learning can learn chemistry rules for prediction or generative tasks by using different molecular representations that can be understood in their natural form by humans such as Simplified Molecular Input Line Entry System (SMILES), IUPAC nomenclature or molecular graphs, molecular fingerprints, among others [ 6 – 9 ]. The implementation of learning-based method has proven to be successful in multiple fields of chemistry, such as Quantitative Structure\\u2009−\\u2009Activity Relationships (QSAR), Quantitative Structure–Property Relationships (QSPR), and molecular generative models.[ 10 , 11 ].\\nFurthermore, deep generative models are gaining rapid attention in the design of molecules. They possess the ability to learn implicit chemical knowledge from data by identifying structural patterns such as valency rules, reactive groups, molecular conformations, hydrogen bond donors and acceptors, among others to produce molecules with specific properties. Unlike hand-encoded rules-based or enumeration methods, which require human intervention to define chemical rules based on human knowledge to generate molecules, these models are independent and less prone to generating molecules that are unavailable for chemical synthesis due to unstable groups [ 12 , 13 ].\\nSeveral deep learning architectures like recurrent neural networks (RNNs), transformers, variational autoencoders (VAEs), and generative adversarial networks (GANs) offer an efficient way to investigate chemical space using statistical techniques. These models can generate targeted molecules and investigate regions of the chemical domain through biased learning methods. Such methods can manipulate the molecular generator to yield molecules that meet specific conditions, showing analogous structures and therefore chemical properties [ 14 – 17 ]. On the other hand, such models could map large areas of chemical space by solely acquiring chemical regulations to reconstruct molecular structures from encoded molecular spaces [ 18 , 19 ]. Deep generative models have become a useful tool for designing molecules due to their cost-effectiveness and time efficiency [ 20 – 22 ]. Due to the significance of deep learning models in contemporary molecule generation, it is imperative to use metrics for evaluating statistical methods that allow chemists and data scientists to compare the efficiency of different molecular generators. To address this requirement, benchmarking platforms have been introduced to quantify the quality and diversity of the distribution of generated molecules. Molecular Sets (MOSES) and GuacaMol are widely accepted benchmarks for measuring the quality, diversity, and fidelity of outputs generated by deep generative models, as well as their ability to explore chemical space [ 23 , 24 ].', 'Since the start of the deep learning era in molecular design, other works have summarized the architectures mentioned above in terms of their theoretical background and applications for drug development or statistical approaches to explore the chemical space [ 12 , 25 – 28 ]. Only a small portion of these reviews has systematically evaluated the implementation of deep learning architectures for molecular generation tasks. The primary challenges faced by deep learning architectures in molecule generation, as well as the most used deep learning architectures for de novo molecule generation and molecular representations, were examined through systematic evaluations of generative models [ 29 ]. In addition, Koutroumpa et al. conducted a systematic analysis of deep generative models to relate the validation of target molecules produced by these models in biological models. Deep generative models demonstrated their relevance in drug design and their capability for generating bioactive compounds, as evidenced in both in vitro and in vivo models [ 30 ]. Although these analyses are beneficial, it is essential to conduct a statistical evaluation of deep learning models using established standards like MOSES benchmark. This benchmarking platform can accurately reflect the robustness of models to generate novel, valid, and unique chemical entities.\\nThe present study aims to comprehensively investigate the quality of deep learning architectures over the last three years to generate chemical entities and to evaluate what are the most important features that affect the quality metrics of deep generative models through a systematic review. This work focuses on answering the following research questions:\\n- How does the configuration of deep learning architectures and training size affect the quality metrics of the generated molecules?\\n- Which deep learning architectures can achieve higher quality metrics of generated molecules?\\n- Which hyperparameters have a higher impact on each deep learning architecture for better molecule generation performance?\\n- What are the most common molecular representations for deep learning models?\\n- What type of biased deep generative methods are most effective in generating novel molecules that show activity for a given target?\\nTherefore, a comprehensive systematic review is presented herein, encompassing a set of articles that assessed MOSES or Guacamol metrics from 2020- June 2024. This work presents a theoretical background on deep generative models and metrics found in the set of retrieves articles, followed by a detailed explanation of methodology using Prisma for article selection is provided, and a discussion section is presented highlighting the most significant finding from the systematic review. Finally, a statistical analysis is performed to analyze the robustness of deep learning models on molecular generation tasks.', '### Theoretical background\\n\\nSince we aim to review, measure, and analyze deep learning techniques used in chemical language models, first we present some theoretical background of the concepts used in this area.\\n\\n### Molecule representations\\n\\nLearning is a data-driven process. From the beginning, when computer science was introduced to chemistry, chemists have attempted to represent chemical entities through different methods that enable computer algorithms to acquire knowledge on how to build molecules. The implementation of deep generative models in chemistry requires the use of an appropriate and precise molecular representation that provides enough information for computers to process and learn through matrix operations. Molecular representations must meet specific requirements to ensure an accurate representation of a real molecule. These requirements include permutation invariance to ensure no alteration by changes in the specified order of atoms, translational invariance to prevent changes from translations in space, and rotational invariance to avoid changes from rotation operations [ 25 ] . On this basis, various representations have been developed through the years for deep learning applications depending on the challenges that deep generative models face and the requirements of the generation tasks. These molecular representations are illustrated in Fig. 1 A.\\nOpen in a new tab\\nIllustration of molecular representations and chemical language models. A displays various molecular representations of propa-2-one (acetone). B showcases RNNs as chemical language models and their autoregressive approach for generating chemical entities where [SOS] and [EOS] stands for start of sentence and end of sentence tokens, respectively. RNNs cells are also shown where the symbols •, *, and\\u2009+\\u2009denote dot-product, elementwise matrix multiplication, and addition, respectively. Each arrow corresponds to matrix multiplication utilizing a learnable matrix. Finally, in the context of RNNs, ‘x’, ‘a’, and ‘c’ correspond to the input, information matrix, and memory term, respectively. C illustrates the schematic representation of VAEs, while D presents the schematic representation of GANs. E displays the transformer model proposed by Vaswani and colleagues.31\\n\\n### Molecular graphs\\n\\nIntroduced more than 30\\xa0years ago, molecular graphs are one of the most revolutionary concepts for representing the chemical identities of covalently bonded molecules [ 32 ]. This approach involves mapping the atoms and bonds of a molecule onto a set of nodes (V) and edges (E) in a square matrix (G\\u2009=\\u2009(V, E)), where the matrix size reflects the total number of non-hydrogen atoms, usually called adjacency matrix. The adjacency matrix first enumerates each atom in the structure and then presents information about the types of atoms in its main diagonal, identified by their atomic numbers. Connectivity is determined by assigning a value of 0 to non-adjacent atoms in the structure or 1 to adjacent pairs of atoms, even this one for adjacency can be replaced by another number (between 1 and 4) to indicate the type of bond (single, double, triple, or aromatic), increasing the amount of information encoded in the matrix [ 33 , 34 ]. Although molecular graphs are not memory efficient due to the large amount of information required to represent a single molecule, one-dimensional representations have been developed to overcome this limitation. These representations are based on strings, which require much less information and provide easily interpretable, human-friendly representations [ 35 ]. Additionally, one-dimensional representations have driven the rise of chemical language models in the recent era of deep learning in cheminformatics, changing the perspective of molecule generation.\\n\\n### Molecular representation for chemical language models\\n\\nNowadays, language models play a pivotal role in designing molecules through deep learning models [ 36 ]. Chemical language models, which use one-dimensional molecule representations as inputs, can generate molecules and learn the syntax, coherence, and grammar rules necessary to build them through training. These models train architectures using one-dimensional string representations of molecules [ 37 , 38 ]. This section offers a concise overview of the most frequently used molecular representations for chemical language models.\\n\\n### Simplified molecular input line entry specification (SMILES)', \"### Simplified molecular input line entry specification (SMILES)\\n\\nThe SMILES notation is a prevalent method employed for molecular representation in the field of deep learning. This approach has been in practice for over three decades and it is comprised of character strings in ASCII (American Standard Code for Information Interchange) format. SMILES enables molecules to be represented by a series of tokens based on their chemical structure, where each element in the periodic table is assigned to a corresponding token using its atomic symbol. In the absence of a specified bond type, the bond type can be inferred as a single bond type. Conversely, when tokens appear in lowercase form, the bond type is identified as aromatic. Otherwise, the bond type is explicitly indicated using non-alphanumeric tokens. Additionally, SMILES employs a special token (brackets) to specify branches or cycles within chemical structures. These notations and rules for molecular representation can be considered as an analogous form of language (chemical language) where words are present in the form of chemical tokens and sentences are molecules. However, similar to spoken languages, SMILES carries the potential for syntactic and grammar errors when dealing with branches and cycles, presenting a challenge for deep learning architectures aiming to accurately reconstruct syntactically valid molecules from latent space [ 39 ]. In addition, SMILES representation is a non-unique molecular representation, but this can be transformed into a unique molecular representation by implementing SMILES canonicalization algorithms, yet multiple SMILES can exist for a single molecule [ 40 ].\\n\\n### IUPAC international chemical identifier (InChI)\\n\\nIntroduced by IUPAC in 2013 as open-source software for molecule encoding, InChI is a string-based molecular representation that employs six layers and multiple sublayers to convey information about molecules. Each layer contains specific details regarding the chemical formula, atom connectivity, atomic charges, and stereochemistry, among others. It also provides information on the reactivity of atoms that may undergo chemical equilibrium, leading to the formation of constitutional isomers and resonance structures. The incorporation of multiple layers of information and its ability to provide information about structural and stereo isomers makes InChI the first canonical representation of molecules. However, unlike SMILES, due to the complexity level of InChI syntaxis and grammar, this representation method is not a user-friendly method and is also prone to valency and branching issues [ 41 ].\\nDeepSMILES is a SMILES-like molecule representation that encodes molecules using a syntax that avoids grammatical errors during molecule generation by utilizing one symbol for specifying branches and cycle closures. To indicate the length of a branch, n number of closing parentheses are used, where ' n ' represents the length of the branch. Similarly, cycles are represented by a number indicating the length of the cycle. This simplification of notation regarding SMILES allows DeepSMILES to solve grammar mistakes that arise when deep learning architectures learn molecular representations. However, this notation still leaves room for implementation that addresses valency constraints and the development of unique molecule representations [ 42 ].\\n\\n### Self-referencing embedded strings (SELFIES)\\n\\nIn 2020 Krenn M, et al., introduced SELFIES which represent molecules using string-based methods [ 43 ]. They use derivation rules to produce valid molecules by avoiding the use of brackets for branches and cycles and employing special symbols to indicate the start of the cycle or branch, ensuring the production of only valid molecules. Instead of utilizing an end marker, the length of the branch or ring is defined by the subsequent token in the string. This method additionally addresses valency constraints that do not yield valency penalties in molecules. Nonetheless, like SMILES notations, a single molecule corresponds to multiple SELFIES representations [ 43 , 44 ].\\nFigure 1 a illustrates an example of a molecular representation, highlighting the primary distinctions between one-dimensional molecular representation and the SMILES notation. In the latter, explicit atom and bond characters are employed, following the aforementioned rules. Similarly, DeepSMILES CC\\u2009=\\u2009OC streamlines the representation of branching and ring closure, while maintaining a similar visual representation for linear molecules such as prop-2-one. In contrast, SELFIES employs brackets to delineate each token in a sequence and utilizes an explicit token to describe the branching in the sequence.\\n\\n### Chemical language models (CLMs) as deep generative models\", '### Chemical language models (CLMs) as deep generative models\\n\\nThe quality of learning data is contingent upon both data representation and the manner in which information is processed between layers in deep learning models until a continuous vectorial representation is achieved that can reliably represent a molecule. CLMs customize natural language processing (NLP) algorithms to learn chemical grammar and syntax from one-dimensional molecular data [ 45 – 47 ]. Several successful deep learning architectures have been introduced into molecular generative models to process sequential text data. Thus far, these models have demonstrated notable progress in text generation tasks.\\n\\n### Recurrent neural networks (RNNs)\\n\\nIntroduced more than 40\\xa0years ago by Hopfield, RNNs are neural networks capable of processing information in a sequential form. RNNs are used to generate and manipulate sequentially structured data, such as one-dimensional molecular representations [ 48 ]. RNNs receive a sequence input and use a set of hidden layers connected between them in a recurrent manner to transform the discrete token representation, generated from one-dimensional data, into a continuous representation. The continuous representation is then fed into a feedforward network to predict the adjacent token in the sequence. Information from the hidden layers is subsequently distributed to adjacent RNNs units, enhancing predictions with shared context. Assuming a benefit in predicting tokens via context-sharing among recurrent units, RNNs face a challenge wherein gradients may vanish or explode during the backpropagation process if the sequences become lengthy enough. This results in a nearly impossible task for RNNs to learn the long-term dependencies within the sequence. To address this limitation of RNNs, gated RNNs-units such as Long-Short Term Memory (LSTM) units and Gated Recurrent Units (GRU) are designed to implement short-term memory by adding trainable parameters that control the flow information of sequence dependencies, avoiding gradient vanishing or explosion at the cost of increasing the trainable parameters in the model (Fig. 1 B) [ 49 , 50 ].\\n\\n### Transformers\\n\\nSince the successful performance of dynamic models for processing sequential data as RNNs, the search for novel architectures capable of capturing context from sequences began until meeting Transformers in 2017 by Vaswani and coworkers [ 31 ]. Transformers rank among the top deep learning architectures, surpassing RNNs in their ability to learn one-dimensional molecule representations, such as SMILES or SELFIES. This success results from their capacity to capture the relationship between tokens in sequences, independent of sequence length, which is attributed to the incorporation of attention mechanisms [ 51 ]. This architecture consists of an encoder-decoder model, where the encoder learns how to map molecules (from string-based methods) into a continuous representation, and the decoder learns how to reconstruct these models from continuous representation into a string-based representation. Positional embeddings and attention mechanisms are crucial for acquiring chemical language cognition abilities. By establishing connections between token positions and computing the attention coefficient using scaled dot product and softmax operations, these mechanisms facilitate language acquisition. To accomplish this, tokens are first embedded using word and positional embeddings methods which converts discrete token representation into a continuous representation. Next, attention mechanism is applied several times in parallel through a series of trainable matrix that allows to linear transform the vectorial representation of chemical sequences. Finally, a feedforward layer is used to generate a fixed-length representation of molecules to allow matrix operations in further layers (decoder model or softmax layer for chemical token prediction).\\n\\n### Variational autoencoders (VAEs)', \"### Variational autoencoders (VAEs)\\n\\nSequential models have surpassed generative models in the realm of chemical language applications. Nonetheless, there have been successful introductions of alternative methods for generating molecules. These methods are founded upon the compression of discrete data, into a continuous value vector, which is later reconstructed into discrete data [ 52 ]. First introduced in 2018 by Gómez-Bombarelli for molecular generative models, VAEs have proven to be a powerful tool for generating novel molecules. In principle, VAEs are generative models designed to model an unknown data distribution using a finite sample from the distribution. This model comprises an encoder and decoder components. The encoder maps the discrete representation of molecules into a continuous latent space using a low-dimensional vector [ 53 ]. This latent vector can be utilized for further classification or regression tasks to organize the space based on specific properties [ 53 , 54 ]. On the other hand, the encoder is utilized to reconstruct molecules from latent space into its discrete representation. This process guarantees the capacity to learn how to generate molecules while complying with syntax and grammar rules. The VAE encoder and decoder may be different types of deep learning neural networks, including transformers, RNNs, and multi-layer perceptron, among others.\\n\\n### Generative adversarial networks (GANs)\\n\\nSince the successful application of autoencoders in generative chemistry and supported by their success in other fields such as image and audio generation, GANs have been introduced to chemistry. These models can increase the diversity of molecules generated while maintaining the probability of the data distribution [ 55 ]. GANs comprise two components: a generator and a discriminator. The generator component of GANs may employ various deep learning architectures like RNNs, transformers, or VAEs, among others. Once trained to build molecules, this component generates random molecules by inputting random noise into the model. In contrast, generating molecules with specific structural patterns depends on a discriminator component, a neural network that identifies if the created data represents an actual molecule or not. This element is essential for retraining the complete model until it can no longer differentiate between genuine and artificial data and updating the generator's parameters [ 56 , 57 ].\\n\\n### Biased generative models\\n\\nUntil this point, a brief theoretical framework of the deep generative architectures involved in this work has been illustrated. Even though these models can generate large virtual libraries of molecules that are grammatically and syntactically correct, producing molecules that are accessible for chemical synthesis and possess specific chemical properties or bioactivities remains a significant challenge for deep learning, particularly when working with biological targets or rare molecules that lack sufficient data for model training. To address these challenges, biased strategies have been incorporated into deep learning models. This allows for the exploration of chemical space in specific directions and the generation of molecules that are synthesis feasible or possess desired properties.\\n\\n### Reinforcement learning (RL)\\n\\nReinforcement Learning refers to a collection of techniques applied to solve decision problems in artificial intelligence (AI) models, such as deep learning. The RL methodology includes evaluating possible actions and their respective outcomes and subsequently devising a treatment plan that strives to achieve the optimal outcome [ 58 ]. RL techniques are implemented in generative deep learning models and can predict whether a generated molecule meets specific conditions. If a generated molecule is desired, the model is rewarded and updates to the model parameters allow for specific direction exploration of the chemical space, resulting in the generation of molecules with specific properties. Several reward functions have been developed to obtain molecules that are synthesizable, accessible, non-toxic, bioactive, permeable to biological membranes, and possess specific physicochemical properties [ 17 , 58 – 62 ]. In addition, multi-objective optimization properties of molecules can be performed [ 14 , 63 , 64 ].\\n\\n### Transfer learning (TL)\", \"### Transfer learning (TL)\\n\\nOn the other hand, the concept of Transfer Learning involves transferring knowledge learned by a model from a particular task to another model that can utilize this information to improve its performance in a comparable task. This approach can be utilized with molecular generative models for transferring expertise on synthesizing a particular group of molecules, thereby facilitating the production of novel molecules that exhibit desired properties [ 65 ]. TL enables generative models to apply their acquired knowledge in producing molecules with particular bioactivities or affinities to biological targets and transfer this knowledge to a model capable of generating grammatically and syntactically correct molecules [ 66 ]. This fine-tuning technique can overcome limitations in specialized datasets for biological targets or molecules with unique properties that lack the necessary information to enable models to learn syntax and grammar rules of chemical language [ 15 , 67 – 70 ].\\n\\n### Conditional learning (CL)\\n\\nSimilarly, conditional learning (CL) can be utilized to direct deep learning architectures in the synthesis of molecules with specified, desired characteristics, obviating the necessity for incorporating a reinforcement learning agent to assess the outputs of the model or depending on fine-tuning data for retraining [ 71 ]. In contrast, CL enables models to integrate domain-specific data about chemical structures, such as chemical properties, biological activities, or functional groups, directly into the training process. This is typically achieved by embedding the relevant information in a vector format or through other encoding techniques, which are then used as input conditions for the model during training [ 72 ]. By conditioning the model on these learned representations, it is possible to sample novel chemical structures that possess similar or improved properties to those in the training dataset. This process enables the model to internalize both chemical sequences and their corresponding properties, thereby facilitating the generation of compounds that fulfill specific chemical or biological criteria. For instance, CL can facilitate the generation of drug-like molecules with optimized solubility, binding affinity, or metabolic stability, thereby significantly reducing the computational resources and time typically required for such tasks [ 73 , 74 ].\\n\\n### Evaluation metrics for deep generative models\\n\\nSo far, we have presented a brief overview of deep learning architectures and bias techniques. However, it is crucial to evaluate the performance of these models fairly and objectively. To tackle this issue, Polykovskiy, et al. have introduced a set of evaluation metrics to identify common problems in generative models, such as overfitting, imbalanced frequent structures, and model collapse [ 24 ].\\nOne of the key requirements for generative models is their ability to learn the syntax and grammar of chemical language models. To determine whether a model can generate valid chemical entities, a validity metric (Eq. 1 ) is introduced using RDkit software to calculate the percentage of chemical entities that do not violate basic chemical rules [ 75 ], It is recommended that this metric be calculated for at least 30,000 molecules.\\n[TABLE CELL] V a l i d i t y V m = V a l i d m o l e c u l e s M o l e c u l e s p r o d u c e d\\nEquation 1 . Validity model equation.\\nIn addition, to evaluate the ability of the model to generate molecules while preserving uniqueness, we employ a uniqueness metric. This metric represents the first 1,000 or 10,000 unique valid molecules produced by the model. The higher the uniqueness value, the better the model's performance, calculated using Eq. 2 .\\n[TABLE CELL] U n i q u e n e s s = s e t V m V m\\nEquation 2 . Uniqueness model's equation.\\nAdditionally, to determine if the model is experiencing overfitting, novelty metrics measure the ratio of valid molecules ( V m ) that do not appear in the training dataset ( T d ). This indicates the model’s proficiency in learning the data distribution and generating unique molecules. Larger values of novelty indicate a lower level of overfitting. The calculation of novelty is illustrated in Eq. 3 .\\n[TABLE CELL] N o v e l t y = 1 - V m T d\\nEquation 3 . Novelty model’s equation.\\n\\n### Methodology.\\n\\n#### Literature search and screening\", '### Methodology.\\n\\n#### Literature search and screening\\n\\nFollowing the Preferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA) methodology for a systematic review and meta-analysis, we conducted a thorough review to complete this manuscript. The PRISMA guidelines stress transparent and complete research practices, which we uphold in this review. The subsequent section discusses all relevant items in detail.\\nThe sources for this study are derived from a peer-reviewed online database, as illustrated in Fig. 2 . This research only considered articles indexed in Scopus, Web of Science, and Google Scholar. Advanced search filters were implemented in each scientific search engine to restrict findings to articles issued from January 2020 to June 2024. Boolean statements were implemented to retrieve articles, and the queries used are presented below:\\n- “Molecule Generation” AND (“Deep Learning” OR “Artificial Intelligence”).\\n- (“Chemical Language Models” OR “Molecular Generative Models”) AND (“Deep Learning” OR “Artificial Intelligence”).\\n- (“Chemical Language Models” OR “Molecular Generative Models” OR “Molecule Generation”) AND (\"Deep Learning” OR “Artificial Intelligence”).\\n- “Molecule Generation” AND (“RNNs” OR “Transformer” OR “VAEs\" OR “Variational Autoencoders” OR “VAE\" OR “GAN” OR “Generative Adversarial Networks” OR “GANs”).\\n- (“Molecular generation”) AND (“recurrent neural networks” OR “transformers” OR “GPT\").\\n- (“Recurrent neural network” OR “transformer” OR “GPT”OR “VAE” OR “GAN”) AND (“drug design” OR “de novo drug”).\\nOpen in a new tab\\nPRISMA flow diagram showing the study selection process. PRISMA flow diagram was generated using [ 76 ].\\n\\n### Eligibility and criteria\\n\\nAs mentioned in the previous section, deep learning methods have transformed the molecular virtual library generation and molecular leads identification processes, leading to a considerable improvement in drug discovery\\'s efficiency and accuracy. Numerous deep learning architectures have been introduced, and various approaches have been developed for designing molecules. This has made it necessary to use impartial metrics to compare and evaluate the pros and cons of various deep learning methods. To address this need for impartial metrics to compare deep learning models, MOSES and Guacamol benchmarking platforms were introduced in 2019 and 2020, respectively [ 23 , 24 ]. This study was restricted to articles that fulfilled the specified criteria, as there was insufficient content reporting these metrics before their introduction in 2020. The metrics are the following:\\n- The manuscript must be written in English.\\n- The manuscript explicitly presents at least two metrics of uniqueness, validity, or novelty for the generated compounds (either in the article or in supplementary material).\\n- The manuscript presents in detail the concept of uniqueness, validity, or novelty metrics, and these concepts fit the MOSES or GUACAMOL metric concepts.\\n- The manuscript focuses on using deep learning generative models to generate de novo molecules.\\n- The implemented model uses conventional deep learning generative methods without the use of quantum computer methods.\\n- The manuscript was published between January 2020 and June 2024 in a peer-reviewed journal or pre-print services.\\n\\n### Data collection', '### Data collection\\n\\nFor the analysis, we extracted article and journal details such as title, publication year, journal name, and the Scimagojr quartile category for each selected item. Additionally, we identified the database name, training dataset size, and physicochemical criteria used for selecting molecules, including K i , IC 50 , EC 50 , LogP, and molecular weight. The data on the molecular representation used for training and the characteristics of molecules in the training dataset were also obtained. The extracted information of molecular representations encompasses the type of molecular representation, the upper and lower limits for the length of representation character and vocabulary for chemical language models, and a binary variable indicating canonization usage for molecular representation. Other aspects considered include the incorporation of stereochemistry in molecular representation, as well as the implementation of a salt-removing procedure. Furthermore, our study collected data on the architecture type, embedding length, number of layers and units in hidden layers, number of trainable parameters, use of dropout, activation temperature for the softmax function, batch size, epochs, learning rate, and optimizer type applied in deep generative architecture. Moreover, various columns that describe the model features were added depending on the features of each kind of deep learning architecture and can be consulted in supplementary material. Additionally, a binary variable was used to determine if the analyzed work employed biased techniques for generating molecules, such as RL, TL, or conditional learning. In cases where biased methods were used, details about the optimization objective of the biased model are provided. Finally, we collected information on the output format of the molecule representation, the number of molecules generated, their uniqueness, validity, novelty, scaffold diversity, scaffold novelty, fragment similarity, similarity to nearest neighbor (SNN), internal diversity, and Frénchet Chemical Distance (FCD) for both biased and unbiased models.\\nFinally, the selected set of research articles comprised 24 RNNs, 23 Transformers, 16 VAEs, 8 GANs and only 1 article for Structurated State Space Sequences (S4) for molecule generation. Since this work focuses on CLMs, only 10 out of 72 articles relate to graph approaches, which were solely incorporated for comparing CLMs and Graph Neural Network approaches in a general sense. A meta-analysis was performed for RNNs study case using the following outcomes: uniqueness, novelty and validity of the generated subset of molecules.\\n\\n## Results and discussion\\n\\n### Deep generative models', 'Since their successful introduction as generative models by Gómez-Bombarelli et al. and Segler et al. in 2018, deep learning has emerged as a fundamental tool for de novo molecule design, these have gained the attention of researchers to implement novel deep generative architectures and approaches that have shown to be useful for performing different tasks in other fields such as text generation, image and audio generation and among others [ 16 , 53 , 77 ].\\nCLMs have emerged as valuable tools for exploring chemical space through one-dimensional molecule string representation. CLMs have demonstrated advantages in molecule generation compared to other approaches. This is illustrated in Fig. 3 , which depicts the growing disparity between research employing CLMs and other methodologies. The rise in publications on molecular generative models employing CLMs can be attributed, in part, to advancements in natural language processing (NLP), which has continued to flourish since the first semester of 2024. This is corroborated by the increasing number of publications pertaining to CLMs for molecule generation, which substantiates the efficacy of CLMs in identifying novel and efficacious molecules for the advancement of novel therapies or the treatment of new diseases [ 78 ]. Consequently, these methodologies have been effectively implemented in the domain of cheminformatics, employing diverse instruments to facilitate the identification of novel compounds [ 79 , 80 ]. In contrast, alternative approaches essentially entail the use of graph neural networks for the generation of molecules. The graph models demonstrated the potential to serve as a robust tool for capturing spatial information about molecules, including atomic geometries and molecule topology [ 81 – 83 ]. This approach has been shown to enhance the validity rates and facilitate the incorporation of diverse spatial relations, such as pharmacophore groups and conformation energies [ 84 – 86 ]. However, graph-based models require a significant amount of computational time for training and generation of molecules, which can be up to one order of magnitude higher than that required by traditional CLMs. Additionally, the cost of training in terms of resources can be more expensive for graph-based models, and the ability to explore the chemical space may vary depending on the task due to the nature of the models, which process data and learn patterns in chemical distributions, such as the presence of large rings or branches [ 87 , 87 ]. In terms of chemical space exploration, CLMs provides a range of architectural options that can efficiently handle one-dimensional molecular representation. These models are relatively straightforward to train and yield high-quality results during the inference stage [ 36 ]. Furthermore, they can be readily deconstructed to reveal the underlying generative models, which may eventually become explainable [ 88 ].\\nOpen in a new tab\\nComparison of the number of deep generative models article publications from 2020 to June 2024\\nRNNs and their variations have gained widespread popularity for molecule generation since the inception of CLMs. In the early days of CLMs, RNNs, and their variants were utilized to understand the distribution of sequential data, particularly for SMILES. Objective evaluation was prioritized throughout this research. RNNs have demonstrated their ability to efficiently learn the grammar and syntax of generating or completing SMILES sequences, resulting in molecules with similar property distributions as those in the training dataset [ 89 ]. These results are shown in Fig. 4 , where RNNs were previously a significant deep learning architecture extensively applied for generative tasks. However, RNNs have seen a reduced usage as molecular generative models due to the long training time and risk of degraded performance associated with long-term dependencies. The surge in this area is largely attributed to the implementation of advanced deep learning designs, such as transformers. These models enable parallel computing and surpass the restrictions of earlier methods by capturing more information about molecular representations more effectively [ 90 ].\\nOpen in a new tab\\nComparison of deep learning architecture models published from 2020 to June 2024', 'Open in a new tab\\nComparison of deep learning architecture models published from 2020 to June 2024\\nFigure 4 shows a major increase in the implementation of transformers as generative models in recent years. This can be attributed to their ability to learn data distributions and produce precise predictions by the implementation of self-attention mechanisms and parallel computing [ 91 ]. On the other hand, Fig. 4 indicates that autoencoder approaches based on GANs and VAEs, which could be implemented as CLMs or graph models, remain as alternatives for molecule generation but with a low rate of use as generative models compared to sequential models. The observed low usage rate of GANs and VAEs could be attributed to the high complexity regarding the time and memory space of these methods, as well as their reduced ability to generate large molecules [ 37 , 92 ]. Despite, the low rate of implementation of autoencoder approaches, VAEs presented a constant rate of implementation as generative models since 2020. In 2023, S4 were introduced as chemical CLMs for drug discovery, demonstrating promising results at generation sequences based on SMILES strings. However, further details regarding the implementation of S4 can be found in the metrics evaluation of CLMs.\\nFinally, left panel of Fig. 5 presents the percentage of deep learning architectures utilized in the analyzed papers. The data indicates that RNNs and transformer models are the most frequently utilized, while GANs, VAEs, and the recently introduced S4 models are employed to a lesser extent in molecule generation. Notably, the slight difference in utilization percentages between RNNs and transformer models indicates a significant surge in the implementation of transformers since their introduction to the deep learning field in 2017. Contrastingly, RNNs were introduced more than 40\\xa0years ago. It is of note that despite the LSTM-RNNs having a greater number of trainable parameters compared to the GRU-RNNs, it remains the most frequently used model within RNN models. This finding could be attributed to the LSTM-RNNs longer exposure time, unlike the more recent introduction of the GRU-RNNs.\\nOpen in a new tab\\nRelative comparison of architectures used as generative CLMs in systematic review. Left panel represents the overall fraction of the deep learning architectures used in 49 retrieved CLMs. Right panel shows the fraction of RNN variations used in articles that used RNNs as generative CLMs', '### Databases and molecular representations\\n\\nAs previously stated in this article, the presence of data is crucial for learning. The efficacy of deep learning models is significantly influenced by the quality of data input. In generating molecular models, structural information about molecules is gathered in different formats, with SMILES and InChI being the two main formats. Additionally, data about their physicochemical and structural characteristics is collected. Furthermore, molecular databases contain relevant information on bioactivities, biological targets, and other crucial biological data. Table 1 presents the databases used for training models in this study.\\nDescription of databases used in retrieved articles for analysis.\\n[TABLE CELL] Description\\n[TABLE CELL] Number of molecules (millions)\\n[TABLE CELL] Molecule representation\\n[TABLE CELL] Structural information of mostly small molecules\\n[TABLE CELL] SMILES and InChI\\n[TABLE CELL] Bioactive molecules with drug-like properties and Bioactivity records of data\\n[TABLE CELL] SMILES and InChI\\n[TABLE CELL] Structural information of drug-like molecules\\n[TABLE CELL] US patent database\\n[TABLE CELL] Reactions extracted by text-mining from United States patents published between 1976 and September 2016\\n[TABLE CELL] DNA-Encoded Library a\\n[TABLE CELL] Structural molecular, combinatorial screening, and DNA-encoded information\\n[TABLE CELL] Natural products structural and biological information\\n[TABLE CELL] SMILES and InChI\\n[TABLE CELL] A comprehensive resource of gene expression in human cells perturbated by small molecules\\n[TABLE CELL] Not applicable\\nOpen in a new tab\\na Indicates databases created by authors and not publicly available, for this case reference indicates the article reference. Number of reported molecules up to September 2024\\nDrug-like molecular databases, such as Zinc and ChEMBL, provide information on molecular structures, bioactivity data, validated bioassays, and physicochemical properties. The extensive use of these drug-like databases is linked to current trends in drug design, which focus on deep learning-based methods for drug discovery and design [ 100 , 101 ]. Approximately 71% of the articles discovered through this systematic review focus on designing and discovering targeted drugs. Of those 71% retrieved articles focused on drug discovery, roughly 9 out of 10 utilized the ChEMBL or Zinc database to train their deep generative models, which are primarily fueled by the curated structural data of synthesizable and validated drug-like molecules. This data serves as an extremely valuable source of information for the deep generative models.\\nLinked to the significant role that databases play in deep learning, molecular representations also play a crucial part in training deep generative models. Although SMILES has limitations, it is still a widely used molecular format for CLMs. In this study, 77.27% of the models used SMILES exclusively for training. This format allows for data augmentation through randomized SMILES and offers a format that is available in almost all databases in a compact memory format that can be learned easily for deep learning models [ 36 , 37 , 102 , 103 ]. On the other hand, the other CLMs articles utilize NLP translation methods to generate targeted compounds using input formats such as target receptor sequences, gene expression signatures, IUPAC names, or physicochemical properties [ 104 – 110 ].\\n\\n### Training dataset size', '### Training dataset size\\n\\nIn practice, data quality alone does not suffice for model training, the quantity of data also plays a crucial role. Therefore, high-quality data are essential for effective training. To learn chemical distributions and patterns, deep generative model training implements probabilistic estimators of data distributions to fit the original data distributions; this process involves the use of large amounts of chemical entities to estimate the unknown parameters and learn the data distribution. In CLMs, the Negative Log Likelihood (NLL) function is implemented for model training to minimize it. This minimization iterative process results in learning unknown parameters to model the chemical data distribution of sequences [ 18 ]. Since different models have varying levels of complexity, they require different amounts of data to learn unknown parameters. Figure 6 illustrates the differences in the training dataset used for each CLM in this analysis. Despite the different complexity of the architectures, no statistical difference was observed between the size of the training datasets in different deep generative models. However, a large dispersion for training dataset size in VAEs is observed. This is mainly driven by a value that corresponds to 30 million chemical entities extracted from PubChem to train TransAntivirus, a VAE architecture that uses IUPAC names for SMILES sequence prediction through transformers-based encoder-decoder. The encoders use a transformer that feeds the output prediction of the IUPAC name to the decoder, which is also a transformer that uses the latent representation of IUPAC names to predict the tokens in the SMILES sequences. This process involves learning two different chemical languages with different constraints and syntax, requiring many parameters to map each language system and large amounts of data to train all these parameters [ 110 ]. Similarly, an outlier is observed in the size of the Transformers training dataset, encompassing 50 million unique chemical structures retrieved from PubChem. These structures are employed to train a transformer-based structure–property multi-modal foundation model (SPMM), a proposal put forth by Chang and Ch. The aforementioned approach is analogous to that of a sequence-to-sequence model, which is capable of generating molecules from a property vector to a SMILES sequence. This enables the learning of a more nuanced representation of molecules, which is additionally adept at performing a multitude of tasks beyond mere molecule generation. These include property prediction and forward/ retro-reaction prediction without any loss of generation metric values. However, the learning process is exhaustive in terms of the data required, necessitating the learning of millions of molecules [ 111 ].\\nOpen in a new tab\\nBoxplot of training dataset size used in different deep learning models for molecular generation\\n\\n### Unbiased models', 'Molecule generation can lead to the production of invalid sequences beyond the chemical space. The assessment of molecular generative models’ capacity to generate valid chemical entities is supported by a metric that measures the ability of CLMs to understand chemical language. To accurately compare the proportion of valid molecules generated by various architectures using CLMs, Fig. 7 presents the valid fraction of unbiased models as reported in the retrieved articles. No statistically significant differences were observed in the validity medians among the deep generative models, for either group or pairwise comparisons. Overall, Transformer architectures display high validity rates for molecule generation. This is accomplished using self-attention mechanisms, which adjust the size of the latent vector depending on the length of the sequence to retain uncompressed sequence information instead of compressing it into a fixed-length vector that may not effectively represent the interaction between chemical tokens. However, the analysis of Transformer architectures uncovered a molecular generative transformer model that achieves a validity rate as low as 6.9%. This study, conducted by Zhumagambetov et al., focuses on generating virtual libraries of compounds. The researchers discovered that increasing the variability of token sampling and adding Gaussian noise to the transformer decoder is a powerful technique for stochastically sampling molecules in chemical space. This technique can generate molecules with unique chemical structures that generalize well in unseen regions of chemical space. However, it is also prone to generating chemical entities that do not belong to the chemical space, thus reducing the validity ratio [ 112 ].\\nOpen in a new tab\\nValidity boxplot of unbiased models evaluated using at least 1000 chemical compounds generated by deep learning models\\nAlthough there are no outliers for GANs in Fig. 7 , a significant data dispersion can be attributed to the constraints of learning chemical language from other biochemical sequences associated with the molecule target reaching values of validity as low as 8.5%. This strategy presented by Méndez-Lucio et al. is an important demonstration of the potential of GANs to generate targeted molecules resembling active compounds using only gene expression and SMILES as training data. This approach reduces the need for extensive information on bioactive molecules [ 104 ]. The model learns to generate both bioactive and realistic chemical entities in SMILES format from gene expression profiles. This approach improves the probability of generating molecules that induce a desired transcriptomic profile but comes at the cost of generating a high proportion of molecules that violate valence chemistry rules due to the challenging tasks of learning how to map valid and real chemical structures in chemical space and simultaneously learning SMILES syntax and grammar rules from gene expression signatures. Subsequently, other approaches have been developed to generate phenotype-tailored compounds using stained cell images as inputs, driven by the high cost of obtaining gene expression signatures and poor validity achieved in previous works. This proof-of-concept utilizes generative models and cellular morphology information to design compounds that have the potential to induce a desired biological response with a high validity rate (56.6%) by implementing SELFIES as the molecular representation [ 113 ]. Moreover, subsequent to the introduction of Mendez-Lucio, which generated targeted molecules from gene expression data, Liu and colleagues proposed a TransGEM. This methodology employs cell line and gene expression embeddings to create molecules using SELFIES sequences. The method is based on an encoder-decoder Transformer architecture, which has significantly enhanced the distribution generation metrics. These values now reach 85% for uniqueness and 100% for validity and novelty. However, the internal diversity is relatively low (79%), indicating that the model is unable to generate a diverse range of groups within the generated set. Nevertheless, this approach has demonstrated that the use of SELFIES and a tenfold binary encoding representation for the gene expression values, which are subsequently embedded into a dense vector, contains more information than other representations presented in the article (gene expression (GE) values and GE one-hot vectors). This representation serves as an input for the transformer decoder, which learns the interaction information between gene expression data and molecule embeddings, resulting in high distribution metric values [ 114 ].', 'In addition, other GAN-based CLMs utilizing discrete molecule representations have emerged as an active research area aimed at mitigating challenges, like model collapse, associated with generative models. For this purpose, NLP methods have been adapted for molecular generative tasks, including MaskGAN, which generates molecules using SMILES as a molecular representation and a text fill-in-the-blank strategy [ 115 ]. Nonetheless, human expertise or other computational approaches are required to determine which scaffolds must be filled to generate novel compounds with potential therapeutic applications, limiting this approach to a proof-of-concept for CLMs that could achieve high molecular validity when the masking ratio is around ten percent of the sequence [ 116 ]. On the other hand, three different strategies implementing the idea proposed by Zhao and colleagues have used their Adversarially Regularized Autoencoders (ARAE) to train molecular GANs [ 117 ]. This approach is based on combining discrete autoencoders with GANs. Specifically, VAEs map discrete molecule representations onto a latent space. Generated latent vectors are then utilized to estimate the discrete distribution of molecules with the assistance of a GAN through adversarial training. ARAE sidesteps typical issues that arise when GANs attempt to learn discrete representations of molecules, which can result in a model collapse problem. The Conditional Adversarially Regularized Autoencoder (CARAE) approach was the first to introduce ARAEs to molecular representation using SMILES as discrete molecular representations. This approach includes a conditional module that can sample molecules with similar properties by tuning the latent vector using a property vector. Ultimately, the tuned latent vector is manipulated by the VAE decoder to reconstruct the original molecule [ 118 ]. Similarly, the cross-adversarial learning method for molecular generation (CRAG) approach utilizes ARAE with Projected Gradient Descent (PGD) to generate adversarial samples. The use of PGD leads to data augmentation without changing the actual molecule distribution, effectively addressing the challenge of precisely estimating representation distribution [ 119 ]. Without a doubt, both models can produce a high percentage of valid molecules, reaching values of 90.3% and 97.6%. In contrast, previous ARAE methodologies exhibited inadequate decoding proficiency for valid SMILES sequences, achieving values as low as 30.7% [ 120 ]. This shortcoming was primarily due to the lack of a smoothed latent landscape, resulting in empty areas that were later sampled by GANs, leading to the creation of invalid molecules by the decoder. This limitation is readily addressed by CARAE and CRAG through the incorporation of a property prediction neural network which is jointly trained with VAEs. This leads to an organized latent space that has a soft transition between encoded molecules with different properties, which significantly reduces the risk of GANs sampling empty spaces [ 53 ]. Furthermore, alternative methodologies for the generation of realistic chemical structures have been proposed, employing Generative Adversarial Imitation Learning (GAIL) [ 121 ]. This approach utilises a discriminator to direct the actor in emulating expert behaviour, through the training of a transformer to generate SMILES. In contrast, the contrastive discriminator is trained with a set of chemical structures exhibiting specific properties, with the objective of retraining the transformer and generating molecules that are analogous to the target structures. Although this model is susceptible to generating invalid molecules when working with SMILES, which can be attributed to the limited data used for training (~\\u2009350\\xa0k chemical sequences), when SELFIES are employed as a molecular representation, the model is capable of generating a perfect validity score while maintaining internal diversity metrics, indicating that the model is capable of generating diverse chemical sequences that conserve the chemical properties of the training dataset in contrast to the expected behavior of generative CLMs presented by Skinnider [ 122 ]. However, VAEs have not shown any significant differences compared to other deep learning architectures, and their median validity results are similar to the median found for GANs.', 'In the final consideration, the multitude of versions of RNNs, these architectures were compared generally as a family with other architectures. The results revealed that their ability to generate valid molecules is close to transformer-based methods. To further investigate the behavior of individual RNN variants, the validity fraction of unbiased molecules generated by each variant was evaluated and presented in Fig. 8 . Statistical analysis indicates no significant difference between the various recurrent deep architectures using group and pairwise median comparisons. It is important to note that only one work that implemented a Nested LSTM (NLSTM) for molecule generation was included in the analysis. This differs from LSTM or GRU in that it has a multi-level memory, which allows for a higher degree of freedom than other recurrent neural network (RNN) methods. This enables the model to handle internal memory over a longer range, but this feature comes at the cost of increasing the number of trainable parameters in memory cells and therefore the time needed to train in comparison to conventional RNN architectures. The NLSTM employs a pre-trained model to represent the tokens of molecules in a dense vector derived from the pre-trained Mol2Vec model. This provides more detailed information about chemical properties to each token. Subsequently, the NLSTM is capable of effectively handling the relation between tokens, thereby enabling the generation of molecules that are 97.6% valid even when the sample temperature of softmax is reduced to 0.75 [ 123 ].\\nOpen in a new tab\\nValidity boxplot of unbiased models evaluated using at least 1000 chemical compounds generated by RNNs variants\\nTo complete the analysis of validity in CLMs, validity was compared to the ability of models to generalize the learning of sequences in CLMs by producing novel molecules. In this context, while maintaining the ability to generate valid molecules, ideal generative models would allow the exploration of a larger chemical space by generating novel entities not present in the training dataset. To this end, we propose the metric Valid/Sample (Validity x Novelty) to compute the ratio of valid to novel molecules generated by deep learning structures, similar to Hong, et al. Novel/Sample metric. [ 118 ] After reviewing literature that reported the validity and novelty of unbiased models, we calculated the Valid/Sample ratio and used a box plot to identify low-value outliers. These outliers were mainly attributed to discontinuous sampling in latent molecular space, low rates of generalization learning, or inadequate methods for pre-processing chemical sequences [ 112 , 124 , 125 ].', 'To analyze the relationship between valid and novelty values, we eliminated outliers from the analysis. Figure 9 demonstrates that there is not a significant relationship between the validity and novelty values of generative models (p-value\\u2009=\\u20090.0618). However, a trend of the inverse relationship between the values of validity and novelty is noticeable (spearman coefficient, ρ\\u2009=\\u2009−0.3575), wherein 82.1% of models fail to attain values equal to or exceeding the median of both novelty and validity (95.6% and 96.5% respectively). This is attributed to the inherent balance of exploring the chemical space and generating valid chemical sequences. [ 126 ] Only a small fraction (17.9%) of the analyzed deep generative models can achieve high novelty values while retaining a high validity ratio of chemical entities. Thus, articles use different approaches to maximize validity and novelty such as reducing the gap between molecular representation in latent space by using ARAEs or VAEs approaches [ 110 , 119 ]. In addition, fill-in-the-blank based strategies to generate novel molecules reduce the degrees of freedom of molecular complexity for molecule generation, resulting in a high validity rate that can be implemented either for SMILES or SELFIES, wherein due to the intrinsic properties of SELFIES models can generate 100% valid and novel chemical sequences [ 36 , 127 ]. In a similar fashion to fill-in-the-blank methods, decorative approaches can reduce the complexity of molecule generation by decorating a carbon backbone with other functional groups [ 128 ]. It is noteworthy that a recent development of transformer-based variational autoencoders (VAEs) has been implemented by Zhu and colleagues. This approach focuses on the target generation of molecules using pharmacophore information. In this method, the information of pharmacophore groups is embedded using a gated graph neural network. Then, SMILES are embedded using a masking language model. Subsequently, all of the aforementioned information is utilized to generate a latent vector through an encoder transformer, which is then employed to train the encoder-decoder transformer in addition to the pharmacophore information embedding. In conclusion, novel molecules with a given pharmacophore group are generated by using a latent vector N(0,I) during the inference stage. This strategy is capable of designing drug molecules based on the superposition of known active molecules when the target is unknown, or the binding site is unclear. It achieves high novelty (97.6%), validity (98.2%), and uniqueness (97.9%) rates even when SMILES strings, which are prone to syntax errors, are used for molecule generation. This is due to the implementation of a large number of attention mechanisms during the training stage, which capture all the necessary relationships between sequences to ensure [ 129 ].\\nOpen in a new tab\\nNovelty trend respect to validity of unbiased models. Color scale indicates Validity x Novelty metric\\nOne of the most promising deep learning architectures recently introduced is DRAGONFLY, which employs a LSTM as CLMs to generate molecules and graph neural network to encode the information of interaction between ligands and targets. This approach integrates information about receptors from graph neural networks, which are used to train the LSTM to learn the distribution of SELFIES that corresponds to each ligand based on its spatial 2D and 3D information about the receptor. This approach optimizes the acquisition of information regarding interaction networks between drug targets and their ligands while maintaining high-quality distribution metrics [ 130 ].\\nSince the uniqueness of models represents a fundamental metric to compare the ability of models to generate diverse sets of chemical sequences, we evaluated and compared the uniqueness of unbiased models reported in CLMs with different architectures. Figure 10 shows boxplots of each general architecture used for CLMs and molecule generation, indicating no significant differences in medians between generative models. In general, deep generative models can reach high uniqueness values for molecular generated sets.\\nOpen in a new tab\\nUniqueness box plot of unbiased models evaluated using at least 1000 chemical compounds generated by deep learning models', 'Open in a new tab\\nUniqueness box plot of unbiased models evaluated using at least 1000 chemical compounds generated by deep learning models\\nIn addition, Fig. 10 shows the evaluation of a common obstacle faced by generative CLMs: the ability to generate unique compounds during the inference stage. Despite the advantages in terms of enhancing validity and novelty through scaffold decoration and fragment-linking approaches, the generation of unique molecules comes at a cost. These approaches reduce the chances of producing invalid molecules while maintaining an acceptable level of novelty. Nevertheless, the ability to produce unique chemical sequences is compromised due to redundancy problems associated with the training process. These records provide clear evidence that these models are susceptible to experiencing overfitting [ 74 , 106 , 124 , 131 , 132 ]. In contrast, incorporating the dynamic addition of blank positions to be filled or generating a token matrix instead of sequentially generating tokens to decorate scaffolds could improve the uniqueness ratio of generated molecules for CLMs [ 127 , 128 ]. Additionally, transformer-based models utilizing sequential text generation for de novo molecule generation have demonstrated near-perfect or perfect uniqueness scores. This is achieved through the implementation of a masked self-attention mechanism, which prevents the model from attending further tokens in sequences and therefore prevents overfitting [ 61 , 133 – 136 ]. While other architectures have concentrated on implementing RNNs or pooling algorithms to compute the attention mechanism and enhance the generation metrics of transformers, resulting in marginal improvements in generation metrics such as uniqueness, they have also led to a significant increase [ 138 ] In the number of trainable parameters and training time [ 137 ]. This suggests that GPT approaches are sufficient for chemical space exploration when the training data is sufficient to train a robust model. Nevertheless, this type of model may prove beneficial in instances where conditional generation is required or when particular properties must be present in chemical structures.', \"For this purpose, 4 out 9 CLMs VAE-based architecture met the analysis requirements and proved to be a potent tool that employs transformers and mask self-attention in their encoder-decoder structures, enabling accurate learning of information from IUPAC and SMILES sequences. This method, previously discussed for evaluating novelty and validity, has significant potential for designing and optimizing molecules. However, it requires extensive data and time for training, which may be a disadvantage in terms of requirements[ 110 ]. In addition, a novel approach called Generative Chemical Transformer (GCT) was introduced, which embeds transformer architecture in VAEs; this strategy takes advantage of the attention mechanism and its ability to pay sparse attention in chemical sequences to deeply understand the geometric structure of SMILES beyond the limitations of semantic discontinuity of chemical language, resulting in good performance in generalizing learning from SMILES sequences and thus drastically reducing the number of repetitive chemical entities produced by the model [ 138 ]. Nevertheless, alternative approaches that are based on the utilization of transformers as the foundation of VAE have been proposed by Yoshaki and colleagues. The authors put forth a proposal wherein the normalized latent vector, which has been learned from transformer encoding, is employed to feed a transformer decoder in conjunction with the SMILES embedding, thereby enabling the generation of novel structures. This implies that the transformer decoder is capable of learning the distribution of SMILES from an embedding vector that contains both the latent vector and the SMILES embedding. This approach has the potential to yield results that are both unique and novel, with a value approaching that of a perfect score. However, this approach also has limitations, as it results in a loss of validity, with a score of 87%, due to the inability to smooth the decoding process in structures that do not adhere to the established chemical rules [ 139 ]. In contrast, Inukai and colleagues The authors proposed a transformer-based VAE that employs fragment tokenization to extract conformational molecules, as opposed to character-level tokenization and tree positional encoding. The former is used to generate embeddings that enrich the information about the position of fragments and simplify the handling of large and complex molecules. These are then fed into a decoder, while the latent vector generated from the encoder transformer is used to feed the cross-attention in the decoder transformer. This approach yields high distribution metrics (exceeding 97% for each metric) while reducing the time needed to train and generate large libraries in comparison to existing VAE architectures [ 140 ]. While other non-transformer-based VAE models have demonstrated the potential for high uniqueness and validity values, the implementation of NRC-VABS is a noteworthy example. NRC-VABS is a normalized reparametrized conditional LSTM-based VAE that employs a beam search to decode the latent vector. The model introduces Hx SMILES as a novel molecular representation, enabling the probabilistic model to learn long-term dependencies in SMILES strings and reducing complexity through the addition of two characters (^_), which indicate rings, and only closing brackets, which indicate branches. All of these are followed by a number that indicates the length of the ring or branch. The NRC-VABS approach allows for the exploration of chemical space in the surrounding area of targeted molecules. This is accomplished by creating a smoothed latent space that can be interpolated to generate novel molecules through a beam search. This approach permits the introduction of a variety of functional groups while preserving structural similarities with the targeted molecule, thereby maintaining the desired properties [ 141 ]. In conclusion, VAEs approaches achieve high uniqueness values with low dispersion; however, they are associated with model collapse or the generation of invalid molecules due to sparse latent space generation. This last limitation can be addressed by incorporating a prediction model that can accommodate the intricate latent space, as demonstrated by Liu et al. in their GRU-based VAEs for the generation of Alzheimer's drug molecules [ 142 ]. Thus far, the discussion has focused exclusively on traditional deep learning architectures. However, the introduction of S4 models as CLMs has been observed to exhibit high uniqueness values. These architectures are neural networks designed to handle long-range dependencies in sequential data, such as time series or natural language. The objective is to effectively capture both short-term and long-term patterns, which are often challenging for traditional models, such as recurrent neural networks (RNNs) or even transformers, to handle efficiently, particularly for very long sequences. The distinctive dual nature of S4s,\", 'short-term and long-term patterns, which are often challenging for traditional models, such as recurrent neural networks (RNNs) or even transformers, to handle efficiently, particularly for very long sequences. The distinctive dual nature of S4s, encompassing convolution during training and recurrent generation, renders them especially fascinating for de novo design, commencing from SMILES and attaining high-quality metrics. Furthermore, they can be utilized in conditional generation to generate products that closely resemble natural ones [ 143 ].', 'Finally, the RNNs attained comparable levels of uniqueness to other architectures. However, each class of RNNs was evaluated in terms of uniqueness to fully analyze the data. Stack RNNs were excluded from statistical analysis because only one article met the requirements. However, this class of recurrent networks has demonstrated an ability to reach values close to 100% through parallel processing of information in each cell when sampling tokens in the sequence. However, this approach compromises space and time complexity. [ 144 ]. The analysis showed no significant difference between the performance of LSTM and GRU. However, LSTM exhibited an anomaly that has been previously studied for RNNs as groups and belongs to scaffold decorations approaches [ 124 ].', \"### Biased models\\n\\nUntil now, we have reviewed the characteristics of unbiased models for molecule generation, but still left room to review deep learning strategies for exploring specific regions of chemical space. CLMs have utilized a range of techniques based on biased model weights to acquire knowledge on the general grammar and syntax of molecular representation, as well as the specific configuration and patterns of molecules that exhibit specific physicochemical properties or bioactivities.\\nFigure 11 illustrates the implementation of biased methods among deep learning architectures. The analysis examined three distinct biased methods, which were utilized with similar frequencies in various deep learning architectures. These methods are mainly used to generate targeted molecules when the available data alone is not sufficient to train deep generative models. This is especially important for rare or newly discovered targets that do not have enough data to train deep learning models.\\nOpen in a new tab\\nDistribution of biased methods among different deep generative models\\n\\n### Transfer learning\\n\\nAmong biased strategies, TL is the most prevalent method due to its user-friendly implementation strategies that are suitable for almost all architectures included in this analysis. For this specific case, fine-tuning is the commonly used transfer learning method for generating targeted molecules. This technique transfers previously learned parameters to the target model, which is responsible for recognizing specific patterns like common scaffolds, functional groups, and atomic configurations without having to re-learn the syntax and grammar of the chemical language. Fine-tuning strategies have proven to be effective tools for exploring chemical space in sequential-based models, as detailed in Table 2 .\\nDescriptive statistics and P-values are presented for the performance and training datasets of generative models that implemented TL\\n[TABLE CELL] Unbiased model\\n[TABLE CELL] Target model\\n[TABLE CELL] Training dataset size\\nOpen in a new tab\\nMedian values are reported for both the Unbiased and target models, and P-values were calculated through the Mann–Whitney U test for paired samples. The number of articles meeting the required metric reporting criteria for analysis is indicated by the sample’s column\\nSuch approaches do not compromise the validity and novelty of generated molecules compared to unbiased models and require significantly fewer chemical sequences to train the model. However, the cost of this reduction in training data is a decrease in the variety of generated molecules probably due to overfitting.\\nFine-tuning has enabled the exploration of chemical space for areas containing molecules with bioactivities linked to proteins that are commonly expressed at high levels in various solid tumors, such as the Epidermal Growth Factor Receptor (EGFR). The resulting chemical sequences are accessible for chemical synthesis, and they have produced promising molecular docking results. Additionally, QSAR models suggest activities for EGFR [ 82 , 107 , 134 ]. Additionally, recent reports have highlighted the emergence of virtual libraries consisting of targeted chemical entities generated using deep generative models and transfer learning techniques. These models focus on generating molecules with the potential to target specific Alzheimer's-related proteins such as BACE-1 and ADAM10, schizophrenia-related targets including D2R, 5-HT1AR, and HT2AR, the Cannabinoid CB2 target, SARS-CoV-2 proteases or compounds exhibiting specific chemical properties [ 97 , 103 , 109 , 124 , 125 , 145 – 149 ]. Finally, transfer learning has demonstrated the potential to generate target molecules for designing functional compounds that can be synthesized and experimentally tested against the Phosphatidylinositide 3-kinase receptor. This leads to the discovery of therapeutical leads with sub-micromolar activity, inhibiting the growth of cancerous tumor cells significantly in in vitro models [ 36 ].\\n\\n### Reinforcement learning\", '### Reinforcement learning\\n\\nUnlike biased models used in transfer learning, RL is used for planning or decision making in sequential processes such as generation of de novo molecules. The implementation of discriminative or predictive neural networks enables generative models to explore specific regions of the chemical space where bioactive or specific structures may exist. RL has been implemented in various deep learning models. However, this analysis has only been found to be applied to RNNs and Transformers (see Fig. 11 ). Table 3 presents the statistical summary of biased models that utilize RL for targeted molecule generation, revealing that the process of reinforcement learning does not affect molecular metrics.\\nDescriptive statistics and P-values are presented for the performance of generative models that implemented RL\\n[TABLE CELL] Unbiased model\\n[TABLE CELL] Target Model\\nOpen in a new tab\\nMedian values are reported for both the Unbiased and target models, and P-values were calculated through the Mann–Whitney U test for paired samples. The number of articles meeting the required metric reporting criteria for analysis is indicated by the sample’s column\\nRL utilizes models to guide generation models in learning parameters that enable molecular decoding in specific regions of chemical space, often utilizing scoring functions and predictive/classification deep learning methods. A wide variety of models and scoring functions have been introduced as agents for RL. First, 90.1% of the articles retrieved that employed reinforcement learning have opted to use Policy gradient methods. Policy gradient uses a score function that returns a value given the chemical properties of the generated molecules, this approach aims to find parameters that maximize the score function that induces the molecular generator to explore in a certain direction of the chemical space, since molecules that can achieve, higher rewards have tended to group in the chemical space. These methods have implemented many predictive modules to calculate the properties of generated molecules using machine learning or deep learning approaches. For example, linear regression-based models enable the computation of a range of physiochemical characteristics, molecular similarity, synthetic viability, and binding affinity metrics for generated chemical entities. These modules have proven to be a powerful tool in generating targeted chemical libraries for potential inhibitors of SARS-CoV-2, acetylcholinesterase, neuraminidase, and κ-opioid receptor [ 64 , 144 ]. Additionally, other predictive models that employ deep learning methods, including multilayer perceptron, convolutional neural networks (CNNs), and LSTMs, among other deep learning architectures, enable the use of generated sequences. These models are not limited to using only chemical descriptors, outperforming chemical descriptors-based methods, and thus avoiding the addition of bias to specific regions of chemical space [ 17 ]. These methods enabled optimizing the properties of molecules and generating novel chemical entities with potential activity to permeate the blood–brain barrier [ 61 , 62 , 150 ]. Additionally, other strategies that implements policy gradient, such as Hill-Climbing, augmented Hill-Climb, and REINVENT algorithms, have demonstrated the ability to achieve state-of-the-art results for molecular generation metrics in terms of validity, novelty and uniqueness [ 131 , 151 , 152 ].\\nFinally, Monteiro et al. have introduced a novel approach based on evolutionary algorithms, demonstrating the potential of combining reinforcement learning, transfer learning, and nondominated sorting algorithms to generate unique and valid chemical sequences with desirable physicochemical and pharmacological properties for targeting biologically relevant molecules, such as the Adenosine A2A receptor (AA2AR), for therapeutic applications. This approach effectively handles the complex trade-off between validity and novelty, leading to unprecedented levels of uniqueness, novelty, and validity for the targeted molecules [ 136 ].\\n\\n### Conditional generation-based methods', 'Conditional Generation-Based Methods have emerged as a tool for generating chemical entities that satisfy desired properties. Conditional Generation is a technique that involves adding constraint tokens to chemical sequences that are not part of the molecule representation vocabulary. This enables the labeling of chemical entities, which in turn trains the model to recognize conditional chemical sequences. Consequently, the model learns to map similar constrained molecules on a latent space, thereby introducing a natural bias that facilitates the generation of molecules with similar properties. This method of conditioning structure generation eliminates the requirement for optimization loops and retraining epochs for fine-tuning. Since its ability to map molecules and generate latent spaces, encoder-decoder architectures like ARAEs, VAEs, and Transformers have been widely used to implement conditional generation strategies, as illustrated in Fig. 11 . Furthermore, to assess the impact of conditional generation based methods on the generation metrics proposed in MOSES, despite the low number of samples for comparing metrics of generated chemical sequences, Table 4 demonstrates that conditional learning strategies can produce molecules as effectively as unbiased models that solely focus on learning chemical languages. The effectiveness of conditional learning in generating natural-like products has been demonstrated using Transformer architectures such as NIMO, which learns to reproduce molecules similar to terpenoids with high validity, novelty, and uniqueness values. This approach does not require exhaustive data sets or extensive computational resources, offering the potential to generate drug-like molecules with natural product scaffolds. These scaffolds may possess favorable ADME properties and minimal environmental impact [ 153 ].\\nDescriptive statistics and P-values are presented for the performance of generative models that implemented conditional generation-based methods\\n[TABLE CELL] Unbiased model\\n[TABLE CELL] Target model\\nOpen in a new tab\\nMedian values are reported for both the Unbiased and target models, and P-values were calculated through the Mann–Whitney U test for paired\\nMulti-objective compound optimization has been motivated by conditional generation methods as it possesses the ability to utilize a specific set of chemical properties to constrain the model producing virtual chemical libraries that not only can potentially be used as therapeutic agents by its structural similarity, but they may also share physicochemical properties. This achievement is demonstrated by the creation of virtual chemical libraries that can bind to EGFR, HTR1A, and S1PR1 receptors. The libraries were generated using conditional generation to produce compounds with physicochemical properties that follow empirical drug-like rules [ 135 ]. Multi-objective optimization is an effective strategy for exploring chemical space and obtaining molecules that follow various physicochemical constraints [ 133 ]. Furthermore, it is imperative to convert discrete molecular representations conditioned by the environment into a latent space. The use of VAE, ARAE, and Transformers lead to a viable approach for both multi-objective and single-objective molecular optimization, resulting in validity and uniqueness values reaching up to 85% and being as versatile as working for text-filling approaches and sequential generation, as well as, working with different molecular representations [ 74 , 104 , 109 , 110 , 113 , 117 , 119 , 138 ].\\nFinally, other approaches have been introduced that combine more than one biasing strategy to generative models, such as combining conditional generation-based methods with transfer learning or reinforcement learning. These strategies demonstrate significant progress in developing deep generative models, providing a highly effective method for exploring unknown regions of chemical space in search of molecules with specific activities suitable for material or drug discovery [ 90 ]. In particular, the implementation of transfer learning and conditional generation-based methods has enabled the generation of molecules using multi-objective optimization, which shows structural features that promote their bioactivity against specific targets while using small datasets for training [ 134 ].', \"Molecule design is fundamental to the discovery of drugs and development of materials, which currently relies mostly on the exploration of chemical space using computational approaches. Within computational approaches, CLMs have been shown to be a versatile tool for exploring chemical space. They enable mapping of chemical space and exploration in specific directions, providing access to regions where bioactive molecules exist by using biased CLMs models. During the period of 2020 to June 2024, this systematic review evaluated deep learning molecular generative models utilizing MOSES metrics. The aim was to assess the model's metrics associated with the overfitting and learning process of chemical language.\\nSince the dawn of the data age, SMILES has remained the most widely used molecular representation format, owing to its readily accessible format. In contrast, SELFIES presents a promising representation format that can enhance model performance while reducing the trade-off between validity and novelty values. This work has shown that CLMs often use transformers and gated RNN variants as generative models, which is consistent with the trend observed in NLP for text generation that has evolved along with generative CLMs [ 154 ]. Nonetheless, in recent years, there has been an increasing tendency to apply the Transformers architecture more often, due to their self-attention mechanism. This allows models to efficiently learn long-term dependencies and thereby efficiently learn chemical language.\\nFinally, the performance of generative CLMs in terms of validity, uniqueness, and novelty is not statistically affected by targeted generative CLMs. Among biased models, TL has been the most used technique among TL, RL, and conditional learning for deep learning architectures when about one thousand molecular entities are available for optimization. However, incorporating multiple biased methods into deep generative models has proven to be a promising technique for targeted molecular generation. These models could produce molecules with improved chemical properties or bioactivities and can travel longer distances within chemical space to reach areas where specific molecules exist.\\nWe hope this review provides a comprehensive understanding of deep generative models, ranging from their theoretical background to the practical implementation of generative CLMs, and offers a clear perspective on the evolution, progress, and opportunity areas of generative CLMs in recent years.\", '## ABBREVIATIONS\\n\\nAdversarially regularized autoencoders\\nConditional learning\\nChemical language models\\nGenerative adversarial networks\\nGated recurrent unit\\nInternational chemical identifier\\nLong short-term memory\\nMolecular sets\\nRecurrent neural networks\\nReinforcement learning\\nSelf-referencing embedded strings\\nSimplified molecular-input line-entry system\\nTransfer learning\\nVariational autoencoders\\n\\n## Author contributions\\n\\nConceptualization, H.F.-H. and E.M.-L.; methodology, H.F.-H.; formal analysis, H.F.-H. and E.M.-L.; data curation, H.F.-H.; writing and editing, H.F.-H. and E.M.-L.; supervision, E.M.-L. All authors have read and agreed to the published version of the manuscript.\\nThis work was partially supported by the Consejo Nacional de Humanidades, Ciencia y Tecnología (CONAHCYT) (H.F.-H. scholarship) and by the Tecnológico de Monterrey.\\n\\n## Aavailability of data and materials\\n\\nNo datasets were generated or analysed during the current study.\\n\\n## Declarations\\n\\n### Competing interests', \"The authors declare no competing interests.\\nPublisher's Note\\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.\\n- 1. Polishchuk PG, Madzhidov TI, Varnek A (2013) Estimation of the size of drug-like chemical space based on GDB-17 data. J Comput Aided Mol Design 27(8):675–679. 10.1007/s10822-013-9672-4 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 2. Reymond J-L, Awale M (2012) Exploring chemical space for drug discovery using the chemical universe database. ACS Chem Neurosci 3(9):649–657. 10.1021/cn3000422 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 3. Lu C, Liu S, Shi W, Yu J, Zhou Z, Zhang X, Lu X, Cai F, Xia N, Wang Y (2022) Systemic evolutionary chemical space exploration for drug discovery. J Cheminform 14(1):19. 10.1186/s13321-022-00598-4 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 4. Maragakis P, Nisonoff H, Cole B, Shaw DE (2020) A deep-learning view of chemical space designed to facilitate drug discovery. J Chem Inf Model 60(10):4487–4496. 10.1021/acs.jcim.0c00321 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 5. LeCun Y, Bengio Y, Hinton G (2015) Deep learning. Nature 521(7553):436–444. 10.1038/nature14539 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 6. Krasnov L, Khokhlov I, Fedorov MV, Sosnin S (2021) Transformer-based artificial neural networks for the conversion between chemical notations. Sci Rep 11(1):14798. 10.1038/s41598-021-94082-y [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 7. Karpov P, Godin G, Tetko IV (2020) Transformer-CNN: swiss knife for QSAR modeling and interpretation. J Cheminform 12(1):17. 10.1186/s13321-020-00423-w [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 8. Chuang KV, Gunsalus LM, Keiser MJ (2020) Learning molecular representations for medicinal chemistry. J Med Chem 63(16):8705–8722. 10.1021/acs.jmedchem.0c00385 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 9. Fang X, Liu L, Lei J, He D, Zhang S, Zhou J, Wang F, Wu H, Wang H (2022) Geometry-enhanced molecular representation learning for property prediction. Nat Mach Intell 4(2):127–134. 10.1038/s42256-021-00438-4 [ Google Scholar ]\\n- 10. Li X, Fourches D (2020) Inductive transfer learning for molecular activity prediction: next-gen QSAR models with molpmofit. J Cheminform 12(1):27. 10.1186/s13321-020-00430-x [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 11. Ma J, Sheridan RP, Liaw A, Dahl GE, Svetnik V (2015) Deep neural nets as a method for quantitative structure-activity relationships. J Chem Inf Model 55(2):263–274. 10.1021/ci500747n [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 12. Vanhaelen Q, Lin Y-C, Zhavoronkov A (2020) The advent of generative chemistry. ACS Med Chem Lett 11(8):1496–1505. 10.1021/acsmedchemlett.0c00088 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 13. Ruddigkeit L, van Deursen R, Blum LC, Reymond J-L (2012) Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17. J Chem Inf Model 52(11):2864–2875. 10.1021/ci300415d [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 14. Goel M, Raghunathan S, Laghuvarapu S, Priyakumar UD (2021) MoleGuLAR: molecule generation using reinforcement learning with alternating rewards. J Chem Inf Model 61(12):5815–5826. 10.1021/acs.jcim.1c01341 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 15. Queiroz LP, Rebello CM, Costa EA, Santana VV, Rodrigues BCL, Rodrigues AE, Ribeiro AM, Nogueira IBR (2023) Transfer learning approach to develop natural molecules with specific flavor requirements. Ind Eng Chem Res 62(23):9062–9076. 10.1021/acs.iecr.3c00722 [ Google Scholar ]\\n- 16. Segler MHS, Kogej T, Tyrchan C, Waller MP (2018) Generating focused molecule libraries for drug discovery with recurrent neural networks. ACS Cent Sci 4(1):120–131. 10.1021/acscentsci.7b00512 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 17. Pereira T, Abbasi M, Ribeiro B, Arrais JP (2021) Diversity oriented deep reinforcement learning for targeted molecule generation. J Cheminform 13(1):21. 10.1186/s13321-021-00498-z [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 18. Arús-Pous J, Blaschke T, Ulander S, Reymond J-L, Chen H, Engkvist O (2019) Exploring the GDB-13 chemical space using deep generative models. J Cheminform 11(1):20. 10.1186/s13321-019-0341-z [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 19. Li X, Xu Y, Yao H, Lin K (2020) Chemical space exploration based on recurrent neural networks: applications in discovering kinase inhibitors. J Cheminform 12(1):42. 10.1186/s13321-020-00446-3 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 20. Li L, Gupta E, Spaeth J, Shing L, Jaimes R, Engelhart E, Lopez R, Caceres RS, Bepler T, Walsh ME (2023) Machine learning optimization of candidate antibody yields highly diverse sub-nanomolar affinity antibody libraries. Nat Commun 14(1):3454. 10.1038/s41467-023-39022-2 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\", '- 21. Li Y, Zhang L, Wang Y, Zou J, Yang R, Luo X, Wu C, Yang W, Tian C, Xu H, Wang F, Yang X, Li L, Yang S (2022) Generative deep learning enables the discovery of a potent and selective RIPK1 inhibitor. Nat Commun 13(1):6891. 10.1038/s41467-022-34692-w [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 22. Saka K, Kakuzaki T, Metsugi S, Kashiwagi D, Yoshida K, Wada M, Tsunoda H, Teramoto R (2021) Antibody design using LSTM based deep generative model from phage display library for affinity maturation. Sci Rep 11(1):5852. 10.1038/s41598-021-85274-7 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 23. Brown N, Fiscato M, Segler MHS, Vaucher AC (2019) GuacaMol: benchmarking models for de novo molecular design. J Chem Inf Model 59(3):1096–1108. 10.1021/acs.jcim.8b00839 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 24. Polykovskiy D, Zhebrak A, Sanchez-Lengeling B, Golovanov S, Tatanov O, Belyaev S, Kurbanov R, Artamonov A, Aladinskiy V, Veselov M, Kadurin A, Johansson S, Chen H, Nikolenko S, Aspuru-Guzik A, Zhavoronkov A (2020) Molecular sets (MOSES): A benchmarking platform for molecular generation models. Front Pharmacol 11:565644 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 25. Mater AC, Coote ML (2019) Deep learning in chemistry. J Chem Inf Model 59(6):2545–2559. 10.1021/acs.jcim.9b00266 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 26. Zeng X, Wang F, Luo Y, Kang S, Tang J, Lightstone FC, Fang EF, Cornell W, Nussinov R, Cheng F (2022) Deep generative molecular design reshapes drug discovery. Cell Rep Med 3(12):100794. 10.1016/j.xcrm.2022.100794 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 27. Sousa T, Correia J, Pereira V, Rocha M (2021) Generative deep learning for targeted compound design. J Chem Inf Model 61(11):5343–5361. 10.1021/acs.jcim.0c01496 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 28. Ivanenkov Y, Zagribelnyy B, Malyshev A, Evteev S, Terentiev V, Kamya P, Bezrukov D, Aliper A, Ren F, Zhavoronkov A (2023) The hitchhiker’s guide to deep learning driven generative chemistry. ACS Med Chem Lett 14(7):901–915. 10.1021/acsmedchemlett.3c00041 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 29. Martinelli DD (2022) Generative machine learning for de novo drug discovery: a systematic review. Comput Biol Med 145:105403. 10.1016/j.compbiomed.2022.105403 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 30. Koutroumpa N, Papavasileiou K, Papadiamantis A, Melagraki G, Afantitis A (2023) A systematic review of deep learning methodologies used in the drug discovery process with emphasis on in vivo validation. Int J Mol Sci 24:6573. 10.3390/ijms24076573 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 31. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017) Attention is all you need. Adv Neural Inf Process Syst 30:1 [ Google Scholar ]\\n- 32. Balaban AT (1985) Applications of graph theory in chemistry. J Chem Inf Comput Sci 25(3):334–343. 10.1021/ci00047a033 [ Google Scholar ]\\n- 33. Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; Dahl, G. E. 2017. Neural message passing for quantum chemistry. in international conference on machine learning. 1263–1272.\\n- 34. Raghunathan S, Priyakumar UD (2022) Molecular representations for machine learning applications in chemistry. Int J Quantum Chem 122(7):e26870. 10.1002/qua.26870 [ Google Scholar ]\\n- 35. David L, Thakkar A, Mercado R, Engkvist O (2020) Molecular representations in AI-driven drug discovery: a review and practical guide. J Cheminform 12(1):56. 10.1186/s13321-020-00460-5 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 36. Moret M, Pachon Angona I, Cotos L, Yan S, Atz K, Brunner C, Baumgartner M, Grisoni F, Schneider G (2023) Leveraging molecular structure and bioactivity with chemical language models for de novo drug design. Nat Commun 14(1):114. 10.1038/s41467-022-35692-6 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 37. Flam-Shepherd D, Zhu K, Aspuru-Guzik A (2022) Language models can learn complex molecular distributions. Nat Commun 13(1):3293. 10.1038/s41467-022-30839-x [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 38. Skinnider MA, Stacey RG, Wishart DS, Foster LJ (2021) Chemical language models enable navigation in sparsely populated chemical space. Nat Mach Intell 3(9):759–770. 10.1038/s42256-021-00368-1 [ Google Scholar ]\\n- 39. Weininger D (1988) SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J Chem Inf Comput Sci 28(1):31–36. 10.1021/ci00057a005 [ Google Scholar ]\\n- 40. Weininger D, Weininger A, Weininger J (1989) SMILES 2 algorithm for generation of unique SMILES notation. J Chem Inf Comput Sci 29(2):97–101. 10.1021/ci00062a008 [ Google Scholar ]', '- 40. Weininger D, Weininger A, Weininger J (1989) SMILES 2 algorithm for generation of unique SMILES notation. J Chem Inf Comput Sci 29(2):97–101. 10.1021/ci00062a008 [ Google Scholar ]\\n- 41. Heller SR, McNaught A, Pletnev I, Stein S, Tchekhovskoi D (2015) InChI, the IUPAC international chemical identifier. J Cheminform 7(1):23. 10.1186/s13321-015-0068-4 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 42. O’Boyle, N.; Dalke, A. Deep SMILES: An adaptation of smiles for use in machine-learning of chemical structures; 2018\\n- 43. Krenn M, Häse F, Nigam A, Friederich P, Aspuru-Guzik A (2020) Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation. Mach Learn Sci Technol 1(4):045024. 10.1088/2632-2153/aba947 [ Google Scholar ]\\n- 44. Krenn M, Ai Q, Barthel S, Carson N, Frei A, Frey NC, Friederich P, Gaudin T, Gayle AA, Jablonka KM, Lameiro RF, Lemm D, Lo A, Moosavi SM, Nápoles-Duarte JM, Nigam A, Pollice R, Rajan K, Schatzschneider U, Schwaller P, Skreta M, Smit B, Strieth-Kalthoff F, Sun C, Tom G, Falk von Rudorff G, Wang A, White AD, Young A, Yu R, Aspuru-Guzik A (2022) SELFIES and the future of molecular string representations. Patterns 3(10):100588. 10.1016/j.patter.2022.100588 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 45. Zheng S, Yan X, Yang Y, Xu J (2019) Identifying structure-property relationships through SMILES syntax analysis with self-attention mechanism. J Chem Inf Model 59(2):914–923. 10.1021/acs.jcim.8b00803 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 46. Ucak UV, Ashyrmamatov I, Ko J, Lee J (2022) Retrosynthetic reaction pathway prediction through neural machine translation of atomic environments. Nat Commun 13(1):1186. 10.1038/s41467-022-28857-w [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 47. Jaeger S, Fulle S, Turk S (2018) Mol2vec: unsupervised machine learning approach with chemical intuition. J Chem Inf Model 58(1):27–35. 10.1021/acs.jcim.7b00616 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 48. Hopfield JJ (1982) Neural networks and physical systems with emergent collective computational abilities. Proc Natl Acad Sci 79(8):2554–2558 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 49. Cho, K.; Van Merriënboer, B.; Bahdanau, D.; Bengio, Y. On the Properties of neural machine translation: encoder-decoder approaches. arXiv preprint arXiv:1409.1259 ; 2014.\\n- 50. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural Comput 9(8):1735–1780 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 51. Chen Y, Wang Z, Zeng X, Li Y, Li P, Ye X, Sakurai T (2023) Molecular language models: RNNs or transformer? Brief Funct Genomics 22(4):392–400. 10.1093/bfgp/elad012 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 52. Kingma, D. P.; Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 ; 2013.\\n- 53. Gómez-Bombarelli R, Wei JN, Duvenaud D, Hernández-Lobato JM, Sánchez-Lengeling B, Sheberla D, Aguilera-Iparraguirre J, Hirzel TD, Adams RP, Aspuru-Guzik A (2018) Automatic chemical design using a data-driven continuous representation of molecules. ACS Cent Sci 4(2):268–276. 10.1021/acscentsci.7b00572 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 54. Tevosyan A, Khondkaryan L, Khachatrian H, Tadevosyan G, Apresyan L, Babayan N, Stopper H, Navoyan Z (2022) Improving VAE based molecular representations for compound property prediction. J Cheminform 14(1):69. 10.1186/s13321-022-00648-x [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 55. Guimaraes, G. L.; Sanchez-Lengeling, B.; Outeiral, C.; Farias, P. L. C.; Aspuru-Guzik, A. Objective-Reinforced Generative Adversarial Networks (Organ) for Sequence Generation Models. arXiv preprint arXiv:1705.10843 2017 .\\n- 56. Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Courville A, Bengio Y (2014) Generative adversarial nets. Adv Neural Inf Process Syst 1:27 [ Google Scholar ]\\n- 57. Blanchard AE, Stanley C, Bhowmik D (2021) Using GANs with adaptive training data to search for new molecules. J Cheminform 13(1):14. 10.1186/s13321-021-00494-3 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 58. Popova M, Isayev O, Tropsha A (2018) Deep reinforcement learning for de novo drug design. Sci Adv 4:7 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 59. Zhou Z, Kearnes S, Li L, Zare RN, Riley P (2019) Optimization of molecules via deep reinforcement learning. Sci Rep 9(1):10752. 10.1038/s41598-019-47148-x [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 60. Atance SR, Diez JV, Engkvist O, Olsson S, De MR (2022) Novo drug design using reinforcement learning with graph-based deep generative models. J Chem Inf Model 62(20):4863–4872. 10.1021/acs.jcim.2c00838 [ DOI ] [ PubMed ] [ Google Scholar ]', '- 60. Atance SR, Diez JV, Engkvist O, Olsson S, De MR (2022) Novo drug design using reinforcement learning with graph-based deep generative models. J Chem Inf Model 62(20):4863–4872. 10.1021/acs.jcim.2c00838 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 61. Mazuz E, Shtar G, Shapira B, Rokach L (2023) Molecule generation using transformers and policy gradient reinforcement learning. Sci Rep 13(1):8799. 10.1038/s41598-023-35648-w [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 62. Pereira T, Abbasi M, Oliveira JL, Ribeiro B, Arrais J (2021) Optimizing blood-brain barrier permeation through deep reinforcement learning for de novo drug design. Bioinformatics. 10.1093/bioinformatics/btab301 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 63. Fang Y, Pan X, Shen H-B (2023) De novo drug design by iterative multiobjective deep reinforcement learning with graph-based molecular quality assessment. Bioinformatics. 10.1093/bioinformatics/btad157 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 64. Domenico A, Nicola G, Daniela T, Fulvio C, Nicola A, De ON (2020) Novo drug design of targeted chemical libraries based on artificial intelligence and pair-based multiobjective optimization. J Chem Inf Model 60(10):4582–4593. 10.1021/acs.jcim.0c00517 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 65. Cai C, Wang S, Xu Y, Zhang W, Tang K, Ouyang Q, Lai L, Pei J (2020) Transfer learning for drug discovery. J Med Chem 63(16):8683–8694. 10.1021/acs.jmedchem.9b02147 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 66. Cai C, Wang S, Xu Y, Zhang W, Tang K, Ouyang Q, Lai L, Pei J (2020) Transfer learning for drug discovery. J Med Chem 63(16):8683–8694 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 67. Merk D, Grisoni F, Friedrich L, Schneider G (2018) Tuning Artificial intelligence on the de novo design of natural-product-inspired retinoid X receptor modulators. Commun Chem 1(1):68 [ Google Scholar ]\\n- 68. Amabilino S, Pogány P, Pickett SD, Green DVS (2020) Guidelines for recurrent neural network transfer learning-based molecular generation of focused libraries. J Chem Inf Model 60(12):5699–5713. 10.1021/acs.jcim.0c00343 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 69. Pesciullesi G, Schwaller P, Laino T, Reymond J-L (2020) Transfer learning enables the molecular transformer to predict regio- and stereoselective reactions on carbohydrates. Nat Commun 11(1):4874. 10.1038/s41467-020-18671-7 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 70. Singh S, Sunoj RBA (2022) Transfer learning approach for reaction discovery in small data situations using generative model. iscience 25(7):104661. 10.1016/j.isci.2022.104661 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 71. Polykovskiy D, Zhebrak A, Vetrov D, Ivanenkov Y, Aladinskiy V, Mamoshina P, Bozdaganyan M, Aliper A, Zhavoronkov A, Kadurin A (2018) Entangled conditional adversarial autoencoder for de novo drug discovery. Mol Pharm 15(10):4398–4405. 10.1021/acs.molpharmaceut.8b00839 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 72. Kang S, Cho K (2019) Conditional molecular design with deep generative models. J Chem Inf Model 59(1):43–52. 10.1021/acs.jcim.8b00263 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 73. Gebauer NWA, Gastegger M, Hessmann SSP, Müller K-R, Schütt KT (2022) Inverse design of 3d molecular structures with conditional generative neural networks. Nat Commun 13(1):973. 10.1038/s41467-022-28526-y [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 74. Yang Y, Zheng S, Su S, Zhao C, Xu J, Chen H (2020) SyntaLinker: automatic fragment linking with deep conditional transformer neural networks. Chem Sci 11(31):8312–8322. 10.1039/D0SC03126G [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 75. Greg Landrum. RDKit: Open-Source Cheminformatics; http://Www.Rdkit.Org . http://www.rdkit.org Accessed 19 Oct 2023\\n- 76. Haddaway NR, Page MJ, Pritchard CC, McGuinness LA (2022) PRISMA2020: an r package and shiny app for producing PRISMA 2020-compliant flow diagrams, with interactivity for optimised digital transparency and open synthesis. Campbell Syst Rev 18(2):e1230. 10.1002/cl2.1230 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 77. Moret M, Grisoni F, Katzberger P, Schneider G (2022) Perplexity-based molecule ranking and bias estimation of chemical language models. J Chem Inf Model 62(5):1199–1206. 10.1021/acs.jcim.2c00079 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 78. Bajorath J (2024) Chemical language models for molecular design. Mol Inform 43(1):e202300288. 10.1002/minf.202300288 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 79. Ballarotto M, Willems S, Stiller T, Nawa F, Marschner JA, Grisoni F, De MD (2023) Novo design of Nurr1 agonists via fragment-augmented generative deep learning in low-data regime. J Med Chem 66(12):8170–8177. 10.1021/acs.jmedchem.3c00485 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]', '- 80. Grisoni F (2023) Chemical language models for de novo drug design: challenges and opportunities. Curr Opin Struct Biol 79:102527. 10.1016/j.sbi.2023.102527 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 81. Iwata H, Nakai T, Koyama T, Matsumoto S, Kojima R, Okuno Y (2023) VGAE-MCTS: a new molecular generative model combining the variational graph auto-encoder and monte carlo tree search. J Chem Inf Model 63(23):7392–7400. 10.1021/acs.jcim.3c01220 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 82. Hu C, Li S, Yang C, Chen J, Xiong Y, Fan G, Liu H, Hong L (2023) ScaffoldGVAE: scaffold generation and hopping of drug molecules via a variational autoencoder based on multi-view graph neural networks. J Cheminform 15(1):91. 10.1186/s13321-023-00766-0 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 83. Zhang G, Zhang Y, Li L, Zhou J, Chen H, Ji J, Li Y, Cao Y, Xu Z, Pian C (2024) Exploring novel fentanyl analogues using a graph-based transformer model. Interdiscip Sci 16(3):712–726. 10.1007/s12539-024-00623-0 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 84. Kong Y, Zhao X, Liu R, Yang Z, Yin H, Zhao B, Wang J, Qin B, Yan A (2022) Integrating concept of pharmacophore with graph neural networks for chemical property prediction and interpretation. J Cheminform 14(1):52. 10.1186/s13321-022-00634-3 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 85. J Gilmer, SS Schoenholz, PF Riley, O Vinyals, GE Dahl. Neural message passing for quantum chemistry. In international conference on machine learning; PMLR, 2017; pp 1263–1272.\\n- 86. Chen B, Pan Z, Mou M, Zhou Y, Fu W (2024) Is fragment-based graph a better graph-based molecular representation for drug design? a comparison study of graph-based models. Comput Biol Med 169:107811. 10.1016/j.compbiomed.2023.107811 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 87. Zhang J, Mercado R, Engkvist O, Chen H (2021) Comparative study of deep generative models on chemical space coverage. J Chem Inf Model 61(6):2572–2581. 10.1021/acs.jcim.0c01328 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 88. Wu Z, Chen J, Li Y, Deng Y, Zhao H, Hsieh C-Y, Hou T (2023) From black boxes to actionable insights: a perspective on explainable artificial intelligence for scientific discovery. J Chem Inf Model 63(24):7617–7627. 10.1021/acs.jcim.3c01642 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 89. van Deursen R, Ertl P, Tetko IV, Godin G (2020) GEN: highly efficient smiles explorer using autodidactic generative examination networks. J Cheminform 12(1):22. 10.1186/s13321-020-00425-8 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 90. Wang J, Hsieh C-Y, Wang M, Wang X, Wu Z, Jiang D, Liao B, Zhang X, Yang B, He Q, Cao D, Chen X, Hou T (2021) Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Nat Mach Intell 3(10):914–922. 10.1038/s42256-021-00403-1 [ Google Scholar ]\\n- 91. Schwaller P, Laino T, Gaudin T, Bolgar P, Hunter CA, Bekas C, Lee AA (2019) Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction. ACS Cent Sci 5(9):1572–1583. 10.1021/acscentsci.9b00576 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 92. Kwon Y, Yoo J, Choi Y-S, Son W-J, Lee D, Kang S (2019) Efficient learning of non-autoregressive graph variational autoencoders for molecular graph generation. J Cheminform 11(1):70. 10.1186/s13321-019-0396-x [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 93. Kim S, Chen J, Cheng T, Gindulyte A, He J, He S, Li Q, Shoemaker BA, Thiessen PA, Yu B, Zaslavsky L, Zhang J, Bolton EE (2023) PubChem 2023 update. Nucleic Acids Res 51(D1):D1373–D1380. 10.1093/nar/gkac956 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 94. Mendez D, Gaulton A, Bento AP, Chambers J, De Veij M, Félix E, Magariños MP, Mosquera JF, Mutowo P, Nowotka M, Gordillo-Marañón M, Hunter F, Junco L, Mugumbate G, Rodriguez-Lopez M, Atkinson F, Bosc N, Radoux CJ, Segura-Cabrera A, Hersey A, Leach AR (2019) ChEMBL: towards direct deposition of bioassay data. Nucleic Acids Res 47(D1):D930–D940. 10.1093/nar/gky1075 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 95. Irwin JJ, Tang KG, Young J, Dandarchuluun C, Wong BR, Khurelbaatar M, Moroz YS, Mayfield J, Sayle RA (2020) ZINC20—A free ultralarge-scale chemical database for ligand discovery. J Chem Inf Model 60(12):6065–6073. 10.1021/acs.jcim.0c00675 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 96. D Lowe. Chemical reactions from US patents (1976 - Sep 2016). Figshare . Dataset; 2017.\\n- 97. Xiong F, Xu H, Yu M, Chen X, Zhong Z, Guo Y, Chen M, Ou H, Wu J, Xie A, Xiong J, Xu L, Zhang L, Zhong Q, Huang L, Li Z, Zhang T, Jin F, He X (2022) 3CLpro inhibitors: DEL-based molecular generation. Front Pharmacol 1:13 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]', '- 98. Sorokina M, Merseburger P, Rajan K, Yirik MA, Steinbeck C (2021) COCONUT online: collection of open natural products database. J Cheminform 13(1):2. 10.1186/s13321-020-00478-9 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 99. NIH LINCS. LINCS L1000. NIH July 2023.\\n- 100. Chen H, Engkvist O, Wang Y, Olivecrona M, Blaschke T (2018) The rise of deep learning in drug discovery. Drug Discov Today 23(6):1241–1250. 10.1016/j.drudis.2018.01.039 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 101. Schneider P, Walters WP, Plowright AT, Sieroka N, Listgarten J, Goodnow RA, Fisher J, Jansen JM, Duca JS, Rush TS, Zentgraf M, Hill JE, Krutoholow E, Kohler M, Blaney J, Funatsu K, Luebkemann C, Schneider G (2020) Rethinking drug design in the artificial intelligence era. Nat Rev Drug Discov 19(5):353–364. 10.1038/s41573-019-0050-3 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 102. Schoenmaker L, Béquignon OJM, Jespers W, van Westen GJP (2023) UnCorrupt SMILES: a novel approach to de novo design. J Cheminform 15(1):22. 10.1186/s13321-023-00696-x [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 103. Moret M, Friedrich L, Grisoni F, Merk D, Schneider G (2020) Generative molecular design in low data regimes. Nat Mach Intell 2(3):171–180. 10.1038/s42256-020-0160-y [ Google Scholar ]\\n- 104. Méndez-Lucio O, Baillif B, Clevert D-A, Rouquié D, De WJ (2020) Novo generation of hit-like molecules from gene expression signatures using artificial intelligence. Nat Commun 11(1):10. 10.1038/s41467-019-13807-w [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 105. Chen Y, Wang Z, Wang L, Wang J, Li P, Cao D, Zeng X, Ye X, Sakurai T (2023) Deep generative model for drug design from protein target sequence. J Cheminform 15(1):38. 10.1186/s13321-023-00702-2 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 106. Zheng S, Lei Z, Ai H, Chen H, Deng D, Yang Y (2021) Deep scaffold hopping with multimodal transformer neural networks. J Cheminform 13(1):87. 10.1186/s13321-021-00565-5 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 107. Wang X, Gao C, Han P, Li X, Chen W, Rodríguez Patón A, Wang S, Zheng P (2023) PETrans: de novo drug design with protein-specific encoding based on transfer learning. Int J Mol Sci 24(2):1146 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 108. Grechishnikova D (2021) Transformer neural network for protein-specific de novo drug generation as a machine translation problem. Sci Rep 11(1):321. 10.1038/s41598-020-79682-4 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 109. Kotsias P-C, Arús-Pous J, Chen H, Engkvist O, Tyrchan C, Bjerrum EJ (2020) Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks. Nat Mach Intell 2(5):254–265. 10.1038/s42256-020-0174-5 [ Google Scholar ]\\n- 110. Mao J, Wang J, Zeb A, Cho K-H, Jin H, Kim J, Lee O, Wang Y, No KT (2023) Transformer-based molecular generative model for antiviral drug design. J Chem Inf Model. 10.1021/acs.jcim.3c00536 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 111. Chang J, Ye JC (2024) Bidirectional generation of structure and properties through a single molecular foundation model. Nat Commun 15(1):2323. 10.1038/s41467-024-46440-3 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 112. Zhumagambetov R, Molnár F, Peshkov VA, Fazli S (2021) Transmol: repurposing a language model for molecular generation. RSC Adv 11(42):25921–25932. 10.1039/D1RA03086H [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 113. Marin Zapata PA, Méndez-Lucio O, Le T, Beese CJ, Wichard J, Rouquié D, Clevert D-A (2023) Cell morphology-guided de novo hit design by conditioning GANs on phenotypic image features. Digital Discov 2(1):91–102. 10.1039/D2DD00081D [ Google Scholar ]\\n- 114. Liu Y, Yu H, Duan X, Zhang X, Cheng T, Jiang F, Tang H, Ruan Y, Zhang M, Zhang H, Zhang Q (2024) TransGEM a molecule generation model based on transformer with gene expression data. Bioinformatics. 10.1093/bioinformatics/btae189 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 115. Fedus, W.; Goodfellow, I.; Dai, A. M. Maskgan: Better Text Generation via Filling in The_. arXiv preprint arXiv:1801.07736 2018.\\n- 116. Lee YJ, Kahng H, Kim SB (2021) Generative adversarial networks for de novo molecular design. Mol Inform 40(10):2100045. 10.1002/minf.202100045 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 117. Zhao, J.; Kim, Y.; Zhang, K.; Rush, A.; LeCun, Y. Adversarially Regularized Autoencoders. In international conference on machine learning; PMLR, 2018; 5902–5911.\\n- 118. Hong SH, Ryu S, Lim J, Kim WY (2020) Molecular generative model based on an adversarially regularized autoencoder. J Chem Inf Model 60(1):29–36. 10.1021/acs.jcim.9b00694 [ DOI ] [ PubMed ] [ Google Scholar ]', '- 118. Hong SH, Ryu S, Lim J, Kim WY (2020) Molecular generative model based on an adversarially regularized autoencoder. J Chem Inf Model 60(1):29–36. 10.1021/acs.jcim.9b00694 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 119. Wu B, Li L, Cui Y, Zheng K (2022) Cross-adversarial learning for molecular generation in drug design. Front Pharmacol 12:1 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 120. Abbasi M, Santos BP, Pereira TC, Sofia R, Monteiro NRC, Simões CJV, Brito RMM, Ribeiro B, Oliveira JL, Arrais JP (2022) Designing optimized drug candidates with generative adversarial network. J Cheminform 14(1):40. 10.1186/s13321-022-00623-6 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 121. Ai C, Yang H, Liu X, Dong R, Ding Y, Guo F (2024) MTMol-GPT: de novo multi-target molecular generation with transformer-based generative adversarial imitation learning. PLoS Comput Biol 20(6):e1012229 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 122. Skinnider MA (2024) Invalid SMILES are beneficial rather than detrimental to chemical language models. Nat Mach Intell 6(4):437–448. 10.1038/s42256-024-00821-x [ Google Scholar ]\\n- 123. Zou J, Zhao L, Shi S (2023) Generation of focused drug molecule library using recurrent neural network. J Mol Model 29(12):361. 10.1007/s00894-023-05772-5 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 124. Bian Y, Xie X-Q (2022) Artificial intelligent deep learning molecular generative modeling of scaffold-focused and cannabinoid CB2 target-specific small-molecule sublibraries. Cells 11(5):915 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 125. Yasonik J (2020) Multiobjective de Novo drug design with recurrent neural networks and nondominated sorting. J Cheminform 12(1):14. 10.1186/s13321-020-00419-6 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 126. Harel S, Radinsky K (2018) Prototype-based compound discovery using deep generative models. Mol Pharm 15(10):4406–4416. 10.1021/acs.molpharmaceut.8b00474 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 127. Wei L, Fu N, Song Y, Wang Q, Hu J (2023) Probabilistic generative transformer language models for generative design of molecules. J Cheminform 15(1):88. 10.1186/s13321-023-00759-z [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 128. Liao Z, Xie L, Mamitsuka H, Zhu S (2023) Sc2Mol: A scaffold-based two-step molecule generator with variational autoencoder and transformer. Bioinformatics. 10.1093/bioinformatics/btac814 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 129. Zhu H, Zhou R, Cao D, Tang J, Li M (2023) A pharmacophore-guided deep learning approach for bioactive molecular generation. Nat Commun 14(1):6234. 10.1038/s41467-023-41454-9 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 130. Atz K, Cotos L, Isert C, Håkansson M, Focht D, Hilleke M, Nippa DF, Iff M, Ledergerber J, Schiebroek CCG, Romeo V, Hiss JA, Merk D, Schneider P, Kuhn B, Grether U, Schneider G (2024) Prospective de Novo drug design with deep interactome learning. Nat Commun 15(1):3408. 10.1038/s41467-024-47613-w [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 131. Langevin M, Minoux H, Levesque M, Bianciotto M (2020) Scaffold-constrained molecular generation. J Chem Inf Model 60(12):5637–5646. 10.1021/acs.jcim.0c01015 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 132. Diao Y, Liu D, Ge H, Zhang R, Jiang K, Bao R, Zhu X, Bi H, Liao W, Chen Z, Zhang K, Wang R, Zhu L, Zhao Z, Hu Q, Li H (2023) Macrocyclization of linear molecules by deep learning to facilitate macrocyclic drug candidates discovery. Nat Commun 14(1):4552. 10.1038/s41467-023-40219-8 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 133. Bagal V, Aggarwal R, Vinod PK, Priyakumar UD (2022) MolGPT: molecular generation using a transformer-decoder model. J Chem Inf Model 62(9):2064–2076. 10.1021/acs.jcim.1c00600 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 134. Haroon S (2023) Generative pre-trained transformer (GPT) based model with relative attention for de novo drug design. Comput Biol Chem. 10.1016/j.compbiolchem.2023.107911 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 135. Wang Y, Zhao H, Sciabola S, Wang W (2023) CMolGPT: a conditional generative pre-trained transformer for target-specific de novo molecular generation. Molecules 28(11):4430 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 136. Monteiro NRC, Pereira TO, Machado ACD, Oliveira JL, Abbasi M, Arrais JP (2023) FSM-DDTR: end-to-end feedback strategy for multi-objective de novo drug design using transformers. Comput Biol Med 164:107285. 10.1016/j.compbiomed.2023.107285 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 137. Fan W, He Y, Zhu F (2024) RM-GPT: enhance the comprehensive generative ability of molecular GPT model via localRNN and realformer. Artif Intell Med 150:102827. 10.1016/j.artmed.2024.102827 [ DOI ] [ PubMed ] [ Google Scholar ]', '- 137. Fan W, He Y, Zhu F (2024) RM-GPT: enhance the comprehensive generative ability of molecular GPT model via localRNN and realformer. Artif Intell Med 150:102827. 10.1016/j.artmed.2024.102827 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 138. Kim H, Na J, Lee WB (2021) Generative chemical transformer: neural machine learning of molecular geometric structures from chemical language via attention. J Chem Inf Model 61(12):5804–5814. 10.1021/acs.jcim.1c01289 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 139. Yoshikai, Y.; Mizuno, T.; Nemoto, S.; Kusuhara, H. A Novel Molecule Generative Model of VAE Combined with Transformer. arXiv preprint arXiv:2402.11950 2024.\\n- 140. Inukai, T.; Yamato, A.; Akiyama, M.; Sakakibara, Y. A Tree-transformer based vae with fragment tokenization for large chemical models; 2024.\\n- 141. Bhadwal AS, Kumar K, Kumar N (2024) NRC-VABS: normalized reparameterized conditional variational autoencoder with applied beam search in latent space for drug molecule design. Expert Syst Appl 240:122396. 10.1016/j.eswa.2023.122396 [ Google Scholar ]\\n- 142. Liu D, Song T, Na K, Wang S (2024) PED: a novel predictor-encoder-decoder model for alzheimer drug molecular generation. Front Artif Intell 7:137418 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 143. Özçelik R, de Ruiter S, Criscuolo E, Grisoni F (2024) Chemical language modeling with structured state space sequence models. Nat Commun 15(1):6176 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 144. Hu P, Zou J, Yu J, De SS (2023) Novo drug design based on stack-RNN with multi-objective reward-weighted sum and reinforcement learning. J Mol Model 29(4):121. 10.1007/s00894-023-05523-6 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 145. Tan X, Jiang X, He Y, Zhong F, Li X, Xiong Z, Li Z, Liu X, Cui C, Zhao Q, Xie Y, Yang F, Wu C, Shen J, Zheng M, Wang Z, Jiang H (2020) Automated design and optimization of multitarget schizophrenia drug candidates by deep learning. Eur J Med Chem 204:112572. 10.1016/j.ejmech.2020.112572 [ DOI ] [ PubMed ] [ Google Scholar ]\\n- 146. Shi T, Huang S, Chen L, Heng Y, Kuang Z, Xu L, Mei H (2020) A molecular generative model of ADAM10 inhibitors by using GRU-based deep neural network and transfer learning. Chemom Intelligent Lab Syst. 10.1016/j.chemolab.2020.104122 [ Google Scholar ]\\n- 147. Lee J, Myeong I-S, Kim Y (2023) The Drug-like molecule pre-training strategy for drug discovery. IEEE Access 11:61680–61687. 10.1109/ACCESS.2023.3285811 [ Google Scholar ]\\n- 148. Li S, Wang L, Meng J, Zhao Q, Zhang L, Liu H (2022) De novo design of potential inhibitors against SARS-CoV-2 Mpro. Comput Biol Med 147:105728. 10.1016/j.compbiomed.2022.105728 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 149. Santana MVS, De S-J (2021) Novo design and bioactivity prediction of SARS-CoV-2 main protease inhibitors using recurrent neural network-based transfer learning. BMC Chem 15(1):8. 10.1186/s13065-021-00737-2 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 150. Suresh N, Kumar NCA, Subramanian S, Srinivasa G (2022) Memory augmented recurrent neural networks for De-novo drug design. PLoS ONE 17:6 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 151. Thomas M, O’Boyle NM, Bender A, de Graaf C (2022) Augmented Hill-Climb increases reinforcement learning efficiency for language-based de novo molecule generation. J Cheminform 14(1):68. 10.1186/s13321-022-00646-z [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 152. Thomas M, Smith RT, O’Boyle NM, de Graaf C, Bender A (2021) Comparison of structure- and ligand-based scoring functions for deep generative models: a GPCR case study. J Cheminform 13(1):39. 10.1186/s13321-021-00516-0 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 153. Shen X, Zeng T, Chen N, Li J, Wu R (2024) NIMO: a natural product-inspired molecular generative model based on conditional transformer. Molecules 29(8):1867 [ DOI ] [ PMC free article ] [ PubMed ] [ Google Scholar ]\\n- 154. Fatima N, Imran AS, Kastrati Z, Daudpota SM, Soomro A (2022) A systematic literature review on text generation using deep neural network models. IEEE Access 10:53490–53503 [ Google Scholar ]', '## Associated Data\\n\\nThis section collects any data citations, data availability statements, or supplementary materials included in this article.\\n\\n### Data Availability Statement\\n\\nNo datasets were generated or analysed during the current study.', '# Images of chemical structures as molecular representations for deep learning\\n\\n- Open access\\n- Published: 07 July 2022\\n- Volume\\xa037 ,\\xa0pages 2293–2303, ( 2022 )\\n- Cite this article\\nDownload PDF\\nYou have full access to this open access article\\nJournal of Materials Research\\nAims and scope\\nSubmit manuscript\\nImages of chemical structures as molecular representations for deep learning\\nDownload PDF\\n- Matthew R. Wilkinson 1 , 2 , 3 na1 ,\\n- Uriel Martinez-Hernandez 4 na1 ,\\n- Chick C. Wilson 2 , 3 , 5 na1 &\\n- Bernardo Castro-Dominguez ORCID: orcid.org/0000-0001-5913-305X 1 , 2 , 3\\nShow authors\\n- 3560 Accesses\\n- 5 Citations\\n- 3 Altmetric\\n- Explore all metrics\\nImplementing Artificial Intelligence for chemical applications provides a wealth of opportunity for materials discovery, healthcare and smart manufacturing. For such applications to be successful, it is necessary to translate the properties of molecules into a digital format so they can be passed to the algorithms used for smart modelling. The literature has shown a wealth of different strategies for this task, yet there remains a host of limitations. To overcome these challenges, we present two-dimensional images of chemical structures as molecular representations. This methodology was evaluated against other techniques in both classification and regression tasks. Images unlocked (1) superior augmentation strategies, (2) application of specialist network architectures and (3) transfer learning, all contributing to superior performance and without prior specialised knowledge on cheminformatics required. This work takes advantage of image feature maps which do not rely on chemical properties and so can represent multi-component systems without further property calculations.\\n\\n### Graphical abstract\\n\\n### Similar content being viewed by others\\n\\n### Review of techniques and models used in optical chemical structure recognition in images and scanned documents\\n\\nOpen access\\n09 September 2022\\n\\n### Automated molecular structure segmentation from documents using ChemSAM\\n\\nOpen access\\n12 March 2024\\n\\n### An explainable deep learning platform for molecular discovery\\n\\n09 December 2024\\n\\n### Explore related subjects\\n\\nDiscover the latest articles and news from researchers in related subjects, suggested using machine learning.\\n- Cheminformatics\\n- Computational Chemistry\\n- Data Storage Representation\\n- Mathematical Applications in Chemistry\\n- Molecular Modelling\\n- Symbolic AI\\nUse our pre-submission checklist\\nAvoid common mistakes on your manuscript.\\n\\n## Introduction', 'Accurately representing the properties of molecules is a critical challenge in the adoption of Artificial Intelligence (AI) for chemical applications. Thus, molecular representation is defined as the process of capturing the complex molecular detail and converting it to a machine-readable form, which can be used as inputs for AI algorithms. Approaches to molecular representation include molecular descriptors, fingerprints, Simplified Molecular Input Line Entry System (SMILES), and molecular graphs with information stored in a variety of encoded file formats as reviewed by [ 1 ]. These approaches vary in success but are all difficult for a common user to implement and often require significant computational resources. For an algorithm to make use of the input, the representation of a molecule must either effectively identify key properties that can be correlated to a target, perhaps on a multidimensional scale, or the representation must determine a degree of structural similarity between input molecules. In this work, the networks extract feature maps from images that do not rely on chemical structure or property details.\\nThe use of AI is especially relevant to industrial manufacturing of chemical products, in particular pharmaceuticals [ 2 ]. As focus shifts towards Industry 4.0, companies are striving for the widespread adoption of AI to overcome high attrition rates and achieve overall more sustainable manufacturing approaches [ 3 ]. Therefore, as a case study, to showcase the potential of image inputs, two open-source datasets were selected each representing key challenges in chemical manufacturing. As pharmaceuticals account for a large portion of the research in this space, these sets were chosen to focus on key areas of interest in solid-state chemistry, a critical step in the drug discovery process.\\nSolid form impacts both the manufacturing process and efficacy of pharmaceutical products [ 4 , 5 ]. Crystallisation is used as a primary purification technique to isolate Active Pharmaceutical Ingredients (API) during synthesis. Altering crystallisation conditions has the potential to impact both the physical and chemical properties of the solid-state API [ 6 ]. Furthermore, manufacturing processes must consider the solid form, as downstream processing relies heavily on bulk properties such as particle size and flowability [ 7 ]. A further level of complexity can be introduced through the synthesis of Co-Crystals (CC)s [ 8 ]. These multi-component materials can be carefully designed to offer increased solubility, bioavailability, and stability [ 9 ]. Due to the vast degree of variation which can be induced by changing the crystallisation components and methods, the process must be carefully engineered to produce the most desirable critical quality attributes.', 'Molecular descriptors are often the first choice for molecular representation in chemical applications of machine learning. Reasons for this include ready availability from open-source software packages and their growing historical record of publication. Examples that focus on the solid form engineering applications relevant to the case study presented in this work include crystallisation propensity, CC propensity, solubility, and even amorphous properties such as glass forming ability [ 10 , 11 , 12 , 13 , 14 , 15 , 16 ]. Descriptors represent tangible molecular properties that a user can readily translate into practical terms which can be controlled experimentally to affect the application’s target [ 17 ]. This is not a perfect scenario; however, as searching vast multidimensional inputs can make it difficult to gather data sufficiently large to cover the search space as well as making identification of specific important properties a challenge. If feature importance rankings are desirable, one is also unable to use dimensionality reduction to combat such limitations, as the original features are not preserved in such methods. As work into molecular representation has advanced, other methods of generating meaningful representations have been reported, which generate varying numbers of molecular descriptors [ 18 ], creature unique molecular fingerprints [ 19 , 20 ], or generate features through unsupervised learning [ 21 ]. In addition, the use of molecular graphs has attracted recent attention as discussed by [ 1 ]. Although promising candidates for molecular representation, in this study, molecular graphs have been excluded due to the lack of evidence in their use representing multi-component chemical systems. Furthermore, graphs have been identified as computationally demanding and do not offer the wider accessibility to AI methods that the authors hope using images can provide. The literature suggests that results vary depending on the choice of molecular representation used and so comparison against all suitable methods was essential for any new method [ 22 ]. To date, chemical applications of AI typically focus on testing different machine-learning or deep-learning algorithms and as such, there is no consideration as to if the information within the chemical representation is optimal.\\nDuring testing different molecular representations, capturing Three-Dimensional (3D) information, a feature heavily dependent on molecular conformation is particularly challenging. Most software implementations use Simplified Molecular Input Line Entry System (SMILES) as inputs which are Two Dimensional (2D), thus, forcing 3D information to be approximated from a predicted molecular conformation rather than the one observed experimentally [ 23 ]. SMILES is also challenging to work with as multiple SMILES can represent the same molecule; therefore, to overcome this, canonical SMILES was proposed [ 24 ]. It should be noted that the different representations are not necessarily detrimental to performance, and this has even been leveraged as an augmentation strategy when training models and so should not alone be responsible for avoiding the use of such notation [ 25 ]. Finally, and most notably, often molecular representation software fails for certain molecules, something which proves very unhelpful when both generating training data and even more so if testing molecules of interest in a deployment scenario.\\nWhen considering the context of this work, there are yet further limitations to consider. For tasks involving multiple components, such as CC prediction, many representations often fall short in their ability to represent the system as a whole, as the employed methods typically see a simple concatenation of the individual component descriptor sets. This fails to account for any emergent properties present as a result of the mixed components. Multicomponent solvents are a prominent example of this issue, where using descriptors of each component does not accurately describe the properties of the mixture itself [ 26 ]. Images are able to overcome this limitation, as the feature map extracted by the convolutional layers is independent of the chemical properties, and as such, there is no information loss or misrepresentation during the concatenation process. This means that there is no need to calculate emergent chemical properties to use as descriptor style features for predicting a desired target.', 'This work takes its inspiration from the use of deep learning for non-traditional image recognition tasks. Such applications have been showcased by [ 27 ] in their book, including sound classification, fraud detection, and malware identification. These examples highlight the potential for representing data as images, even when doing so is not an immediately obvious step. By using images to represent data, transfer learning can be implemented to take advantage of pre-trained network architectures that have been rigorously researched and assessed for their capability to accurately classify images. Assessment for such networks can be seen in the annual ImageNet Large-Scale Visual Recognition Challenge [ 28 ]. Combining the simplicity of generating chemical structures with these specialist network architectures offers vast potential in improving the performance of intelligent models both within solid form engineering and in wider chemical applications. At the time of writing, literature has recorded two examples of images used as inputs to models for chemical applications of deep learning [ 29 , 30 ].\\nIn this work, transfer learning with ResNet architectures was applied for improved performance. We provide for the first time, comprehensive comparisons to other methods of molecular representations using publicly available datasets. Previously untested datasets were used to prove that the methods can be extended to new applications as well as showing that images are suitable inputs to deep-learning models in both classification and regression tasks. Previous work assesses convolutional neural networks against other machine-learning methods, but there is no comparison between molecular inputs. Finally, we demonstrate the modelling of multi-component systems where two inputs must be mapped to one output, something as yet unseen in chemical literature to the best of our knowledge. To overcome the limitations associated with current molecular representations, this work demonstrates that the use of images representing a molecule’s skeletal structure captures sufficient, relevant features. This representation is not only in\\xa0a machine-readable format, but the images can easily be generated by a user through the use of readily available molecular drawing packages. The advantages of image inputs were evaluated by comparing the performance against published models with a number of different molecular representations. All the datasets and methods used to assess performance in this work have been made open-source and are accessible alongside their respective publications.', '## Results and discussion\\n\\n### Model performance\\n\\nMetrics for all of the models are shown in Table 1 . In all cases, the ResNet models with augmentations outperformed all other models.\\nTABLE 1 Metrics for all models recorded as the mean of 3 independent trials with the best metrics shown in bold.\\nFull size table\\nAttributing the performance to the use of images alone would paint a misleading picture. As previously mentioned, ResNet architectures are highly specialised for their intended deep-learning applications which will undoubtedly contribute to a portion of the observed performance gain. Further consideration must also be assigned to the additional advantages that are available through the use of augmentations that can be leveraged when working with images as model inputs. Data augmentation is known to improve a model’s ability to generalise. When augmenting the inputs, a user is artificially enlarging both the diversity and the size of the training data which in turn improves its performance and reduces the risk of overfitting [ 31 ]. The value in these augmentations is made especially clear when looking at the solubility data set, where the model performs worse than those with other inputs when augmentations are not used. In both data sets there is a significant performance gain suggesting that augmentation strategies should be common practice when using image models.\\nDuring training, the models used learning rate decay and as such it is essential to consider the risk of overfitting. Figure 1 shows the loss for the validation and training sets during the training cycle. As expected, the training loss gradually reduces over time; however, it is observed that towards the end of the training process, there is an increase in validation loss, which is indicative of overfitting. For this reason, it is essential to stop training at a suitable point so as to avoid modelling noise in the data.\\nTraining (blue) and validation (yellow) loss for Co-crystal (left) and Solubility (right) during training of the ResNet with augmentations models. The plot represents 1 randomly selected split during the cross-validations process.\\nFull size image\\n\\n### Model evaluation', '### Model evaluation\\n\\nDespite the advantages of image inputs, as with all models, mistakes are present during inference. By visualising the validation examples in which the model was incorrect, the user can begin to identify problematic functional groups or molecular structures. Figure 2 shows validation examples in which the model was incorrect or uncertain about its prediction, displaying the input image with the predicted label, actual label, loss, and the probability which describes how certain the model is of its prediction. In the bottom right case, the model gets the prediction correct; however, it is uncertain; hence, the result makes it into the top candidates to assess in this case. The molecules displayed are taken during the final validation pass during the first iteration of the cross-validation process.\\nIn the CC dataset, single-ring aromatic compounds with two functional groups attached to the ring (e.g. 3-Methylbenzamide) appear to cause problems for the model. By comparing the image models with and without augmentations applied, one can suggest that the augmentation strategy was not entirely effective in resolving this issue, as these structures appear in both cases. A more high-level analysis seems to suggest that the lower molecular weight structures, being those with more simple structures, are harder to accurately predict for. This follows logically as more simple structures will have less unique details and as such prove more difficult for the model to differentiate. Analysis methods like this are a clear advantage when compared to assessing vast descriptor tables, and conclusions can be further used to inform further experimental study or design new augmentation strategies in order to try and minimise the errors in predictions.\\nTop examples on which the model was more unsure or incorrect in prediction on the validation set of the CC data set without augmentations (left) and with augmentations (right).\\nFull size image\\nAs the domain of the different inputs was not the same, it was important that testing was not limited exclusively to deep-learning methods. Experience and literature show that when working with tabular data, random forests often match or outperform neural networks [ 32 ]. This is especially apparent when working with datasets of limited size, or where the domain of the data prevents the use of domain specific-network architectures as is the case for tabular data [ 33 ]. Neural networks have seen vast success in applications including image recognition, speech recognition, and natural language processing; however, when working with tabular data, they present limitations. As such, measuring the advantages of using images and residual networks against random forests models was key to demonstrating that there truly is an advantage to the proposed method.\\n\\n### Input generation', '### Input generation\\n\\nUsing meaningful and unique chemical identifiers remains a challenge when passing molecular information to a computer. In this work, SMILES was used to uniquely identify each molecule. Despite its popularity, SMILES still presents issues especially when they are the start point for representation calculations. SMILES is a 2D representation which lacks any kind of 3D conformational detail. As a result of this, when calculating 3D descriptors, there are assumptions that must be made, thus introducing uncertainty. Although this uncertainty is reproducible and consistent, it impacts how meaningful the features can be, something especially important to consider if a representation is used to correlate molecular properties to a given target. 3D factors affect the properties of a molecule and so it is therefore important that a user gives these details to the model with the greatest possible degree of accuracy.\\nIn contrast, using images removes the challenge of extrapolating the dimensionality as they are, like the SMILES codes, 2D. Images are significantly less reliant on exact conformations for accurate representations compared to descriptor calculations, as different conformations of the molecule can be accounted for through careful selection of appropriate image augmentation steps. Furthermore, images rely on the network to extract meaningful features rather than providing specific chemical property information that is challenging to get completely correct. It is reasonable to argue that 2D images also lack the 3D chemical information that descriptors do, but the contrast in approach must be considered here. In a descriptor style model, the algorithm is aiming to correlate chemical properties to the output, where as in the images case, it is the feature map which is a result of the convolutional operations. As the convolutional features are not reliant on any chemical information and give no consideration to the context of the image they act upon, missing the 3D chemical detail is unlikely to limit the predictive power. Passing 3D molecular structures as inputs could well offer advantages but the increase in complexity and computational demand would be significant.\\n\\n### Data pre-processing and augmentations', 'The importance of careful image generation cannot be overstated in this work. When generating the training data, software packages with drawing tools or the ability to produce images are an important step in the process. For this work, RDKit was specifically chosen to ensure all images were reproducible and consistent, whilst additionally acting to minimise the level of diversity that can be introduced through hand drawing molecules. Deep-learning methods have been applied to hand-written digits and text and have\\xa0even seen high levels of success despite the associated challenges. Training an effective image-processing network, which is able to translate hand-drawn structures to suitable inputs for property prediction is far beyond the scope of this work. The focus here is in extracting useful features that the network can map to the desired output and as such, it was necessary to remove as many of the common issues associated with manual drawing as was possible to maximise the potential for success. Even with the use of software tools, there is still a reasonable level of diversity that a user can introduce if they are to draw a molecule. Therefore, it is essential that the model is trained on more than a single example of each structure so that the network is exposed to as many conformational changes as it is likely to see. This is done through the use of augmentations. When using images as inputs, the variety of augmentation options far exceeds those compared to representations captured as tabular data. Despite the wealth of opportunity, image augmentations must be chosen with caution when working with molecules. Having generated uniform inputs where bond lengths, bond angles and functional group structures are reproducible, it is unnecessary to expose the network to overly distorted structures. This is because there is no reason for a distorted structure to be presented during inference, assuming the input generation remains controlled. As a result, any functional group that appears unusual to the network is a result of its genuine structure rather than a poorly constructed input. For this reason, augmentations involving distortions or warping were not included. There is a strong possibility that translations, rotations or reflections will be seen between molecules containing similar chemical structures. This is especially likely if a user was to generate structures manually using alternative software packages, as there is no universal system for reproducible structure drawing with a predefined orientation. In addition, if structures were to be generated manually, careful consideration must be given to ensure that the final image is the same pixel size as the images used to train the model. Convolutional neural networks are by design equipped to handle the translational effects and so augmentations in this work were only applied to cope with the effects of rotations and reflections rather than the position of the molecule in the image [ 34 ].\\nUsing images of molecular structures as inputs allows end users to easily generate inputs without requiring the programming expertise often needed for descriptor calculations. This approach is not only advantageous due to the accuracy of the predictions, but the simplicity associated with drawing chemical structures (even when software packages such as ChemDraw/RDKit are required) far outweighs the lengthy and complex process of calculating molecular descriptors. Furthermore, manually drawing structures for evaluation removes the challenges associated with failed descriptor calculations that often occur when working with molecules such as ions or salts. It is important that structures are generated using software packages as they maintain a level of consistency in aspects such as bond lengths, bond angles, and functional group notation. This image uniformity is likely important to maintaining performance. To account for a more diverse set of input images, augmentation strategies are recommended for future studies. This is particularly relevant when the images are generated from a wide range of software packages and passed to a single model. Although this was beyond the scope of this work, the authors suggest RanDepict as an effective solution to handling inputs from different drawing packages [ 35 ].', 'For high-throughput evaluation, images are faster to generate than the other inputs used in this work, and having the flexibility to either manually or autonomously generate a user readable input can only be advantageous. When trying to draw conclusions from the predictions, having a structure that users can see and interpret offers far more potential for success than scanning vast tables of seemingly arbitrary descriptor values. With that said, images are not able to correlate specific properties to the target label in the manner in which descriptors can. Such correlations often act across multiple dimensions making interpretation difficult as dimensionality reduction techniques must be implemented, destroying the original features.', '## Conclusions\\n\\nThis work presents the use of 2D chemical images as molecular representations allowing for the utilisation of transfer learning, taking advantage of specialist deep-learning network architectures and, thus, achieving a superior performance when compared to chemical descriptor models. Images also allow the user to leverage data augmentation which further increases the predictive capabilities of the model by expanding the size and diversity of the datasets. The evaluation process shows the potential of the models in both classification and regression tasks as well as providing benchmarks using other common forms of molecular representation to demonstrate the transfer learning approach has a clear advantage. Images as molecular representations do not require specialist chemometrics understanding. We foresee that this methodology will address limitations in descriptor and fingerprint methods widening the scope for the application of AI in the materials discovery community.\\nThe datasets selected represent key areas of interest in the field of chemical manufacturing and can be used as examples of both classification and regression tasks. The datasets in this work include aqueous solubility and co-crystal propensity via mechanochemistry, each regarding small organic molecules.\\n\\n#### Aqueous solubility\\n\\nThis dataset contains unique SMILES codes and their corresponding logS solubility values in water at 25 °C. The full dataset used in this work was a combination of the open-source AqSolDB [ 36 ], and the data published by [ 37 ] Cleaning steps were applied to remove any repeated SMILES codes.\\n\\n#### Co-crystal propensity via mechanochemistry\\n\\nThis dataset published by [ 14 ] offers 1000 co-crystallisation events and records their outcome as determined by powder X-ray diffraction. Crystalline products were recorded as 1 and amorphous products recorded as 0.\\n\\n### Data preparation\\n\\nSMILES codes were used as unique chemical identifiers in all cases. These were employed as inputs when generating all of the representations used. The images were generated systematically using the RDKit cheminformatics Python package ( https://github.com/rdkit/rdkit ). Both the solubility images and CC images were generated at 250 × 250 pixels. In the CC dataset where two input molecules make up each data point, images were generated for each molecule individually and then stacked one above the other resulting in a 250 × 500 pixel image (see Fig. 3 ). The image sizes were chosen to balance the computation time required for training and the clarity of the molecular structure in the image. Using larger images gave rise to excessive white space, which in some cases impacted performance. Smaller images could not be easily interpreted by the end user; a limitation that significantly detracts from the interpretability of the predictions. It is important to note that preliminary work found using greyscale images provided no performance improvement; therefore, only colour images were used in this study\\nStacked images used as inputs for the CC data set.\\nFull size image\\nFor assessing the model against literature methods, the following input styles were used:\\n- Spectrophore [ 19 ]\\n- PubChem Fingerprint [ 38 ]\\n- Extended Connectivity Fingerprint (ECFP) [ 20 ]\\n- MACCS Key Fingerprint [ 39 ]\\n- Mol2Vec [ 21 ]\\n- RDKit Descriptors ( https://www.rdkit.org/ )\\n- Mordred Chemical Descriptors [ 18 ]\\nMordred descriptors were generated using the Mordred Python package ( https://github.com/mordred-descriptor/mordred ). All other non-image representations were generated with DeepChem ( https://github.com/deepchem/deepchem ) and its inbuilt functionality [ 40 ]. In this work, any single descriptor that failed for all compounds was removed. In the case of the CC dataset, descriptors were individually calculated for each component and then combined through concatenation. Cleaning was applied independently for every representation calculated in an effort to minimise the number of descriptors lost to failed calculations. Where whole molecules failed to calculate they were removed. This changed the number of training examples; however, given the change in size was minimal, the benchmark was included. This was not true for the PubChem fingerprint on the solubility set where the failed calculation rate was too high to remain a useful benchmark.\\n\\n### Data augmentation\\n\\nAll of the data augmentation methods outlined below were applied only to the training dataset. Due to the nature of the different datasets, augmentation was applied independently to each dataset as outlined.\\n\\n#### Solubility data augmentation\\n\\nRotations were applied (6 × 30°) to each training example first, and then the resulting images were subjected to three reflections. These reflections occurred along the horizontal axis, along the vertical axis and along both axes together. A full example of the augmentation process is shown in Fig. 4 .\\n\\n#### Co-crystal data augmentation', '#### Co-crystal data augmentation\\n\\nComponents were additionally stacked in both possible positions, namely component 1 above component 2 and vice versa. Following this, all of the augmentations outlined for the solubility dataset were carried out (Fig. 4 ).\\nImage augmentations applied to the training sets of both datasets.\\nFull size image\\n\\n### Model implementation\\n\\nA ResNet architecture was used to evaluate the performance of images as inputs. Full details on ResNet architectures along with a comprehensive list of the model’s performance in competitions can be seen in the original publication showcasing their use [ 41 ]. The network was loaded with pre-trained weights from the ImageNet dataset, and two untrained fully connected layers were added at the end of the network see Fig. 5 .\\nFull network architecture used for deep learning.\\nFull size image\\nImplementation of the random forest algorithm was carried out using the Scikit-Learn python package.\\n\\n### Evaluation and metrics\\n\\nFor performance evaluation, each dataset was split into training and validation subsets using tenfold cross-validation. In every case, three independent trials were carried out to ensure any differences in performance were both statistically significant and not arising due to the seeding in the random aspects of the models. All the metrics recorded were calculated by taking the mean of the three trials. The metrics used to assess the performance were accuracy (ACC) and Area Under the Receiver Operating Characteristic Curve (ROC) for the classification task. In the regression task, R 2 , mean-squared error (MSE), root-mean-squared error (RMSE), and mean absolute error (MAE) were recorded. These metrics were chosen as they are all commonly used in the literature for evaluating AI models.\\n\\n## Data availability\\n\\nAll data used in this work were open-source and can be found alongside the original publications [ 14 , 36 , 37 ].\\n\\n## Code availability', 'The source code for this project is available at https://github.com/MRW-Code/images_as_molecular_representations .\\n- L. David, A. Thakkar, R. Mercado, O. Engkvist, Molecular representations in AI-driven drug discovery: a review and practical guide. J. Cheminform. 12 (1), 56 (2020). https://doi.org/10.1186/s13321-020-00460-5 Article CAS Google Scholar\\n- S. Nagaprasad, D.L. Padmaja, Y. Qureshi, S.L. Bangare, M. Mishra, B.D. Mazumdar, Investigating the impact of machine learning in pharmaceutical industry. J. Pharm. Res. Int. 33 , 6–14 (2021). https://doi.org/10.9734/JPRI/2021/v33i46A32834 Article Google Scholar\\n- K.-K. Mak, M.R. Pichika, Artificial intelligence in drug development: present status and future prospects. Drug Discov. Today 24 , 773–780 (2019). https://doi.org/10.1016/j.drudis.2018.11.014 Article Google Scholar\\n- L.S. Taylor, D.E. Braun, J.W. Steed, Crystals and crystallization in drug delivery design. Cryst. Growth Des. 21 (3), 1375–1377 (2021). https://doi.org/10.1021/acs.cgd.0c01592 Article CAS Google Scholar\\n- C.R. Gardner, C.T. Walsh, Ö. Almarsson, Drugs as materials: valuing physical form in drug discovery. Nat. Rev. Drug Discov. 3 (11), 926–934 (2004). https://doi.org/10.1038/nrd1550 Article CAS Google Scholar\\n- J.K. Haleblian, Characterization of habits and crystalline modification of solids and their pharmaceutical applications. J. Pharm. Sci. 64 (8), 1269–1288 (1975). https://doi.org/10.1002/jps.2600640805 Article CAS Google Scholar\\n- N. Pudasaini, P.P. Upadhyay, C.R. Parker, S.U. Hagen, A.D. Bond, J. Rantanen, Downstream processability of crystal habit-modified active pharmaceutical ingredient. Org. Process Res. Dev. 21 (4), 571–577 (2017). https://doi.org/10.1021/acs.oprd.6b00434 Article CAS Google Scholar\\n- N. Qiao, M. Li, W. Schlindwein, N. Malek, A. Davies, G. Trappitt, Pharmaceutical cocrystals: an overview. Int. J. Pharm. 419 (1), 1–11 (2011). https://doi.org/10.1016/j.ijpharm.2011.07.037 Article CAS Google Scholar\\n- D.J. Good, N. Rodríguez-Hornedo, Solubility advantage of pharmaceutical cocrystals. Cryst. Growth Des. 9 (5), 2252–2264 (2009). https://doi.org/10.1021/cg801039j Article CAS Google Scholar\\n- A. Ghosh, L. Louis, K.K. Arora, B.C. Hancock, J.F. Krzyzaniak, P. Meenan, S. Nakhmanson, G.P.F. Wood, Assessment of machine learning approaches for predicting the crystallization propensity of active pharmaceutical ingredients. CrystEngComm 21 (8), 1215–1223 (2019). https://doi.org/10.1039/C8CE01589A Article CAS Google Scholar\\n- J.G.P. Wicker, R.I. Cooper, Will it crystallise? Predicting crystallinity of molecular materials. CrystEngComm 17 (9), 1927–1934 (2015). https://doi.org/10.1039/C4CE01912A Article CAS Google Scholar\\n- A. Alhalaweh, A. Alzghoul, W. Kaialy, D. Mahlin, C.A.S. Bergström, Computational predictions of glass-forming ability and crystallization tendency of drug molecules. Mol. Pharm. 11 (9), 3123–3132 (2014). https://doi.org/10.1021/mp500303a Article CAS Google Scholar\\n- J.G.P. Wicker, L.M. Crowley, O. Robshaw, E.J. Little, S.P. Stokes, R.I. Cooper, S.E. Lawrence, Will they co-crystallize? CrystEngComm 19 (36), 5336–5340 (2017). https://doi.org/10.1039/C7CE00587C Article CAS Google Scholar\\n- J.R. Gröls, B. Castro-Dominguez, Mechanochemical co-crystallization: insights and predictions. Comput. Chem. Eng. (2021). https://doi.org/10.1016/j.compchemeng.2021.107416 Article Google Scholar\\n- D.S. Palmer, N.M. O’Boyle, R.C. Glen, J.B.O. Mitchell, Random forest models to predict aqueous solubility. J. Chem. Inf. Model. 47 (1), 150–158 (2007). https://doi.org/10.1021/ci060164k Article CAS Google Scholar\\n- R.M. Bhardwaj, A. Johnston, B.F. Johnston, A.J. Florence, A random forest model for predicting the crystallisability of organic molecules. CrystEngComm 17 (23), 4272–4275 (2015). https://doi.org/10.1039/C4CE02403F Article CAS Google Scholar\\n- T. Barnard, H. Hagan, S. Tseng, G.C. Sosso, Less may be more: an informed reflection on molecular descriptors for drug design and discovery. Mol. Syst. Des. Eng. 5 (1), 317–329 (2020). https://doi.org/10.1039/C9ME00109C Article CAS Google Scholar\\n- H. Moriwaki, Y.-S. Tian, N. Kawashita, T. Takagi, Mordred: a molecular descriptor calculator. J. Cheminform. 10 (1), 4 (2018). https://doi.org/10.1186/s13321-018-0258-y Article CAS Google Scholar\\n- R. Gladysz, F.M.D. Santos, W. Langenaeker, G. Thijs, K. Augustyns, H.D. Winter, Spectrophores as one-dimensional descriptors calculated from three-dimensional atomic properties: applications ranging from scaffold hopping to multi-target virtual screening. J. Cheminform. 10 , 9 (2018). https://doi.org/10.1186/s13321-018-0268-9 Article CAS Google Scholar\\n- D. Rogers, M. Hahn, Extended-connectivity fingerprints. J. Chem. Inf. Model. 50 , 742–754 (2010). https://doi.org/10.1021/ci100050t Article CAS Google Scholar', '- D. Rogers, M. Hahn, Extended-connectivity fingerprints. J. Chem. Inf. Model. 50 , 742–754 (2010). https://doi.org/10.1021/ci100050t Article CAS Google Scholar\\n- S. Jaeger, S. Fulle, S. Turk, Mol2vec: unsupervised machine learning approach with chemical intuition. J. Chem. Inf. Model. 58 , 27–35 (2018). https://doi.org/10.1021/acs.jcim.7b00616 Article CAS Google Scholar\\n- A. Bender, How similar are those molecules after all? Use two descriptors and you will have three different answers. Expert Opin. Drug Discov. 5 (12), 1141–1151 (2010). https://doi.org/10.1517/17460441.2010.517832 Article CAS Google Scholar\\n- D. Weininger, SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules. J. Chem. Inf. Comput. Sci. 28 (1), 31–36 (1988) Article CAS Google Scholar\\n- N.M. O’Boyle, Towards a Universal SMILES representation—a standard method to generate canonical SMILES based on the InChI. J. Cheminform. 4 (1), 22 (2012). https://doi.org/10.1186/1758-2946-4-22 Article CAS Google Scholar\\n- E.J. Bjerrum, SMILES enumeration as data augmentation for neural network modeling of molecules. CoRR (2017). arXiv:1703.07076\\n- E. Torabian, M.A. Sobati, New structure-based models for the prediction of flash point of multi-component organic mixtures. Thermochim. Acta 672 , 162–172 (2019). https://doi.org/10.1016/j.tca.2018.11.012 Article CAS Google Scholar\\n- J. Howard, S. Gugger, Deep Learning for Coders with Fastai and PyTorch (O’Reilly Media, Sebastopol, 2020), pp. 36–39 Google Scholar\\n- O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, ImageNet large scale visual recognition challenge. Int. J. Comput. Vis. 115 (3), 211–252 (2015) Article Google Scholar\\n- T. Shi, Y. Yang, S. Huang, L. Chen, Z. Kuang, Y. Heng, H. Mei, Molecular image-based convolutional neural network for the prediction of ADMET properties. Chemometr. Intell. Lab. Syst. 194 , 103853 (2019). https://doi.org/10.1016/J.CHEMOLAB.2019.103853 Article CAS Google Scholar\\n- Y. Matsuzaka, Y. Uesawa, Optimization of a deep-learning method based on the classification of images generated by parameterized deep snap a novel molecular-image-input technique for quantitative structure-activity relationship (QSAR) analysis. Front. Bioeng. Biotechnol. 7 , 65 (2019) Article Google Scholar\\n- C. Shorten, T.M. Khoshgoftaar, A survey on image data augmentation for deep learning. J. Big Data 6 (1), 60 (2019). https://doi.org/10.1186/s40537-019-0197-0 Article Google Scholar\\n- M. Fernández-Delgado, E. Cernadas, S. Barro, D. Amorim, Do we need hundreds of classifiers to solve real world classification problems? J. Mach. Learn. Res. 15 , 3133–3181 (2014) Google Scholar\\n- S. Wang, C. Aggarwal, H. Liu, Random-forest-inspired neural networks. ACM Trans. Intell. Syst. Technol. (2018). https://doi.org/10.1145/3232230 Article Google Scholar\\n- S. Albawi, T.A. Mohammed, S. Al-Zawi, Understanding of a convolutional neural network, in 2017 International Conference on Engineering and Technology (ICET) (2017), pp. 1–6. https://doi.org/10.1109/ICEngTechnol.2017.8308186\\n- H.O. Brinkhaus, K. Rajan, A. Zielesny, C. Steinbeck, Randepict—random chemical structure depiction generator. ChemRxiv (2022). https://doi.org/10.26434/chemrxiv-2022-t1kbb Article Google Scholar\\n- M.C. Sorkun, A. Khetan, S. Er, AqSolDB, a curated reference set of aqueous solubility and 2D descriptors for a diverse set of compounds. Sci. Data 6 (1), 143 (2019). https://doi.org/10.1038/s41597-019-0151-1 Article CAS Google Scholar\\n- Q. Cui, S. Lu, B. Ni, X. Zeng, Y. Tan, Y.D. Chen, H. Zhao, Improved prediction of aqueous solubility of novel compounds by going deeper with deep learning. Front. Oncol. (2020). https://doi.org/10.3389/fonc.2020.00121 Article Google Scholar\\n- S. Kim, Exploring chemical information in PubChem. Curr. Protoc. (2021). https://doi.org/10.1002/cpz1.217 Article Google Scholar\\n- J.L. Durant, B.A. Leland, D.R. Henry, J.G. Nourse, Reoptimization of MDL keys for use in drug discovery. J. Chem. Inf. Comput. Sci. 42 , 1273–1280 (2002). https://doi.org/10.1021/ci010132r Article CAS Google Scholar\\n- B. Ramsundar, P. Eastman, P. Walters, V. Pande, K. Leswing, Z. Wu, textitDeep Learning for the Life Sciences (O’Reilly Media, Sebastopol, 2019). https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837\\n- K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2016), pp. 770–778\\nDownload references', \"## Acknowledgments\\n\\nThe authors would like to thank Dr Tom Fincham Haines and the Department of Computer Science at the University of Bath for their support in accessing the hardware resources needed for this work.\\nThe authors would like to acknowledge the PhD studentship funded by CMAC Future Manufacturing Research Hub and the Centre for Sustainable and Circular Technologies at the University of Bath.\\n\\n## Author information\\n\\nAuthor notes\\n- These authors contributed equally to this work.\\n\\n### Authors and Affiliations\\n\\n- Department of Chemical Engineering, University of Bath, Claverton Down, Bath, Somerset, BA2 7AY, UK Matthew R. Wilkinson\\xa0&\\xa0Bernardo Castro-Dominguez\\n- EPSRC Centre for Innovative Manufacturing in Continuous Manufacturing and Crystallisation (CMAC), University of Bath, Claverton Down, Bath, Somerset, BA2 7AY, UK Matthew R. Wilkinson,\\xa0Chick C. Wilson\\xa0&\\xa0Bernardo Castro-Dominguez\\n- Centre of Sustainable and Circular Technologies (CSCT), University of Bath, Claverton Down, Bath, Somerset, BA2 7AY, UK Matthew R. Wilkinson,\\xa0Chick C. Wilson\\xa0&\\xa0Bernardo Castro-Dominguez\\n- Centre for Autonomous Robotics (CENTAUR), University of Bath, Claverton Down, Bath, Somerset, BA2 7AY, UK Uriel Martinez-Hernandez\\n- Department of Chemistry, University of Bath, Claverton Down, Bath, Somerset, BA2 7AY, UK Chick C. Wilson\\n- Matthew R. Wilkinson View author publications Search author on: PubMed Google Scholar\\n- Uriel Martinez-Hernandez View author publications Search author on: PubMed Google Scholar\\n- Chick C. Wilson View author publications Search author on: PubMed Google Scholar\\n- Bernardo Castro-Dominguez View author publications Search author on: PubMed Google Scholar\\n\\n### Contributions\\n\\nMRW conceived of the idea, developed the code, ran the computations, and compiled the results. UMH assisted in developing the method of representing two component systems. MRW took the lead in preparing the manuscript with support from BCD, all other authors provided critical feedback leading to the final manuscript. BCD, UMH, and CCW supervised the project. BCD and CCW were responsible for the PhD funding to support MRW.\\n\\n### Corresponding author\\n\\nCorrespondence to Bernardo Castro-Dominguez .\\n\\n## Ethics declarations\\n\\n### Conflict of interest\\n\\nThe authors declare that there are no competing interests.\\n\\n## Rights and permissions\\n\\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ .\\nReprints and permissions\\n\\n## About this article\\n\\n### Cite this article\\n\\nWilkinson, M.R., Martinez-Hernandez, U., Wilson, C.C. et al. Images of chemical structures as molecular representations for deep learning. Journal of Materials Research 37 , 2293–2303 (2022). https://doi.org/10.1557/s43578-022-00628-9\\nDownload citation\\n- Received : 08 April 2022\\n- Accepted : 13 June 2022\\n- Published : 07 July 2022\\n- Issue Date : 28 July 2022\\n- DOI : https://doi.org/10.1557/s43578-022-00628-9\\n\\n### Share this article\\n\\nAnyone you share the following link with will be able to read this content:\\nGet shareable link\\nSorry, a shareable link is not currently available for this article.\\nCopy to clipboard\\nProvided by the Springer Nature SharedIt content-sharing initiative\\n- Molecular representation\\n- Deep learning\\n- Artificial Intelligence\\n- Pharmaceutical\\n- Industry 4.0\\n- Bernardo Castro-Dominguez View author profile\\nUse our pre-submission checklist\\nAvoid common mistakes on your manuscript.\\nAdvertisement\"]\n",
            "[{'source': 'https://portlandpress.com/biochemj/article/477/23/4559/227194/Deep-learning-and-generative-methods-in', 'type': 'html'}, {'type': 'html', 'source': 'https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/'}, {'source': 'https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/', 'type': 'html'}, {'source': 'https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/', 'type': 'html'}, {'source': 'https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/', 'type': 'html'}, {'pages': 0, 'producer': 'Skia/PDF m66', 'subject': '', 'trapped': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'moddate': '', 'type': 'pdf', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'creator': '', 'format': 'PDF 1.5', 'creationDate': '', 'title': '', 'page': 0, 'total_pages': 16, 'author': '', 'creationdate': '', 'modDate': '', 'keywords': ''}, {'subject': '', 'page': 1, 'author': '', 'producer': 'Skia/PDF m66', 'file_path': '/tmp/tmplgbr61h3.pdf', 'modDate': '', 'trapped': '', 'title': '', 'type': 'pdf', 'creator': '', 'creationDate': '', 'total_pages': 16, 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'moddate': '', 'format': 'PDF 1.5', 'pages': 1, 'creationdate': '', 'keywords': ''}, {'trapped': '', 'title': '', 'producer': 'Skia/PDF m66', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'creationDate': '', 'creationdate': '', 'modDate': '', 'subject': '', 'total_pages': 16, 'pages': 2, 'keywords': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'type': 'pdf', 'page': 2, 'creator': '', 'moddate': '', 'format': 'PDF 1.5', 'author': ''}, {'type': 'pdf', 'author': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'modDate': '', 'total_pages': 16, 'creator': '', 'moddate': '', 'subject': '', 'title': '', 'keywords': '', 'creationDate': '', 'format': 'PDF 1.5', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'pages': 3, 'page': 3, 'producer': 'Skia/PDF m66', 'creationdate': '', 'trapped': ''}, {'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'trapped': '', 'moddate': '', 'title': '', 'type': 'pdf', 'creationDate': '', 'subject': '', 'keywords': '', 'producer': 'Skia/PDF m66', 'author': '', 'format': 'PDF 1.5', 'pages': 4, 'modDate': '', 'page': 4, 'creationdate': '', 'creator': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'total_pages': 16}, {'creator': '', 'moddate': '', 'creationDate': '', 'creationdate': '', 'subject': '', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'keywords': '', 'type': 'pdf', 'format': 'PDF 1.5', 'title': '', 'modDate': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'author': '', 'trapped': '', 'pages': 5, 'producer': 'Skia/PDF m66', 'page': 5, 'total_pages': 16}, {'trapped': '', 'modDate': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'creator': '', 'type': 'pdf', 'producer': 'Skia/PDF m66', 'moddate': '', 'keywords': '', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'title': '', 'creationdate': '', 'author': '', 'subject': '', 'page': 6, 'format': 'PDF 1.5', 'pages': 6, 'creationDate': '', 'total_pages': 16}, {'moddate': '', 'pages': 7, 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'file_path': '/tmp/tmplgbr61h3.pdf', 'creationDate': '', 'subject': '', 'type': 'pdf', 'page': 7, 'trapped': '', 'keywords': '', 'author': '', 'total_pages': 16, 'format': 'PDF 1.5', 'creator': '', 'modDate': '', 'creationdate': '', 'producer': 'Skia/PDF m66', 'title': ''}, {'trapped': '', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'type': 'pdf', 'pages': 8, 'creationdate': '', 'total_pages': 16, 'moddate': '', 'modDate': '', 'author': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'format': 'PDF 1.5', 'creationDate': '', 'title': '', 'keywords': '', 'subject': '', 'creator': '', 'producer': 'Skia/PDF m66', 'page': 8}, {'creator': '', 'creationDate': '', 'subject': '', 'format': 'PDF 1.5', 'pages': 9, 'title': '', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'moddate': '', 'keywords': '', 'trapped': '', 'author': '', 'total_pages': 16, 'type': 'pdf', 'file_path': '/tmp/tmplgbr61h3.pdf', 'creationdate': '', 'modDate': '', 'producer': 'Skia/PDF m66', 'page': 9}, {'pages': 10, 'format': 'PDF 1.5', 'keywords': '', 'total_pages': 16, 'file_path': '/tmp/tmplgbr61h3.pdf', 'creationdate': '', 'creationDate': '', 'creator': '', 'trapped': '', 'author': '', 'type': 'pdf', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'title': '', 'producer': 'Skia/PDF m66', 'subject': '', 'modDate': '', 'moddate': '', 'page': 10}, {'page': 11, 'title': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'total_pages': 16, 'pages': 11, 'subject': '', 'format': 'PDF 1.5', 'creationDate': '', 'keywords': '', 'author': '', 'trapped': '', 'creator': '', 'modDate': '', 'producer': 'Skia/PDF m66', 'creationdate': '', 'type': 'pdf', 'moddate': '', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf'}, {'pages': 12, 'modDate': '', 'moddate': '', 'subject': '', 'title': '', 'author': '', 'page': 12, 'total_pages': 16, 'type': 'pdf', 'format': 'PDF 1.5', 'file_path': '/tmp/tmplgbr61h3.pdf', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'creator': '', 'producer': 'Skia/PDF m66', 'creationDate': '', 'creationdate': '', 'keywords': '', 'trapped': ''}, {'pages': 13, 'trapped': '', 'author': '', 'title': '', 'keywords': '', 'total_pages': 16, 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'subject': '', 'type': 'pdf', 'creator': '', 'producer': 'Skia/PDF m66', 'page': 13, 'modDate': '', 'creationDate': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'creationdate': '', 'moddate': '', 'format': 'PDF 1.5'}, {'moddate': '', 'modDate': '', 'type': 'pdf', 'pages': 14, 'creationdate': '', 'title': '', 'total_pages': 16, 'trapped': '', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'format': 'PDF 1.5', 'page': 14, 'creator': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'producer': 'Skia/PDF m66', 'creationDate': '', 'subject': '', 'author': '', 'keywords': ''}, {'subject': '', 'keywords': '', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'producer': 'Skia/PDF m66', 'type': 'pdf', 'creator': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'modDate': '', 'title': '', 'page': 15, 'total_pages': 16, 'creationdate': '', 'creationDate': '', 'author': '', 'pages': 15, 'moddate': '', 'format': 'PDF 1.5', 'trapped': ''}, {'type': 'pdf', 'creator': 'LaTeX with hyperref package', 'moddate': '2018-12-03T01:37:12+00:00', 'format': 'PDF 1.5', 'pages': 0, 'page': 0, 'total_pages': 13, 'modDate': 'D:20181203013712Z', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'producer': 'pdfTeX-1.40.17', 'title': '', 'keywords': '', 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'trapped': '', 'subject': '', 'author': '', 'creationDate': 'D:20181203013712Z', 'creationdate': '2018-12-03T01:37:12+00:00'}, {'keywords': '', 'format': 'PDF 1.5', 'creationDate': 'D:20181203013712Z', 'author': '', 'creationdate': '2018-12-03T01:37:12+00:00', 'creator': 'LaTeX with hyperref package', 'modDate': 'D:20181203013712Z', 'page': 1, 'title': '', 'moddate': '2018-12-03T01:37:12+00:00', 'total_pages': 13, 'producer': 'pdfTeX-1.40.17', 'subject': '', 'trapped': '', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'type': 'pdf', 'pages': 1, 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf'}, {'keywords': '', 'title': '', 'total_pages': 13, 'file_path': '/tmp/tmpd75a7p3t.pdf', 'moddate': '2018-12-03T01:37:12+00:00', 'creationdate': '2018-12-03T01:37:12+00:00', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20181203013712Z', 'type': 'pdf', 'creator': 'LaTeX with hyperref package', 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'format': 'PDF 1.5', 'subject': '', 'page': 2, 'trapped': '', 'author': '', 'pages': 2, 'modDate': 'D:20181203013712Z'}, {'keywords': '', 'modDate': 'D:20181203013712Z', 'creationDate': 'D:20181203013712Z', 'creationdate': '2018-12-03T01:37:12+00:00', 'producer': 'pdfTeX-1.40.17', 'type': 'pdf', 'trapped': '', 'author': '', 'title': '', 'format': 'PDF 1.5', 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'moddate': '2018-12-03T01:37:12+00:00', 'subject': '', 'total_pages': 13, 'creator': 'LaTeX with hyperref package', 'pages': 3, 'file_path': '/tmp/tmpd75a7p3t.pdf', 'page': 3}, {'keywords': '', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'creationdate': '2018-12-03T01:37:12+00:00', 'page': 4, 'pages': 4, 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'creator': 'LaTeX with hyperref package', 'format': 'PDF 1.5', 'total_pages': 13, 'title': '', 'author': '', 'modDate': 'D:20181203013712Z', 'trapped': '', 'subject': '', 'type': 'pdf', 'moddate': '2018-12-03T01:37:12+00:00', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20181203013712Z'}, {'creationDate': 'D:20181203013712Z', 'page': 5, 'modDate': 'D:20181203013712Z', 'creator': 'LaTeX with hyperref package', 'trapped': '', 'type': 'pdf', 'creationdate': '2018-12-03T01:37:12+00:00', 'author': '', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'format': 'PDF 1.5', 'moddate': '2018-12-03T01:37:12+00:00', 'title': '', 'subject': '', 'pages': 5, 'total_pages': 13, 'producer': 'pdfTeX-1.40.17', 'keywords': ''}, {'moddate': '2018-12-03T01:37:12+00:00', 'creationdate': '2018-12-03T01:37:12+00:00', 'producer': 'pdfTeX-1.40.17', 'author': '', 'subject': '', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'pages': 6, 'format': 'PDF 1.5', 'type': 'pdf', 'total_pages': 13, 'title': '', 'creator': 'LaTeX with hyperref package', 'keywords': '', 'modDate': 'D:20181203013712Z', 'trapped': '', 'page': 6, 'creationDate': 'D:20181203013712Z', 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf'}, {'trapped': '', 'modDate': 'D:20181203013712Z', 'moddate': '2018-12-03T01:37:12+00:00', 'creationDate': 'D:20181203013712Z', 'pages': 7, 'file_path': '/tmp/tmpd75a7p3t.pdf', 'keywords': '', 'total_pages': 13, 'page': 7, 'subject': '', 'format': 'PDF 1.5', 'producer': 'pdfTeX-1.40.17', 'title': '', 'type': 'pdf', 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'author': '', 'creator': 'LaTeX with hyperref package', 'creationdate': '2018-12-03T01:37:12+00:00'}, {'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'creator': 'LaTeX with hyperref package', 'creationDate': 'D:20181203013712Z', 'total_pages': 13, 'author': '', 'type': 'pdf', 'modDate': 'D:20181203013712Z', 'creationdate': '2018-12-03T01:37:12+00:00', 'keywords': '', 'page': 8, 'producer': 'pdfTeX-1.40.17', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'trapped': '', 'moddate': '2018-12-03T01:37:12+00:00', 'title': '', 'format': 'PDF 1.5', 'subject': '', 'pages': 8}, {'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'modDate': 'D:20181203013712Z', 'pages': 9, 'author': '', 'creationDate': 'D:20181203013712Z', 'subject': '', 'title': '', 'moddate': '2018-12-03T01:37:12+00:00', 'type': 'pdf', 'creator': 'LaTeX with hyperref package', 'producer': 'pdfTeX-1.40.17', 'creationdate': '2018-12-03T01:37:12+00:00', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'page': 9, 'keywords': '', 'total_pages': 13, 'format': 'PDF 1.5', 'trapped': ''}, {'title': '', 'keywords': '', 'pages': 10, 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'producer': 'pdfTeX-1.40.17', 'modDate': 'D:20181203013712Z', 'creationdate': '2018-12-03T01:37:12+00:00', 'moddate': '2018-12-03T01:37:12+00:00', 'type': 'pdf', 'creationDate': 'D:20181203013712Z', 'subject': '', 'format': 'PDF 1.5', 'author': '', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'total_pages': 13, 'page': 10, 'trapped': '', 'creator': 'LaTeX with hyperref package'}, {'file_path': '/tmp/tmpd75a7p3t.pdf', 'format': 'PDF 1.5', 'modDate': 'D:20181203013712Z', 'producer': 'pdfTeX-1.40.17', 'creationDate': 'D:20181203013712Z', 'subject': '', 'moddate': '2018-12-03T01:37:12+00:00', 'page': 11, 'creationdate': '2018-12-03T01:37:12+00:00', 'total_pages': 13, 'creator': 'LaTeX with hyperref package', 'title': '', 'trapped': '', 'type': 'pdf', 'keywords': '', 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'author': '', 'pages': 11}, {'author': '', 'producer': 'pdfTeX-1.40.17', 'total_pages': 13, 'moddate': '2018-12-03T01:37:12+00:00', 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'creationdate': '2018-12-03T01:37:12+00:00', 'type': 'pdf', 'modDate': 'D:20181203013712Z', 'keywords': '', 'creationDate': 'D:20181203013712Z', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'page': 12, 'title': '', 'creator': 'LaTeX with hyperref package', 'subject': '', 'pages': 12, 'format': 'PDF 1.5', 'trapped': ''}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41467-022-28494-3'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41467-022-28494-3'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41467-022-28494-3'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41467-022-28494-3'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41467-022-28494-3', 'type': 'html'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41467-022-28494-3'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4', 'type': 'html'}, {'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4', 'type': 'html'}, {'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4', 'type': 'html'}, {'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, {'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4', 'type': 'html'}, {'type': 'html', 'source': 'https://www.mdpi.com/journal/molecules/special_issues/deep_learning_structure'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'type': 'html', 'source': 'https://www.mdpi.com/1420-3049/25/12/2764'}, {'type': 'html', 'source': 'https://www.mdpi.com/1420-3049/25/12/2764'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'type': 'html', 'source': 'https://www.mdpi.com/1420-3049/25/12/2764'}, {'type': 'html', 'source': 'https://www.mdpi.com/1420-3049/25/12/2764'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'type': 'html', 'source': 'https://www.mdpi.com/1420-3049/25/12/2764'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'type': 'html', 'source': 'https://www.mdpi.com/1420-3049/25/12/2764'}, {'type': 'html', 'source': 'https://www.mdpi.com/1420-3049/25/12/2764'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'source': 'https://www.mdpi.com/1420-3049/25/12/2764', 'type': 'html'}, {'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6', 'type': 'html'}, {'type': 'html', 'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6'}, {'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6', 'type': 'html'}, {'type': 'html', 'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6'}, {'type': 'html', 'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6'}, {'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6', 'type': 'html'}, {'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6', 'type': 'html'}, {'type': 'html', 'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6'}, {'type': 'html', 'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6'}, {'type': 'html', 'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6'}, {'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6', 'type': 'html'}, {'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6', 'type': 'html'}, {'source': 'https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6', 'type': 'html'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41598-025-95720-5'}, {'source': 'https://www.nature.com/articles/s41598-025-95720-5', 'type': 'html'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41598-025-95720-5'}, {'source': 'https://www.nature.com/articles/s41598-025-95720-5', 'type': 'html'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41598-025-95720-5'}, {'source': 'https://www.nature.com/articles/s41598-025-95720-5', 'type': 'html'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41598-025-95720-5'}, {'source': 'https://www.nature.com/articles/s41598-025-95720-5', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41598-025-95720-5', 'type': 'html'}, {'source': 'https://www.nature.com/articles/s41598-025-95720-5', 'type': 'html'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41598-025-95720-5'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41598-025-95720-5'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41598-025-95720-5'}, {'source': 'https://www.nature.com/articles/s41598-025-95720-5', 'type': 'html'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41598-025-95720-5'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41598-025-95720-5'}, {'source': 'https://www.nature.com/articles/s41598-025-95720-5', 'type': 'html'}, {'type': 'html', 'source': 'https://www.nature.com/articles/s41598-025-95720-5'}, {'source': 'https://www.nature.com/articles/s41598-025-95720-5', 'type': 'html'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'type': 'html', 'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/'}, {'source': 'https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9'}, {'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9'}, {'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9', 'type': 'html'}, {'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9', 'type': 'html'}, {'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9'}, {'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9'}, {'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9', 'type': 'html'}, {'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9'}, {'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9', 'type': 'html'}, {'type': 'html', 'source': 'https://link.springer.com/article/10.1557/s43578-022-00628-9'}]\n",
            "[[ 0.05092553 -0.05014317 -0.07179789 ...  0.03706881 -0.01070079\n",
            "   0.00379026]\n",
            " [-0.02215424 -0.01456097 -0.02037292 ...  0.03246162 -0.00664896\n",
            "   0.02265034]\n",
            " [ 0.0002654  -0.0439369  -0.00585382 ...  0.01355444 -0.01761536\n",
            "   0.03836043]\n",
            " ...\n",
            " [-0.01172701 -0.03025642 -0.04975166 ...  0.01744015 -0.05331216\n",
            "   0.01008309]\n",
            " [-0.00092603  0.00089994 -0.06485261 ...  0.00548394 -0.05393237\n",
            "  -0.00344392]\n",
            " [-0.009017   -0.04945246 -0.03225749 ... -0.01162416 -0.04294282\n",
            "   0.00555723]]\n"
          ]
        }
      ],
      "source": [
        "collection_data = vectorstore._collection.get(include=[\"embeddings\",\"documents\",'metadatas'])\n",
        "print(collection_data.get('ids', []),)\n",
        "print(collection_data.get('documents', []))\n",
        "print(collection_data.get('metadatas', []))\n",
        "print(collection_data.get('embeddings', []))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df",
        "outputId": "5b89a295-31e3-4a0a-c4cd-33e6e1ea3a7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_variables=['context', 'question'] input_types={} partial_variables={} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context from research papes to answer the question in detail minimum 500 words. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\"), additional_kwargs={})]\n",
            "SMILES, which stands for Simplified Molecular-Input Line-Entry System, is a line notation used to represent chemical structures. It encodes the connection table and stereochemistry of a molecule as a line of text using short ASCII strings. SMILES utilizes a grammar structure where alphabets denote atoms, special characters indicate bond types, encapsulated numbers represent rings, and parentheses represent side chains.\n",
            "\n",
            "SMILES is a popular specification for describing chemical structures in a linear format. It is used as a sequence input in deep learning models like CheMixNet for predicting chemical properties. CheMixNet leverages SMILES notations along with molecular fingerprints to improve upon existing approaches for learning from vector or text representations of molecules. The model generates SMILES character by character from left to right without using an external dictionary for chemical abbreviations, learning these abbreviations as part of the model.\n",
            "\n",
            "In SMILES, organic molecules attach to form long continuous chains known as branches. The complete encoder-decoder framework used for SMILES generation is fully differentiable and trained end-to-end using backpropagation. During decoding, SMILES are generated one character at a time, and an attention mechanism helps the decoder access information produced earlier in the encoder, minimizing the loss of important details. The attention output can also be used for repositioning a predicted structure into an orientation that better matches the original input image.\n"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
        "from langsmith import Client\n",
        "client = Client(api_key=LANGCHAIN_API_KEY)\n",
        "# prompt_object = client.pull_prompt(\"chatbot\", include_model=True)\n",
        "\n",
        "# Define the prompt template using input variables\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context from research papes to answer the question in detail minimum 500 words. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\"\"\")\n",
        "\n",
        "print(prompt)\n",
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)\n",
        "\n",
        "# Post-processing\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Chain\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Question\n",
        "print(rag_chain.invoke(\"What is SMILES tell everything about it in detail\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e8e856-bafd-469e-b99a-11596b18aad4",
      "metadata": {
        "id": "18e8e856-bafd-469e-b99a-11596b18aad4"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "edd7beeb-21fa-4f4b-b8fa-5a4f70489a16",
      "metadata": {
        "id": "edd7beeb-21fa-4f4b-b8fa-5a4f70489a16"
      },
      "outputs": [],
      "source": [
        "# Documents\n",
        "question = \"What kinds of pets do I like?\"\n",
        "document = \"My favorite pet is a cat.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0552ea4-935d-4dfa-bd2b-56d148e96304",
      "metadata": {
        "id": "e0552ea4-935d-4dfa-bd2b-56d148e96304"
      },
      "source": [
        "[Count tokens](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb) considering [~4 char / token](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "df119cca-1676-4caa-bad4-11805d69e616",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df119cca-1676-4caa-bad4-11805d69e616",
        "outputId": "08f39b94-7539-4d31-9527-e250c83d7fbf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "num_tokens_from_string(question, \"cl100k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f04fd74-829f-472c-a1bc-ec6521a0529f",
      "metadata": {
        "id": "4f04fd74-829f-472c-a1bc-ec6521a0529f"
      },
      "source": [
        "[Text embedding models](https://python.langchain.com/docs/integrations/text_embedding/openai)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "6bd98786-755d-4d49-ba97-30c5a623b74e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bd98786-755d-4d49-ba97-30c5a623b74e",
        "outputId": "d7130914-6424-4854-9c7b-c90545cc1c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.006222310476005077, -0.006061272229999304, -0.026451561599969864, -0.020352467894554138, 0.0056349425576627254, -0.008037385530769825, 0.028191709890961647, -0.010441510006785393, 0.027259130030870438, 0.013420348055660725, 0.06445972621440887, -0.016194358468055725, 0.025595178827643394, 0.016282696276903152, 0.0048562223091721535, -0.031569819897413254, 0.005982452072203159, -0.00033394136698916554, 0.0012122254120185971, -0.04223364591598511, 0.009387334808707237, -0.002535228617489338, -0.019733130931854248, -0.005538017023354769, 0.04108477756381035, -0.06169067695736885, 0.04840322211384773, -0.029915019869804382, 0.0035526345018297434, 0.04842689633369446, -0.06559431552886963, 0.05941709503531456, -0.07555248588323593, -0.0007875484297983348, -0.023788658902049065, -0.04315219447016716, -0.032032158225774765, -0.019618835300207138, -0.014419357292354107, 0.05566913262009621, 0.01879522204399109, 0.002243547234684229, -0.05868314951658249, -0.051063422113657, 0.022266339510679245, -0.012864183634519577, -0.022421887144446373, 0.06182055175304413, -0.02373102679848671, -0.06208764761686325, -0.016852153465151787, 0.003738647559657693, 0.050278812646865845, -0.04256988316774368, -0.023102061823010445, -0.022187812253832817, 0.034776706248521805, 0.024805039167404175, -0.05842166021466255, -0.008013466373085976, 0.04033150523900986, -0.001955340150743723, -0.03222067654132843, 0.049259766936302185, 0.015838002786040306, 0.010767270810902119, -0.08347824215888977, 0.032192423939704895, 0.046924952417612076, -0.03559058904647827, 0.05548948049545288, -0.02429872378706932, 0.01198603305965662, -0.016376353800296783, 0.0013767048949375749, -0.012627207674086094, -0.039111118763685226, 0.07820290327072144, 0.008970683440566063, -0.03766179457306862, 0.01895843632519245, -0.05883047729730606, -0.02358003333210945, -0.009172654710710049, -0.054935213178396225, 0.053952157497406006, -0.05240539461374283, 0.0026102664414793253, 0.0063984934240579605, 0.059731047600507736, -0.04408351704478264, 0.029073666781187057, -0.016399770975112915, -0.07623711228370667, 0.009799780324101448, 0.007496033329516649, -0.009890802204608917, -0.006045255344361067, -0.02850692719221115, -0.0428740456700325, -0.004080966580659151, -0.036088354885578156, -0.0527612529695034, 0.021450763568282127, 0.030037250369787216, 0.01786760985851288, 0.015905577689409256, 0.034350376576185226, -0.01812109723687172, 0.02363850548863411, -0.05234648287296295, -0.014596424996852875, 0.01265321858227253, -0.059035394340753555, 0.018571363762021065, -0.023240476846694946, -0.036214083433151245, 0.025008603930473328, 0.0030875015072524548, 0.004920251667499542, -0.022183984518051147, -0.013271444477140903, 0.09297160059213638, -0.03406680002808571, -0.016684599220752716, 0.008161990903317928, -0.011368989944458008, -0.003504280699416995, 0.014322439208626747, 0.0538010448217392, -0.024336237460374832, -0.027956442907452583, -0.01984374038875103, 0.014147549867630005, 0.056704774498939514, 0.06660541892051697, 0.035353709012269974, 0.038736287504434586, 0.04888185113668442, 0.00838612113147974, -0.004395216703414917, 0.03520061448216438, -0.009493265300989151, -0.011935634538531303, -0.006906851194798946, 0.04367882385849953, -0.03165670111775398, -0.07639797776937485, 0.029765918850898743, -0.020529726520180702, -0.009268160909414291, -0.014753740280866623, -0.041690628975629807, 0.02674628421664238, 0.08705016225576401, 0.048058804124593735, 0.007582631893455982, 0.01816888153553009, 0.031955286860466, 0.00041392940329387784, 0.06307274848222733, 0.04522327706217766, 0.017325609922409058, 0.008565298281610012, -0.008582072332501411, -0.013929125852882862, -0.02154286950826645, 0.0069761332124471664, -0.009223918430507183, -0.013340090401470661, -0.04090652987360954, 0.007913832552731037, -0.026527568697929382, -0.033383894711732864, -0.024662230163812637, -0.0788801833987236, 0.012774978764355183, -0.0016996216727420688, -0.013259803876280785, -0.022261817008256912, -0.002170777413994074, -0.055035147815942764, -0.01793244108557701, 0.03790370747447014, 0.005097468849271536, -0.01058557815849781, 0.1172015517950058, 0.012220829725265503, -0.0017323552165180445, 0.021787727251648903, 0.021449929103255272, -0.024996211752295494, -0.0062807295471429825, -0.003195086494088173, -0.028913464397192, 0.010833497159183025, -0.009153824299573898, 0.006836880464106798, 0.0506306029856205, -0.028558949008584023, -0.02062748372554779, 0.09718089550733566, 0.020679784938693047, -0.0018793073249980807, 0.0024828596506267786, 0.010115155950188637, 0.037040505558252335, -0.012137283571064472, -0.006627485156059265, 0.07121410220861435, -0.022658921778202057, -0.014101369306445122, -0.0264345183968544, 0.016777360811829567, 0.00962794292718172, 0.04009684547781944, 0.06611631065607071, 0.03699716925621033, 0.028152503073215485, -0.009452222846448421, 0.01136341318488121, -0.0094816405326128, -0.007854917086660862, 0.026372084394097328, -0.007117667235434055, -0.004156750626862049, -0.002880386309698224, -0.00557001493871212, -0.014998636208474636, -0.06312571465969086, -0.0017526198644191027, 0.08024539053440094, 0.0019628778100013733, -0.03730218857526779, 0.04536611586809158, 0.0168409813195467, -0.017978807911276817, 0.0017182460287585855, 0.03215448930859566, 0.04417799785733223, -0.061807651072740555, 0.007454653736203909, 0.0949333906173706, 0.049554355442523956, -0.025820989161729813, -0.03422542288899422, 0.009838230907917023, 0.035022951662540436, -0.015508243814110756, 0.05620777979493141, 0.011158211156725883, -0.0617312528192997, -0.0049789054319262505, 0.017650291323661804, -0.09215918928384781, 0.016243530437350273, -0.036797039210796356, 0.025979379191994667, -0.02499113418161869, 0.04356689751148224, 0.060674820095300674, -0.01125197671353817, 0.03261711820960045, 0.005400656722486019, -0.020767496898770332, 0.009640897624194622, -0.004393969662487507, -0.060756441205739975, 0.033224258571863174, 0.020760992541909218, 0.021893706172704697, -0.04034193605184555, 0.0453353188931942, 0.0032870396971702576, 0.014589757658541203, 0.02079039253294468, -0.009038198739290237, 0.008882427588105202, 0.02293095365166664, -0.05524638295173645, -0.004743624944239855, 0.022031128406524658, 0.04962436109781265, -0.07838759571313858, 0.005264940205961466, -0.0038582179695367813, -0.06632698327302933, -0.005239606834948063, 0.01688847877085209, -0.017003891989588737, 0.007343181874603033, -0.01591329090297222, -0.03413620591163635, -0.02517874352633953, -0.045086994767189026, -0.023525595664978027, -0.014312485232949257, 0.03576505184173584, 0.026165293529629707, -0.008626342751085758, 0.0034218106884509325, 0.013346115127205849, -0.017145894467830658, -0.0832761749625206, -0.02690863236784935, 0.007166056893765926, -0.01800401322543621, -0.07352308183908463, -0.030146488919854164, 0.05765384063124657, -0.043825991451740265, -0.013815945014357567, -0.019679615274071693, -0.006803792901337147, 0.03382819518446922, 0.02548799477517605, -0.02237756736576557, -0.0012384685687720776, -0.04824494197964668, 0.03150542452931404, -0.012546392157673836, 0.05707462504506111, -0.031844623386859894, -0.031232377514243126, -0.029145359992980957, -0.022220730781555176, 0.010013540275394917, 0.0880306288599968, 0.03828343749046326, -0.03909068927168846, -0.029900480061769485, 0.008164994418621063, -0.07986867427825928, 0.01667923294007778, -0.007998897694051266, 0.028702260926365852, -0.07239975780248642, 0.06913494318723679, -0.06683997809886932, 0.054620448499917984, 0.08031804114580154, -0.006420033052563667, 0.03177199512720108, -0.03995697572827339, 0.010199376381933689, -0.01985473372042179, -0.008054823614656925, 0.018221329897642136, 0.06913547217845917, -0.01836201921105385, -0.011506939306855202, 0.0799444168806076, 0.0071341204456985, -0.0027047060430049896, -0.025045007467269897, -0.07816136628389359, -0.011572420597076416, 0.01011291891336441, 0.06198987364768982, -0.02619713358581066, -0.03872048482298851, 0.05082957446575165, -0.030150186270475388, 0.02136915735900402, -0.08276428282260895, 0.02205660752952099, 0.01609732210636139, -0.02525123581290245, 0.010621946305036545, 0.024748075753450394, 0.007750580087304115, -0.03866662085056305, 0.025272179394960403, 0.00799619685858488, -0.022710222750902176, -0.06430424749851227, -0.05956410616636276, -0.003157079918310046, 0.008517741225659847, 0.004720916971564293, 0.0013632802292704582, -0.03453361243009567, 0.06115865707397461, 0.05254590883851051, 0.021484987810254097, -0.021190987899899483, 0.054971449077129364, 0.0023969297762960196, -0.014507941901683807, 0.04610981047153473, 0.06678164750337601, 0.018558833748102188, 0.10207806527614594, 0.06751010566949844, -0.02613859437406063, -0.00978553295135498, 0.0218732301145792, -0.05150146409869194, -0.003227489534765482, -0.008518537506461143, 0.032509639859199524, -0.03365805745124817, -0.023541413247585297, -0.012840299867093563, 0.0031310180202126503, -0.018941566348075867, -0.019284185022115707, -0.04839293286204338, 0.02901245653629303, 0.02481534704566002, 0.04845448583364487, -0.005530698224902153, 0.0257271695882082, -0.040302712470293045, -0.038247473537921906, -0.008933342061936855, 0.03836940973997116, 0.0021311885211616755, 0.001238861819729209, 0.03263643756508827, 0.01910492032766342, 0.011987969279289246, -0.0072154696099460125, -0.011135516688227654, -0.013482240960001945, -0.09619176387786865, -0.00284109846688807, -0.031617358326911926, -0.007321139331907034, 0.04293518513441086, 0.03888135030865669, 0.0074499561451375484, 0.0453876331448555, -0.03224983438849449, -0.0036641890183091164, -0.0532708466053009, 0.038342952728271484, 0.010178589262068272, -0.030120987445116043, 0.0007784761837683618, -0.02136644721031189, -0.03135555237531662, 0.08762139081954956, -0.006176386959850788, -0.06497150659561157, -0.055412355810403824, -0.015292159281671047, -0.01894320361316204, 0.060275353491306305, -0.12686148285865784, 0.03842536360025406, -0.08040332049131393, 0.032158687710762024, -0.0853026732802391, -0.03741225600242615, -0.0045944154262542725, 0.026269273832440376, 0.07520689070224762, -0.034906283020973206, 0.012010338716208935, -0.012123403139412403, -0.02453664131462574, -0.017574802041053772, -0.10276437550783157, 0.019098637625575066, -0.0028959326446056366, 0.050150930881500244, 0.03936423361301422, 0.04118887335062027, 0.0312623456120491, 0.016740063205361366, -0.033514127135276794, 0.00039950141217559576, -0.00017502308764960617, -0.013173354789614677, 0.01108616590499878, -0.026806635782122612, 0.05038892477750778, 0.0043753948993980885, -0.007672145497053862, -0.01787707954645157, 0.010911660268902779, 0.04259378835558891, 0.013805950060486794, -0.000926888023968786, 0.03156478703022003, -0.010552387684583664, -0.01848777011036873, -0.037695787847042084, 0.04864494875073433, -0.009853307157754898, -0.01900371164083481, 0.026866190135478973, -0.08510293811559677, -0.020982330664992332, -0.0010560571681708097, 0.03521820530295372, 0.02425103262066841, 0.04154380038380623, 0.015209326520562172, -0.023058230057358742, -0.02073162980377674, -0.00881422683596611, -0.03437693789601326, 0.04885832592844963, -0.03684130311012268, 0.041908830404281616, 0.03726545348763466, -0.031555142253637314, -0.025085479021072388, -0.025665991008281708, 0.02009359560906887, 0.005692091770470142, 0.0015471898950636387, 0.023352358490228653, -0.010443704202771187, -0.0007931442814879119, 0.0389215461909771, 0.013341039419174194, 0.00792199932038784, 0.08238022774457932, -0.010153889656066895, -0.08826595544815063, 0.003708837553858757, -0.00790178868919611, -0.04541262984275818, 0.034236326813697815, 0.007414258550852537, -0.04952846094965935, 0.03958142548799515, 0.0012699613580480218, 0.07592562586069107, -0.03631739318370819, -0.007693612482398748, 0.022386768832802773, -0.031089749187231064, 0.006291577126830816, 0.03324173390865326, 0.013601651415228844, -0.043673060834407806, 0.014343158341944218, -0.031789641827344894, -0.016721952706575394, -0.001181011670269072, 0.0014951539924368262, 0.02646714821457863, 0.03900761902332306, -0.13507145643234253, -2.1695657778764144e-05, 0.016054658219218254, -0.04502986744046211, -0.021668104454874992, 0.028080036863684654, -0.03338281065225601, 0.025173330679535866, -0.03887387365102768, -0.013341051526367664, 0.045678529888391495, -0.015623877756297588, -0.00323879555799067, -0.006672833114862442, -0.05083855241537094, 0.040168989449739456, 0.02950318716466427, 0.020274553447961807, 0.06246742978692055, -0.01947663724422455, -0.006250296253710985, 0.033749498426914215, -0.019073044881224632, -0.0012707780115306377, 0.05592648312449455, 0.017843112349510193, 0.03027539886534214, -0.010540924035012722, -0.005811755545437336, -0.020448779687285423, -0.018625767901539803, 0.01837206445634365, 0.002297384897246957, -0.008037511259317398, 0.038389649242162704, -0.023721015080809593, 0.03076195903122425, 0.03552326560020447, 0.023314451798796654, -0.02375273033976555, 0.02098756842315197, 0.027866147458553314, 0.022313034161925316, -0.0298688355833292, -0.015213570557534695, 0.0008680954342707992, -0.013819228857755661, -0.0043741436675190926, 0.03083951584994793, -0.0033141898456960917, 0.04076112434267998, -0.04712656885385513, -0.013860278762876987, 0.03981253132224083, -0.013642427511513233, 0.05852948874235153, -0.03348103165626526, -9.035384573508054e-05, 0.0424821637570858, 0.0040221065282821655, -0.011862930841743946, -0.0358462892472744, -0.00833798572421074, 0.014579486101865768, -0.02034306339919567, 0.04573488608002663, -0.048322319984436035, -0.039896924048662186, 0.02610732987523079, 0.053058914840221405, -0.005257606040686369, -0.01872539520263672, 0.0030635499861091375, -0.016886241734027863, -0.04179335758090019, -0.006699382793158293, 0.03158465400338173, 0.0653468519449234, -0.008893278427422047, -0.05161214992403984, -0.04014836251735687, 0.03632934018969536, 0.021560199558734894, -0.0030419768299907446, 0.04321299493312836, 0.02225159853696823, 0.002754639368504286, -0.04505162686109543, 0.06500685960054398, 0.01167864166200161, -0.034678924828767776, 0.04138224944472313, -0.01842549629509449, -0.0708552896976471, 0.008461566641926765, 0.05401470139622688, -0.019911985844373703, -0.010294688865542412, 0.09427517652511597, 0.02156023308634758, -0.07226941734552383, -0.055048197507858276, 0.0051818047650158405, -0.04928750917315483, 0.011914144270122051, 0.04594628885388374, 0.0030446224845945835, -0.03713378682732582, 0.030263403430581093, 0.009819657541811466, -0.04331134259700775, 0.00227575795724988, -0.018270086497068405, -0.06504759192466736, 0.021116551011800766, 0.00029753701528534293, 0.01646040566265583, -0.011993364430963993, -0.033073969185352325, -0.03747263923287392, -0.0426548533141613, 0.0016005506040528417, 0.0736650824546814, -0.04177403450012207, 0.04702145978808403, 0.03837857022881508, -0.05159685015678406, 0.010306221432983875, -0.0007469553966075182, 0.018175018951296806, 0.04068892449140549, -0.019249115139245987, 0.012687825597822666, -0.05245411768555641, -0.009167675860226154, -0.023831289261579514, 0.01964111253619194, 0.007377552799880505, 0.014204662293195724, 0.04547350853681564, 0.005664472468197346, 0.006113715935498476, -0.06038382649421692, 0.028820138424634933, -0.03317975625395775, -0.04722796380519867, 0.0030220444314181805, 0.019605185836553574, 0.047453586012125015, -0.03490660339593887, -0.008620934560894966, 0.0012332773767411709, 0.022557426244020462, -0.009802593849599361, -0.014318830333650112, -0.0054045068100094795, 0.020086942240595818, 0.03582363948225975, 0.011382166296243668, -0.009027134627103806, -0.01102243922650814, 0.023939859122037888, -0.006164007354527712, 0.0033958274871110916, -0.006284547038376331, -0.05482916533946991, 0.033326078206300735, 0.008532715030014515, -0.07069732248783112, 0.02062596008181572, 0.009252901189029217, -0.05240609496831894, 0.03851873800158501, 0.07828879356384277, 0.013988192193210125, 0.040955785661935806, -0.09619496762752533, -0.09545956552028656, 0.007432819809764624, -0.06277358531951904, -0.006722668185830116, -0.004455231595784426, -0.029110366478562355, 0.052251722663640976, 0.006692181807011366, 0.029053829610347748, 0.05157654359936714, -0.053826648741960526, 0.05922183021903038, 0.004383416380733252, 0.0029034793842583895, -0.009292484261095524, -0.0667487308382988, -0.01939607597887516, 0.02696274034678936, 0.013988045044243336, 0.059236761182546616, 0.002888416638597846, 0.004973513539880514, -0.01084493100643158, -0.03393896669149399, 0.003472422482445836, 0.0035342243500053883, -0.0486472025513649, 0.007264544256031513, -0.007920731790363789, 0.015597708523273468, 0.01313565019518137, -0.004419966135174036, -0.019788257777690887, 0.011834892444312572, 0.004210616461932659, 0.004688818473368883, -0.04259055480360985, 0.007920368574559689, 0.030654333531856537, -0.003336508758366108, 0.0006730898167006671, -0.004574465099722147, 0.06419017910957336, 0.03328429162502289]\n",
            "[-0.006332166027277708, -0.00822352897375822, -0.014464504085481167, -0.05680663511157036, 0.009324422106146812, 0.0034724415745586157, 0.01438699010759592, -0.04281593859195709, 0.026579787954688072, 0.010314357466995716, 0.05600443482398987, -0.0077405814081430435, 0.03188120201230049, 0.019051309674978256, -0.012084736488759518, 0.0006474329857155681, 0.025722267106175423, 0.019833169877529144, -0.012804540805518627, -0.051161814481019974, 0.01274709403514862, -0.018077002838253975, 0.0006599840126000345, -0.0018343885894864798, 0.07481805980205536, -0.03220312297344208, 0.03170042857527733, -0.0496043860912323, -0.01436623465269804, 0.05247478187084198, -0.07438939809799194, 0.05319599434733391, -0.08905528485774994, -0.004539891146123409, -0.007467073854058981, -0.02387746423482895, -0.005717173218727112, -0.011360274627804756, -0.001178929815068841, 0.08863552659749985, 0.0044535440392792225, -0.003807578468695283, -0.032894279807806015, -0.04083385691046715, 0.030320018529891968, -0.018680255860090256, 0.006341264583170414, 0.020881125703454018, 0.0035658932756632566, -0.05230938643217087, 0.0009052691166289151, 0.017750155180692673, 0.031087886542081833, -0.03909201920032501, -0.03184995427727699, -0.03367836773395538, 0.009556821547448635, 0.03411555662751198, -0.04403672739863396, -0.025905828922986984, 0.02140767127275467, 0.014590607024729252, -0.01976892724633217, 0.04207693785429001, 0.0133728152140975, 0.007383553311228752, -0.07653940469026566, 0.057538047432899475, 0.026084309443831444, -0.056671466678380966, 0.04390762746334076, -0.026888687163591385, 0.01908860169351101, -0.049889691174030304, -0.04149121791124344, -0.008219531737267971, -0.03214683756232262, 0.07472586631774902, 0.009675280191004276, -0.02018657512962818, 0.05356384813785553, -0.03166160359978676, 0.0031261269468814135, 0.00035878107883036137, -0.05731692537665367, 0.06256542354822159, -0.04949212074279785, -0.012230362743139267, -0.018528858199715614, 0.06648014485836029, -0.028633499518036842, 0.040603503584861755, -0.01527594868093729, -0.07021017372608185, 0.04457944259047508, 0.0031444295309484005, 0.012913639657199383, -0.0029484869446605444, -0.016442833468317986, -0.039709191769361496, -0.005683897528797388, -0.023486675694584846, -0.04422970116138458, 0.018331078812479973, 0.027376793324947357, 0.008549010381102562, 0.013967692852020264, 0.056536972522735596, -0.006944487802684307, -0.0037593375891447067, -0.036546602845191956, 0.013240653090178967, -0.005799820646643639, -0.043644633144140244, 0.04145650193095207, -0.01681729592382908, -0.05503241345286369, 0.054537806659936905, 0.023914525285363197, 0.004378036130219698, -0.01223444752395153, 0.008723508566617966, 0.06534495949745178, -0.013200300745666027, -0.002521870192140341, 0.012102282606065273, -0.007893688045442104, 0.019675863906741142, 0.005670183803886175, 0.0235742200165987, -0.04653119295835495, -0.06479053944349289, -0.017249969765543938, 0.008806219324469566, 0.04770113527774811, 0.06243157386779785, 0.06225805729627609, 0.012229103595018387, 0.056920815259218216, 0.04495227709412575, 0.0050054206512868404, 0.01994835026562214, -0.022710168734192848, -0.009610173292458057, 0.00883969385176897, 0.050657037645578384, -0.025265468284487724, -0.06010043993592262, -0.011994162574410439, -0.006970300804823637, -0.0027360415551811457, -0.016772858798503876, -0.05334392189979553, 0.03094140999019146, 0.0749802365899086, 0.03730805218219757, -0.0004868867981713265, -0.01346841175109148, 0.016997165977954865, 0.014580465853214264, 0.05846763029694557, 0.03106069378554821, 0.02590048313140869, 0.008903543464839458, 0.04764170944690704, -0.05475321412086487, 0.00526118790730834, -0.001267503364942968, -0.012367586605250835, -0.016237078234553337, -0.011895635165274143, 0.018267054110765457, -0.04517221450805664, -0.05446302145719528, 0.0028297100216150284, -0.07901404052972794, -0.002109702443704009, -0.04214769974350929, -0.019700301811099052, -0.03537498787045479, -0.01990681327879429, -0.04373051971197128, -0.031143777072429657, 0.0280934888869524, 0.0036479937843978405, -0.02598438784480095, 0.052178144454956055, -0.0026143882423639297, -0.018596434965729713, -0.0024850349873304367, -0.011735893785953522, 0.012306573800742626, -0.02950161322951317, 0.024958811700344086, 0.009245157241821289, 0.01619976945221424, 0.0011012167669832706, -0.004583443980664015, 0.005106269847601652, -0.04255131632089615, -0.011067737825214863, 0.08707249909639359, -0.015222282148897648, 0.01964983157813549, 0.017697390168905258, 0.023367833346128464, 0.06822340935468674, -0.023420805111527443, 0.00851165410131216, 0.04357055947184563, 0.0033629355020821095, 0.013211466372013092, -0.037161022424697876, -0.018958065658807755, 0.009195245802402496, 0.03374553471803665, 0.0503048375248909, 0.03234122693538666, 0.027880877256393433, -0.019160529598593712, -0.015961211174726486, -0.032207272946834564, -0.013776656240224838, 0.06494968384504318, -0.008976002223789692, 0.009217838756740093, -0.017863819375634193, -0.000179340218892321, -0.014248539693653584, -0.06959766894578934, 0.0005034460336901248, 0.05081868916749954, -0.029033692553639412, -0.054429005831480026, 0.060356028378009796, 0.02009434811770916, -0.007180685177445412, 0.0024295635521411896, 0.012292956933379173, 0.025607775896787643, -0.033042557537555695, 0.03182971104979515, 0.09817341715097427, 0.03828128054738045, -0.02921072207391262, -0.041004203259944916, 0.010082505643367767, 0.02144678868353367, -0.01952504739165306, 0.04563678056001663, -0.0005767563707195222, -0.0734441801905632, 0.018948154523968697, 0.003481189953163266, -0.06789784133434296, 0.016766902059316635, -0.0465465746819973, 0.04862577095627785, 0.0013537199702113867, 0.01806773990392685, 0.059684157371520996, -0.031093960627913475, 0.003590796608477831, -0.009707399643957615, -0.05589175224304199, 0.016218632459640503, 0.0045119537971913815, -0.06887847930192947, 0.01570233330130577, -0.00435404060408473, 0.02753852866590023, -0.06483042985200882, 0.054110296070575714, 0.030400734394788742, 0.012666002847254276, 0.009101622737944126, -0.03019559383392334, 0.01685759425163269, 0.01127224788069725, -0.017107682302594185, 0.04425226151943207, 0.040421538054943085, 0.0692676231265068, -0.045873869210481644, 0.001799353281967342, -0.042142849415540695, -0.08645506948232651, 0.00037267725565470755, -0.008230806328356266, -0.010932687669992447, -0.03542396426200867, -0.04157625883817673, -0.03963689133524895, -0.038282930850982666, -0.045541223138570786, -0.007115588523447514, -0.00986792054027319, 0.04499901831150055, 0.045585740357637405, -0.001969846896827221, -0.008020787499845028, 0.029372410848736763, -0.007574901916086674, -0.08391363173723221, 0.007323253434151411, -0.016556745395064354, -0.009944021701812744, -0.06365818530321121, -0.01592523790895939, 0.032241564244031906, -0.0349540188908577, -0.02536497637629509, -0.0236594770103693, -0.015519935637712479, 0.04556244611740112, 0.03729462996125221, -0.01044116634875536, 0.014047646895051003, -0.044568102806806564, -0.006697603967040777, -0.010848555713891983, 0.06686494499444962, -0.020082199946045876, -0.02836650237441063, -0.01621049828827381, -0.010868759825825691, -0.009586572647094727, 0.0723666101694107, 0.0493353009223938, -0.03850741684436798, -0.03824865445494652, 0.035955626517534256, -0.05630674585700035, 0.04673558846116066, 0.006996144540607929, 0.01332033146172762, -0.03728889301419258, 0.05801180750131607, -0.07026010751724243, 0.016898375004529953, 0.07401490956544876, 0.013965525664389133, -0.0212226714938879, -0.03751461207866669, -0.02838832512497902, 0.00282649090513587, -0.029189126566052437, 0.03619273379445076, 0.08719077706336975, -0.010461793281137943, -0.002568766474723816, 0.06550285220146179, -0.002613422693684697, 0.005313934292644262, -0.011251251213252544, -0.07948709279298782, 0.010855510830879211, -0.03457155451178551, 0.016783662140369415, -0.04165758565068245, -0.020406430587172508, 0.0715617761015892, -0.015655532479286194, 0.026090404018759727, -0.06659998744726181, 0.05224449560046196, 0.056511007249355316, 0.017046598717570305, -0.018007047474384308, 0.01662961207330227, -0.002810560166835785, -0.03735863044857979, -0.026178767904639244, -0.01765221729874611, -0.03127446398139, -0.04251177981495857, -0.055521100759506226, -0.029061391949653625, 0.009475814178586006, 0.02411748841404915, 0.024302106350660324, -0.01916712336242199, 0.06904362142086029, 0.04810529202222824, 0.006879673805087805, -0.020226402208209038, 0.0362129844725132, -0.007707476150244474, -0.007303836289793253, 0.038551248610019684, 0.04516085982322693, 0.016278481110930443, 0.07324723899364471, 0.06400588154792786, -0.042708270251750946, 0.006876244675368071, 0.008690990507602692, -0.06383899599313736, 0.005963462870568037, -0.024305257946252823, 0.02000952884554863, 0.005981534253805876, -0.03734573349356651, -0.017394481226801872, 0.009905540384352207, -0.005490302573889494, 0.019328374415636063, -0.010236674919724464, 0.009785202331840992, 0.018778719007968903, 0.02250603586435318, -0.010842468589544296, 0.06498701870441437, -0.022937873378396034, -0.07191147655248642, -0.03074895776808262, 0.05830919370055199, 0.011134573258459568, -0.013015764765441418, 0.030235735699534416, 0.0004525792901404202, -0.01071858312934637, -0.013246256858110428, -0.026432542130351067, -0.0075041851960122585, -0.045131176710128784, 0.014123723842203617, -0.023337209597229958, 0.008039399050176144, 0.03732844069600105, 0.03261367231607437, -0.016295919194817543, 0.011899242177605629, -0.04125257581472397, -0.0360620841383934, -0.04083339869976044, 0.027883945032954216, 0.012370949611067772, -0.0330536775290966, 0.012007993645966053, -0.023094192147254944, -0.031121373176574707, 0.06505684554576874, -0.014722189866006374, -0.0654212087392807, -0.028665632009506226, -0.02615348994731903, -0.04427174851298332, 0.07924190908670425, -0.1002398431301117, 0.011774376034736633, -0.056673940271139145, 0.01424897275865078, -0.07334012538194656, -0.026393042877316475, -0.026391349732875824, 0.02103222720324993, 0.04178436100482941, -0.015826821327209473, 0.02259708009660244, -0.022911436855793, -0.0338742621243, -0.003314427100121975, -0.07270954549312592, 0.02440410852432251, -0.027278529480099678, 0.057143885642290115, 0.012586717493832111, 0.04048451781272888, 0.05638353154063225, 0.017322992905974388, 0.002955250209197402, 0.012201902456581593, 0.027013855054974556, -0.03360707312822342, 0.03853802755475044, -0.05042532831430435, 0.04623866081237793, 0.04317362606525421, -0.03579794615507126, -0.00826738215982914, 0.007685997989028692, 0.04648011550307274, -0.016383161768317223, -0.027333442121744156, 0.023596294224262238, 0.005037402268499136, -0.015474437735974789, -0.028902791440486908, 0.05736904591321945, -0.00972397904843092, -0.016784504055976868, 0.020581595599651337, -0.0358554981648922, -0.041382405906915665, -0.015182760544121265, 0.034463297575712204, 0.049216561019420624, 0.03603394329547882, 0.005361166782677174, -0.001956503139808774, -0.03463797643780708, 0.010307135060429573, -0.023112192749977112, 0.06212340667843819, -0.051610104739665985, 0.02416435070335865, 0.01722479984164238, -0.021909696981310844, -0.03314421325922012, -0.019558336585760117, 0.023371903225779533, 0.0006045065820217133, -0.017339732497930527, -0.011083601973950863, 0.029917342588305473, 0.010519241914153099, 0.030903294682502747, -0.014101543463766575, 0.00886147003620863, 0.07726714760065079, -0.014075946994125843, -0.1010916605591774, -0.006181885953992605, -0.00011818493658211082, -0.04697272554039955, 0.032761719077825546, 0.02796330489218235, -0.022617796435952187, 0.022733259946107864, -0.010036539286375046, 0.08836553245782852, -0.036275431513786316, -0.012090381234884262, 0.00838710367679596, -0.025823574513196945, -0.0037644929252564907, 0.03884034976363182, -0.0002816852065734565, -0.026303429156541824, 0.04891647398471832, -0.048927754163742065, -0.015803853049874306, 0.007156419567763805, -0.0037000898737460375, 0.020839646458625793, 0.054395198822021484, -0.1271219700574875, -0.0007126582786440849, -0.026926366612315178, -0.055495429784059525, -0.0308977123349905, -0.002688049804419279, -0.03751259297132492, 0.008813347667455673, -0.006400954443961382, -0.033489104360342026, 0.016286266967654228, -0.0373542420566082, -0.007113006431609392, -0.03364536166191101, -0.03855767846107483, 0.07387395948171616, 0.022095952183008194, 0.04845088720321655, 0.04654344543814659, -0.007086847443133593, 0.02162167802453041, 0.05996650084853172, 0.01523300539702177, -0.004160447046160698, 0.06727038323879242, 0.04506966471672058, -0.02884279377758503, 0.016462424769997597, 0.0023751724511384964, -0.03576979041099548, 0.0074481964111328125, 0.018824676051735878, -0.02269722707569599, 0.010151311755180359, 0.04736117273569107, -0.004762188531458378, 0.022513914853334427, 0.016081098467111588, 0.07208140939474106, -0.006282265763729811, -0.035358328372240067, 0.04348227009177208, -0.010724597610533237, -0.024346180260181427, -0.031239351257681847, -0.030752314254641533, -0.004488928243517876, -0.008404526859521866, 0.0364457368850708, -0.00010694823140511289, 0.048931803554296494, -0.027220575138926506, -0.031127355992794037, 0.06168631464242935, -0.02230219729244709, 0.0673442855477333, -0.07004055380821228, 0.03433092311024666, 0.03939211741089821, 0.0063563683070242405, -0.02993098832666874, -0.03053787350654602, -0.017320914193987846, 0.029356107115745544, -0.007400559727102518, -0.001063718693330884, -0.08094216138124466, -0.04971543699502945, 0.010527956299483776, 0.05319151654839516, -0.020878326147794724, -0.04340608790516853, -0.002123191487044096, -0.004895600490272045, -0.024794306606054306, -0.016330623999238014, 0.014798174612224102, 0.04500267654657364, 0.005800032056868076, -0.05775583162903786, -0.044906847178936005, 0.05679899826645851, -0.0026284148916602135, -0.004540069028735161, 0.05726579204201698, 0.015373311936855316, 0.0037175938487052917, -0.04462716728448868, 0.04449509456753731, 0.019130278378725052, -0.03844049572944641, 0.06321712583303452, -0.031549688428640366, -0.10324614495038986, -0.009517990984022617, 0.045298296958208084, -0.0179174542427063, -0.0006701464299112558, 0.08859806507825851, 0.01353198941797018, -0.06984159350395203, -0.028150038793683052, 0.017993653193116188, -0.0452108159661293, 0.029224826022982597, 0.007014616392552853, 0.003665035590529442, -0.013532786630094051, 0.03266507759690285, -0.034742265939712524, -0.020327800884842873, 0.04707027226686478, 0.018165981397032738, -0.07866979390382767, 0.02604988031089306, -0.0016427671071141958, -0.0014642373425886035, -0.0007573147304356098, -0.03333986923098564, -0.03981851413846016, -0.04702983796596527, -0.006005658768117428, 0.07156381011009216, -0.02430306188762188, 0.05201772227883339, 0.03854680061340332, -0.0416838563978672, 0.00575868459418416, 0.020131174474954605, 0.015297907404601574, 0.038923610001802444, -0.054991573095321655, 0.025102682411670685, -0.03900117054581642, -0.022632533684372902, -0.0344005785882473, 0.04712942615151405, 0.018979281187057495, 0.03914406895637512, 0.019407618790864944, 0.008313983678817749, 0.007092860992997885, -0.07480093091726303, 0.016998428851366043, 0.008450815454125404, -0.03562851622700691, 0.01646801270544529, 0.038919564336538315, 0.03906508535146713, -0.023946324363350868, 0.0005150760407559574, -0.009831213392317295, 0.04780401661992073, -0.022037258371710777, 0.0035951053723692894, -0.0029385071247816086, 0.0015799847897142172, -0.004059208557009697, 0.044138070195913315, 0.019071077927947044, -0.01370670460164547, 0.022095631808042526, 0.009637444280087948, -0.006692444905638695, -0.028850538656115532, -0.01834847778081894, 0.04889790713787079, 0.021382395178079605, -0.08816438168287277, 0.05290139466524124, -0.005105198360979557, -0.04378552734851837, 0.03775044530630112, 0.0970325618982315, 0.031236477196216583, 0.02365771308541298, -0.07075947523117065, -0.05468496307730675, 0.03649242967367172, -0.05565623566508293, 0.024845078587532043, 0.0013713219668716192, -0.00873659085482359, 0.026381943374872208, 0.006252927239984274, 0.01170879416167736, 0.07334553450345993, -0.05454261973500252, 0.058864761143922806, 0.004063171800225973, 0.03257272019982338, 0.012366145849227905, -0.05245770514011383, 0.005463084205985069, 0.029185254126787186, 0.019454581663012505, 0.06921549141407013, 0.005755291786044836, -0.015896691009402275, 0.00431612366810441, 0.004982156213372946, 0.0003810603229794651, 0.009668832644820213, -0.07874227315187454, 0.0008262699120678008, -0.007617757190018892, 0.024712571874260902, 0.017329618334770203, -0.021482758224010468, -0.012106366455554962, -0.0024927551858127117, 0.005462105851620436, -0.010723200626671314, -0.02634696662425995, 0.057716820389032364, 0.0035632692743092775, 0.0015746088465675712, 0.007447585929185152, -0.012017943896353245, 0.03510730341076851, 0.05536537244915962]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "embd = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "query_result = embd.embed_query(question)\n",
        "document_result = embd.embed_query(document)\n",
        "len(query_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e0e35f-6861-4c5e-9301-04fd5408f8f8",
      "metadata": {
        "id": "f5e0e35f-6861-4c5e-9301-04fd5408f8f8"
      },
      "source": [
        "[Cosine similarity](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions) is reccomended (1 indicates identical) for OpenAI embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "b8001998-b08c-4560-b124-bfa1fced8958",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8001998-b08c-4560-b124-bfa1fced8958",
        "outputId": "aaceec97-6bb0-4644-ad98-5048a39ef010"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cosine Similarity: 0.8535652119095083\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "similarity = cosine_similarity(query_result, document_result)\n",
        "print(\"Cosine Similarity:\", similarity)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aea73bc-98e3-4fdc-ba72-d190736bed20",
      "metadata": {
        "id": "8aea73bc-98e3-4fdc-ba72-d190736bed20"
      },
      "source": [
        "[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5778c31a-6138-4130-8865-31a08e82b9fb",
      "metadata": {
        "id": "5778c31a-6138-4130-8865-31a08e82b9fb"
      },
      "outputs": [],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798e731e-c6ff-46e3-a8bc-386832362af2",
      "metadata": {
        "id": "798e731e-c6ff-46e3-a8bc-386832362af2"
      },
      "source": [
        "[Splitter](https://python.langchain.com/docs/modules/data_connection/document_transformers/recursive_text_splitter)\n",
        "\n",
        "> This text splitter is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e668d339-3951-4662-8387-c3d296646906",
      "metadata": {
        "collapsed": true,
        "id": "e668d339-3951-4662-8387-c3d296646906"
      },
      "outputs": [],
      "source": [
        "# Split\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "427303a1-3ed4-430c-bfc7-cb3e48022f1d",
      "metadata": {
        "id": "427303a1-3ed4-430c-bfc7-cb3e48022f1d"
      },
      "source": [
        "[Vectorstores](https://python.langchain.com/docs/integrations/vectorstores/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baa90aaf-cc1b-46a1-9fba-cf20804dcb41",
      "metadata": {
        "id": "baa90aaf-cc1b-46a1-9fba-cf20804dcb41"
      },
      "outputs": [],
      "source": [
        "# Index\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "vectorstore = Chroma.from_documents(documents=splits,\n",
        "                                    embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
        "\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba890329-1411-4922-bd27-fe0490dd1208",
      "metadata": {
        "id": "ba890329-1411-4922-bd27-fe0490dd1208"
      },
      "source": [
        "## Part 3: Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930",
      "metadata": {
        "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930"
      },
      "outputs": [],
      "source": [
        "# # Index\n",
        "# from langchain_google_genai import google_vector_store\n",
        "# from langchain_community.vectorstores import Chroma\n",
        "# vectorstore = Chroma.from_documents(documents=splits,\n",
        "#                                     embedding=GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\"))\n",
        "\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "id": "57c2de7a-93e6-4072-bc5b-db6516f96dda",
      "metadata": {
        "id": "57c2de7a-93e6-4072-bc5b-db6516f96dda"
      },
      "outputs": [],
      "source": [
        "docs = retriever.get_relevant_documents(\"what is NER?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "id": "db96f877-60d3-4741-9846-e2903831583d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db96f877-60d3-4741-9846-e2903831583d",
        "outputId": "3c399d7a-d949-45dd-8b57-56bc8f598af1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "id": "omcvmTTyYSWF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omcvmTTyYSWF",
        "outputId": "0909935a-54a5-43c4-8d0e-e0b566c28029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'type': 'html', 'source': 'https://link.springer.com/article/10.1007/s00521-021-05961-4'}, page_content='### 5.1 SMILES reconstruction\\n\\nForty-six tests (9x5+1 for more details see 4.1 ) were conducted to assess the accuracy of SMILES reconstruction on different portion of training data. The results obtained are detailed in Table 6 .\\nTable 6 SMILES reconstruction on different portion of training data\\nFull size table\\nChanges in accuracy and editing distance for different size of training data sets are presented in Figs. 13 and 14 . As it can be seen from Fig. 13 , the reconstruction accuracy increases from 0.247 \\\\(\\\\pm \\\\) 0.027 for 10% of randomly selected samples to 0.877 \\\\(\\\\pm \\\\) 0.009 for 50% of samples accordingly. From 60% onwards accuracy stays around 0.8 on average with slight fluctuations. This is an expected result. However slight variations in accuracy starting from 60% of samples needs to be addressed. It is a difficult task to identify the exact reason of what is influencing these changes in accuracy. One of the possible reasons for such behaviour is early signs of overfitting. Obviously there is no evidence of this phenomenon spotted during training. However, it is possible that with increasing of samples number a model starts to memorize the input. This can be addressed by a better sampling algorithm. For example, Butina clustering [ 13 ] can be a useful technique to design a sampling algorithm.\\nIn addition to accuracy, model performance was measured using Hamming and Levenshtein distances. Both belong to a family of editing distances and give a different perspective on the results obtained. They show a similar trend to accuracy. The Hamming distance decreased from 4.374 \\\\(\\\\pm \\\\) 0.817 to 0.663 \\\\(\\\\pm \\\\) 0.071 for 10% and 50% of sample cases respectively. The Levenshtein distance decreased from 4.299 \\\\(\\\\pm \\\\) 0.127 to 0.648 \\\\(\\\\pm \\\\) 0.031 for the same percentage of samples. Both show slight fluctuation in editing distance from 60% onwards.\\nSMILES reconstruction accuracy for different percentage of training sample\\nFull size image\\nSMILES reconstruction editing distances for different percentages of training samples\\nFull size image\\nAccording to the carried out experiment the best result (accuracy 0.877 \\\\(\\\\pm \\\\) 0.009 ) was obtained for 50% of randomly selected samples. This configuration was selected for building a production variational autoencoder , which can be later used for training classification and regression models with SMILES input. Training was conducted using 40 epochs, and produced a model with accuracy 0.872 . The editing distances for the production model were 0.682 and 0.677 for Hamming and Levenshtein respectively.\\n\\n### 5.2 LogD prediction'), Document(metadata={'subject': '', 'title': '', 'author': '', 'moddate': '', 'pages': 5, 'total_pages': 16, 'producer': 'Skia/PDF m66', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'modDate': '', 'keywords': '', 'creationDate': '', 'page': 5, 'type': 'pdf', 'creationdate': '', 'format': 'PDF 1.5', 'creator': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'trapped': ''}, page_content='Figure . Heatmaps are here depicted representing the computed attention during character prediction in the order left to right, top to bottom: [, @, , /, =, . The complete encoder-decoder framework is fully differentiable and was trained end-to-end using  suitable form of backpropagation, enabling SMILES to be fully generated using only raw images as input. During decoding SMILES were generated  character at  time, from left to right. Additionally, no external dictionary was used for chemical abbreviations (superatoms) rather these were learned as part of the model, thus images may contain superatoms and the SMILES are still generated  character at  time. This model operates on raw images and directly generates chemically valid SMILES with no explicit subcomponent recognition required. Datasets Segmentation Dataset To our knowledge, no dataset addressing molecular structure segmentation has been published. To provide sufficient data to train  neural network while minimizing manual effort required to curate such  dataset, we developed  pipeline for automatically generating segmentation data. To programmatically generate data, in summary, the following steps were performed: ) remove structures from journal and patent pages, ii) overlay structures onto the pages, iii) produce  ground truth mask identifying the overlaid structures, and iv) randomly crop images from the pages containing structures and the corresponding mask. In detail, OSRA\\u200b was utilized to identify bounding boxes of candidate molecules within the pages of  large number of publications, both published journal articles and patents. The regions expected to contain molecules were whited-out, thus leaving pages without molecules. OSRA was not always correct in finding structures and occasionally non-structures (.., charts) were removed suggesting that cleaner input may further improve model performance. Next, images of molecules made publically available by the United States Patent and Trademark Office (USPTO)\\u200b28 were randomly overlaid onto the pages while ensuring that no structures overlapped with any non-white pixels. Structure images were occasionally perturbed using affine transformations, changes in background shade, and/or lines added around the structure (to simulate table grid lines). We also generated the true mask for each overlaid page; these masks were zero-valued except where pixels were part of  molecule (pixels assigned  value of ). During training, samples of 128x128 pixels were randomly cropped from the'), Document(metadata={'title': '', 'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'trapped': '', 'modDate': 'D:20181203013712Z', 'source': 'http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf', 'creationDate': 'D:20181203013712Z', 'moddate': '2018-12-03T01:37:12+00:00', 'creationdate': '2018-12-03T01:37:12+00:00', 'file_path': '/tmp/tmpd75a7p3t.pdf', 'author': '', 'subject': '', 'format': 'PDF 1.5', 'keywords': '', 'page': 1, 'total_pages': 13, 'pages': 1, 'type': 'pdf'}, page_content='and to construct features similar to engineered chemical features, with minimal assistance from an expert chemist. This approach that leverages representation learning of deep neural networks is  signiﬁcant departure from the traditional research paradigm in chemistry. In this work, we develop CheMixNetset of neural networks for predicting chemical properties by leveraging multiple molecular representations as inputs. We used simpliﬁed molecular-input line-entry system (SMILES) [10] notations as sequence inputs and molecular ﬁngerprints as vector inputs. SMILES is  line notation of chemical structures which encodes the connection table and the stereochemistry of  molecule as  line of text. Our work improves upon the existing state-ofthe-art approach of directly learning from vector representations such as molecular ﬁngerprints or chemical text representations such as SMILES by harnessing the network structure of both forms of representations. CheMixNet is  variation of multi-input-single-output (MISO) [11] architectures that learn the chemical properties from  mix of intermediate features learned from two different input representations -  vector input in the form of molecular ﬁngerprints and  sequence input in the form of SMILES strings. In our experiments, we used MACCS ﬁngerprints -  ﬁrst 2D representation of chemical structure using 167 features. Although MACCS usually perform worse than other molecular ﬁngerprints, we chose MACCS because of its simplicity and ease of interpretation. We perform signiﬁcant experimentation to determine the best neural network structure for the CheMixNet architectures. We evaluated the effectiveness of our mixed approach for building DNN architectures by training CheMixNet on six different datasetslarge dataset composed of . million samples from the Harvard Clean Energy Project (CEP) database and ﬁve other relatively smaller datasets from the MoleculeNet [12, 13] benchmark. Compared to other DL models, CheMixNet architecture outperforms fully connected MLP models trained on molecular ﬁngerprints, recurrent neural networks (RNN) and -dimensional convolutional neural network (CNN) models trained on SMILES, as well as other models - convolutional molecular graphs (ConvGraph) [14] and Chemception [15]. For instance, we achieved  mean absolute percentage error (MAPE) of .24 % on the CEP dataset; this is signiﬁcantly better than the MAPE of .43 % using CNN-RNN model. The CheMixNet architectures, as well as the benchmark models, are made accessible for the research community at https://github.com/paularindam/CheMixNet [16].  Background and Related Works In this section, we present  description of the two molecular representations we use in this work - SMILES and molecular ﬁngerprints, and discuss existing deep neural architectures for predictive modeling of chemical properties in the Quantitative structure-activity relationship (QSAR)/Quantitative structure-property relationship (QSPR) [17] modeling. . SMILES & Fingerprints Line notations are linear representations of chemical structures which encode the connection table and the stereochemistry of  molecule as  line of text [18]. SMILES [10] is the most popular speciﬁcation in the form of  line notation to describe the structure of chemical species using short ASCII strings encoding molecular structures and speciﬁc instances. One or more organic molecules attach to form long continuous chains known as branches. SMILES has  grammar structure in which alphabets denote atoms, special characters such as = and ≡bond denote the type of bonds, encapsulated numbers indicate rings, and parentheses represent side chains. In this work, we limit ourselves to character level representation and do not explicitly encode the grammar. Molecular ﬁngerprints are representations of chemical structures, successfully used in similarity search [19], clustering [20], classiﬁcations [21], drug discovery [22], and virtual screening [23],  standard and computationally efﬁcient abstract representation where structural features are represented by either bits in  bit string or counts in  count vector. Fingerprints were motivated by the need to ﬁnd materials that match target material properties. They follow the assumptions that the properties of the material is  direct function of its structure and that materials with similar structure are likely to have similar physical-chemical character. Different ﬁngerprints represent different aspects of  molecule, and thus each type of ﬁngerprint can have different suitability for mapping to particular physical property. Various machine learning (ML) algorithms have been used to predict the activity or property of chemicals using molecular descriptors and/or ﬁngerprints as input features. In our'), Document(metadata={'producer': 'Skia/PDF m66', 'creationDate': '', 'author': '', 'type': 'pdf', 'total_pages': 16, 'creationdate': '', 'source': 'https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf', 'keywords': '', 'modDate': '', 'pages': 4, 'moddate': '', 'trapped': '', 'title': '', 'file_path': '/tmp/tmplgbr61h3.pdf', 'format': 'PDF 1.5', 'page': 4, 'creator': '', 'subject': ''}, page_content=\"Figure . An example showing the output of the segmentation model when processing  journal article page from Salunke et al.\\u200b14 All of the text and other extraneous items are completely removed with the exception of  few faint lines that are amenable to automated post processing. state vector from the encoder was used to initialize the GridLSTM cell states and the SMILES sequence was then generated  character at  time, similar to the decoding method described in Sutskever et al.\\u200b26 (wherein sentences were generated  word at  time while translating English to French). Decoding is started by projecting  special start token into the GridLSTM (initialized by the encoder and conditioned on an initial context vector as computed by the attention mechanism), processing this input in the cell, and predicting the first character of the output sequence. Subsequent characters are produced similarly, with each prediction conditioned on the previous cell state, the current attention, and the previous output projected back into the network. The logits vector for each character produced by the network is of length , where  is the number of available characters (65 characters in this case).  softmax activation is applied to the logits to compute  probability distribution over characters, and the highest scoring character is selected for  particular step in the sequence. Sequences were generated until  special end-of-sequence token was predicted, at which point the completed SMILES string was returned. During inference we found accuracy improved when predicting images at several different (low) resolutions and returning sequences of the highest confidence, which was determined by multiplying together the softmax output of each predicted character in the sequence. The addition of an attention mechanism in the decoder helped solve several challenges. Most importantly, attention enabled the decoder to access information produced earlier in the encoder and minimized the loss of important details that may otherwise be overly compressed when encoding the state vector. Additionally, attention enabled the decoder to reference information closer to the raw input during the prediction of each character and was important considering the significance of pixelwise features in low resolution structure images. See Figure  for an example of the computed attention and how the output corresponds to various characters recognized during the decoding process. Apart from using attention for improved performance, the attention output is useful for repositioning  predicted structure into an orientation that better matches the original input image. This is done by converting the SMILES into  connection table using the open source Indigo toolkit,\\u200b27 and repositioning each atom in 2D space according to the coordinates of each character' computed attention. The repositioned structure then more closely matches the original positioning and orientation in the input image, enabling users to more easily identify and correct mistakes when comparing the output with the original source.\")]\n"
          ]
        }
      ],
      "source": [
        "print(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2",
      "metadata": {
        "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2"
      },
      "source": [
        "## Part 4: Generation\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
        "outputId": "5aa89152-6331-45dd-9174-a0a14ba27881"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context from research papes to answer the question in detail minimum 500 words. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question}\\nContext: {context}\\n\"), additional_kwargs={})])"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prompt\n",
        "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context from research papes to answer the question in detail minimum 500 words. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "e4461264-5cac-479a-917c-9bf589826da4",
      "metadata": {
        "id": "e4461264-5cac-479a-917c-9bf589826da4"
      },
      "outputs": [],
      "source": [
        "# LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d",
      "metadata": {
        "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d"
      },
      "outputs": [],
      "source": [
        "# Chain\n",
        "chain = prompt | llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
        "outputId": "c3f8dd91-b2cd-4ce9-9c49-3a6e97527373"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content='Based on the context provided, NER refers to Named Entity Recognition, specifically in the context of the BC5CDR task. The experiment results for ChemProt relation extraction and BC5CDR NER indicate that pre-trained language models are generally the best solutions for these natural language processing tasks. Models like BioBERT (+PubMed) and RoBERTa achieve comparable results with Sci-BERT in these tasks.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--a5769e56-f382-4730-aec3-023040ddb2d5-0', usage_metadata={'input_tokens': 3546, 'output_tokens': 79, 'total_tokens': 3625, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Run\n",
        "chain.invoke({\"context\":docs,\"question\":\"What is NER\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65770e2d-3d5e-4371-abc9-0aeca9646885",
      "metadata": {
        "id": "65770e2d-3d5e-4371-abc9-0aeca9646885"
      },
      "outputs": [],
      "source": [
        "from langchain import hub\n",
        "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f53e5840-0a0f-4428-a4a4-6922800aff89",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f53e5840-0a0f-4428-a4a4-6922800aff89",
        "outputId": "b4412e02-a986-4009-ae52-e3947dde645f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt_hub_rag"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ffe29a1-5527-419e-9f12-8a3061d12885",
      "metadata": {
        "id": "8ffe29a1-5527-419e-9f12-8a3061d12885"
      },
      "source": [
        "[RAG chains](https://python.langchain.com/docs/expression_language/get_started#rag-search-example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "id": "8208a8bc-c75f-4e8e-8601-680746cd6276",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "8208a8bc-c75f-4e8e-8601-680746cd6276",
        "outputId": "47d79faa-bba7-4c4a-f329-c9b17ab2beba"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I am sorry, but the provided context does not contain information about Task Decomposition and Self Reflection. Therefore, I cannot answer your question.'"
            ]
          },
          "execution_count": 106,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "rag_chain.invoke(\"What is Task Decomposition and Self Reflection?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1CTl-nqddyg8",
      "metadata": {
        "id": "1CTl-nqddyg8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "TDS1RcnPdpNL",
      "metadata": {
        "id": "TDS1RcnPdpNL"
      },
      "source": [
        "# Basic RAG using PINECONE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CvBmv7vVdy_W",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvBmv7vVdy_W",
        "outputId": "c04f7ec4-00e3-42e5-ef9e-e7d2a8dfee55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.2.8-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting pinecone\n",
            "  Downloading pinecone-7.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.34 in /usr/local/lib/python3.11/dist-packages (from langchain_pinecone) (0.3.66)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain_pinecone) (2.0.2)\n",
            "Collecting langchain-tests<1.0.0,>=0.3.7 (from langchain_pinecone)\n",
            "  Downloading langchain_tests-0.3.20-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting langchain-openai>=0.3.11 (from langchain_pinecone)\n",
            "  Downloading langchain_openai-0.3.25-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2025.6.15)\n",
            "Collecting pinecone-plugin-assistant<2.0.0,>=1.6.0 (from pinecone)\n",
            "  Downloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone) (4.14.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone) (2.4.0)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.4.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (24.2)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (2.11.7)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain_pinecone) (1.91.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai>=0.3.11->langchain_pinecone) (0.9.0)\n",
            "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (8.3.5)\n",
            "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_asyncio-0.26.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.28.1)\n",
            "Collecting syrupy<5,>=4 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading syrupy-4.9.1-py3-none-any.whl.metadata (38 kB)\n",
            "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting pytest-benchmark (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_benchmark-5.1.0-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pytest-codspeed (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_codspeed-3.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (6.3 kB)\n",
            "Collecting pytest-recording (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading pytest_recording-0.13.4-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting vcrpy>=7.0 (from langchain-tests<1.0.0,>=0.3.7->langchain_pinecone)\n",
            "  Downloading vcrpy-7.0.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.3 in /usr/local/lib/python3.11/dist-packages (from pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (2.32.3)\n",
            "Requirement already satisfied: aiohttp>=3.9.0 in /usr/local/lib/python3.11/dist-packages (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (3.11.15)\n",
            "Collecting aiohttp-retry<3.0.0,>=2.9.1 (from pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone)\n",
            "  Downloading aiohttp_retry-2.9.1-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (6.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.0->pinecone[asyncio]<8.0.0,>=6.0.0->langchain_pinecone) (1.20.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.23.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai>=0.3.11->langchain_pinecone) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.34->langchain_pinecone) (0.4.1)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.6.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.3->pinecone-plugin-assistant<2.0.0,>=1.6.0->pinecone) (3.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai>=0.3.11->langchain_pinecone) (2024.11.6)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from vcrpy>=7.0->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.17.2)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from pytest-benchmark->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (9.0.0)\n",
            "Requirement already satisfied: cffi>=1.17.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (1.17.1)\n",
            "Requirement already satisfied: rich>=13.8.1 in /usr/local/lib/python3.11/dist-packages (from pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (13.9.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.17.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.22)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.8.1->pytest-codspeed->langchain-tests<1.0.0,>=0.3.7->langchain_pinecone) (0.1.2)\n",
            "Downloading langchain_pinecone-0.2.8-py3-none-any.whl (22 kB)\n",
            "Downloading pinecone-7.2.0-py3-none-any.whl (524 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.3/524.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.3.25-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_tests-0.3.20-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_assistant-1.7.0-py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Downloading aiohttp_retry-2.9.1-py3-none-any.whl (10.0 kB)\n",
            "Downloading pytest_asyncio-0.26.0-py3-none-any.whl (19 kB)\n",
            "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
            "Downloading syrupy-4.9.1-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vcrpy-7.0.0-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_benchmark-5.1.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_codspeed-3.2.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (25 kB)\n",
            "Downloading pytest_recording-0.13.4-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: pinecone-plugin-interface, vcrpy, syrupy, pytest-socket, pytest-benchmark, pytest-asyncio, pinecone-plugin-assistant, pytest-recording, pytest-codspeed, pinecone, aiohttp-retry, langchain-tests, langchain-openai, langchain_pinecone\n",
            "Successfully installed aiohttp-retry-2.9.1 langchain-openai-0.3.25 langchain-tests-0.3.20 langchain_pinecone-0.2.8 pinecone-7.2.0 pinecone-plugin-assistant-1.7.0 pinecone-plugin-interface-0.0.7 pytest-asyncio-0.26.0 pytest-benchmark-5.1.0 pytest-codspeed-3.2.0 pytest-recording-0.13.4 pytest-socket-0.7.0 syrupy-4.9.1 vcrpy-7.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_pinecone pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb474b9-1e38-4a20-91cc-24cdce6d8631",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fb474b9-1e38-4a20-91cc-24cdce6d8631",
        "outputId": "96fd86b5-c977-4b69-eff1-6f4ea1714017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pinecone vector store 'langchain-demo' created and retriever initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "#### INDEXING ####\n",
        "\n",
        "# Load blog\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "import pinecone\n",
        "import os\n",
        "\n",
        "# Initialize Pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "\n",
        "# Create Pinecone index (if it doesn't exist)\n",
        "index_name = \"langchain-demo\"  # Choose a unique index name\n",
        "dimension = 768  # Dimension of Google's embedding-001 model\n",
        "\n",
        "# Check if index exists, if not create it\n",
        "if index_name not in [index.name for index in pc.list_indexes()]:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dimension,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1') # Specify cloud and region\n",
        "    )\n",
        "\n",
        "# Load blog (assuming this part is still needed for Pinecone indexing)\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "blog_docs = loader.load()\n",
        "\n",
        "# Split\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=300,\n",
        "    chunk_overlap=50)\n",
        "\n",
        "# Make splits\n",
        "splits = text_splitter.split_documents(blog_docs)\n",
        "\n",
        "# Create vector store\n",
        "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = PineconeVectorStore.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=embeddings,\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "# Get retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "print(f\"Pinecone vector store '{index_name}' created and retriever initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I_b7Fwb_gRgX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "I_b7Fwb_gRgX",
        "outputId": "504def1e-0187-4738-8124-19f5d785d8b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing: https://portlandpress.com/biochemj/article/477/23/4559/227194/Deep-learning-and-generative-methods-in\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\n",
            "Error loading https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236: 403 Client Error: Forbidden for url: https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\n",
            "Failed to load document\n",
            "\n",
            "Processing: https://www.osti.gov/servlets/purl/1427646\n",
            "Successfully loaded 22 documents\n",
            "\n",
            "Processing: https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf\n",
            "Successfully loaded 16 documents\n",
            "\n",
            "Processing: http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf\n",
            "Successfully loaded 13 documents\n",
            "\n",
            "Processing: https://www.nature.com/articles/s41467-022-28494-3\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://link.springer.com/article/10.1007/s00521-021-05961-4\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.mdpi.com/journal/molecules/special_issues/deep_learning_structure\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\n",
            "Error loading https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050: 403 Client Error: Forbidden for url: https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\n",
            "Failed to load document\n",
            "\n",
            "Processing: https://www.mdpi.com/1420-3049/25/12/2764\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://www.nature.com/articles/s41598-025-95720-5\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Processing: https://link.springer.com/article/10.1557/s43578-022-00628-9\n",
            "Successfully loaded 1 documents\n",
            "\n",
            "Summary:\n",
            "- Successfully loaded: 61 documents\n",
            "- Failed URLs: 2\n",
            "Failed URLs: ['https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236', 'https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050']\n",
            "- Total chunks after splitting: 199\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'PineconeVectorStore' object has no attribute '_collection'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-130-3038183343.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDocumentProcessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfailed_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Unpack the tuple here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mvectorstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_vector_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Pass only splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[0;31m# Get retriever\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-130-3038183343.py\u001b[0m in \u001b[0;36mcreate_vector_store\u001b[0;34m(self, splits)\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mindex_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m )\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nVector store created with {vectorstore._collection.count()} chunks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvectorstore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PineconeVectorStore' object has no attribute '_collection'"
          ]
        }
      ],
      "source": [
        "import bs4\n",
        "import fitz  # PyMuPDF\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader, PyMuPDFLoader\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "import requests\n",
        "import tempfile\n",
        "import os\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "from langchain.schema import Document\n",
        "from urllib.parse import urlparse\n",
        "import time\n",
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "import pinecone\n",
        "import os\n",
        "\n",
        "# Initialize Pinecone\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "pc = Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
        "\n",
        "# Create Pinecone index (if it doesn't exist)\n",
        "index_name = \"langchain-demo\"  # Choose a unique index name\n",
        "dimension = 768  # Dimension of Google's embedding-001 model\n",
        "if index_name not in [index.name for index in pc.list_indexes()]:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=dimension,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1') # Specify cloud and region\n",
        "    )\n",
        "class DocumentProcessor:\n",
        "    def __init__(self):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=5000,\n",
        "            chunk_overlap=250\n",
        "        )\n",
        "        self.embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "        self.ua = UserAgent()\n",
        "        self.headers = {\n",
        "            'User-Agent': self.ua.random,\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "        }\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update(self.headers)\n",
        "\n",
        "    def load_html(self, url):\n",
        "        \"\"\"Enhanced HTML loader with better error handling\"\"\"\n",
        "        try:\n",
        "            response = self.session.get(url, timeout=30)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            # Check if content-type is PDF\n",
        "            content_type = response.headers.get('Content-Type', '')\n",
        "            if 'application/pdf' in content_type:\n",
        "                return self.load_pdf_from_url(url)\n",
        "\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "            # Remove unwanted elements\n",
        "            for element in soup(['script', 'style', 'nav', 'footer', 'iframe', 'noscript']):\n",
        "                element.decompose()\n",
        "\n",
        "            # Try to find main content areas\n",
        "            article = (soup.find('article') or\n",
        "                      soup.find('main') or\n",
        "                      soup.find(class_=re.compile('content|main|body|post')) or\n",
        "                      soup.find('div', role='main') or\n",
        "                      soup)\n",
        "\n",
        "            # Extract all text with structure\n",
        "            content = self._extract_structured_content(article)\n",
        "            if not content:\n",
        "                raise ValueError(\"No content extracted from HTML\")\n",
        "\n",
        "            return [Document(page_content=content, metadata={'source': url, 'type': 'html'})]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {url}: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def _extract_structured_content(self, element):\n",
        "        \"\"\"Extract content while preserving document structure\"\"\"\n",
        "        content = []\n",
        "\n",
        "        def process_element(elem):\n",
        "            if isinstance(elem, bs4.NavigableString):\n",
        "                text = elem.strip()\n",
        "                if text and len(text) > 10:\n",
        "                    content.append(text)\n",
        "                return\n",
        "\n",
        "            tag = elem.name\n",
        "            if not tag:\n",
        "                return\n",
        "\n",
        "            text = elem.get_text(' ', strip=True)\n",
        "            if not text or len(text) <= 10:\n",
        "                return\n",
        "\n",
        "            # Handle headings\n",
        "            if tag.startswith('h') and tag[1:].isdigit():\n",
        "                level = int(tag[1:])\n",
        "                content.append(f\"\\n{'#'*level} {text}\\n\")\n",
        "            # Handle list items\n",
        "            elif tag == 'li':\n",
        "                content.append(f\"- {text}\")\n",
        "            # Handle table cells\n",
        "            elif tag in ['td', 'th']:\n",
        "                content.append(f\"[TABLE CELL] {text}\")\n",
        "            # Handle regular paragraphs\n",
        "            elif tag == 'p':\n",
        "                content.append(text)\n",
        "            # Recursively process containers\n",
        "            else:\n",
        "                for child in elem.children:\n",
        "                    process_element(child)\n",
        "\n",
        "        process_element(element)\n",
        "        full_text = '\\n'.join(content)\n",
        "        full_text = re.sub(r'\\n{3,}', '\\n\\n', full_text)\n",
        "        full_text = re.sub(r'[ \\t]{2,}', ' ', full_text)\n",
        "        return full_text.strip()\n",
        "\n",
        "    def load_pdf_from_url(self, url):\n",
        "        \"\"\"Improved PDF loader with retries and better cleaning\"\"\"\n",
        "        max_retries = 3\n",
        "        retry_delay = 2\n",
        "\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                response = self.session.get(url, timeout=30)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".pdf\") as tmp_file:\n",
        "                    tmp_file.write(response.content)\n",
        "                    tmp_path = tmp_file.name\n",
        "\n",
        "                loader = PyMuPDFLoader(tmp_path)\n",
        "                docs = loader.load()\n",
        "\n",
        "                # Clean up the extracted text\n",
        "                for doc in docs:\n",
        "                    doc.page_content = self._clean_pdf_text(doc.page_content)\n",
        "                    doc.metadata.update({\n",
        "                        'source': url,\n",
        "                        'type': 'pdf',\n",
        "                        'pages': doc.metadata.get('page', '')\n",
        "                    })\n",
        "\n",
        "                os.unlink(tmp_path)\n",
        "                return docs\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Attempt {attempt + 1} failed for {url}: {str(e)}\")\n",
        "                if attempt < max_retries - 1:\n",
        "                    time.sleep(retry_delay)\n",
        "                else:\n",
        "                    if 'tmp_path' in locals() and os.path.exists(tmp_path):\n",
        "                        os.unlink(tmp_path)\n",
        "                    return []\n",
        "\n",
        "    def _clean_pdf_text(self, text):\n",
        "        \"\"\"Clean and normalize PDF text\"\"\"\n",
        "        # Remove page numbers and footers\n",
        "        text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
        "        # Normalize whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        # Remove lonely characters\n",
        "        text = re.sub(r'(?<!\\w)\\w(?!\\w)', '', text)\n",
        "        # Fix hyphenated words\n",
        "        text = re.sub(r'(\\w+)-\\s+(\\w+)', r'\\1\\2', text)\n",
        "        return text\n",
        "\n",
        "    def process_documents(self, urls):\n",
        "        \"\"\"Process documents with better error handling\"\"\"\n",
        "        all_docs = []\n",
        "        failed_urls = []\n",
        "\n",
        "        for url in urls:\n",
        "            print(f\"\\nProcessing: {url}\")\n",
        "            try:\n",
        "                if url.lower().endswith('.pdf'):\n",
        "                    docs = self.load_pdf_from_url(url)\n",
        "                else:\n",
        "                    docs = self.load_html(url)\n",
        "\n",
        "                if docs:\n",
        "                    all_docs.extend(docs)\n",
        "                    print(f\"Successfully loaded {len(docs)} documents\")\n",
        "                else:\n",
        "                    failed_urls.append(url)\n",
        "                    print(\"Failed to load document\")\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_urls.append(url)\n",
        "                print(f\"Error processing {url}: {str(e)}\")\n",
        "\n",
        "        if not all_docs:\n",
        "            raise ValueError(\"No documents were successfully loaded\")\n",
        "\n",
        "        print(f\"\\nSummary:\")\n",
        "        print(f\"- Successfully loaded: {len(all_docs)} documents\")\n",
        "        print(f\"- Failed URLs: {len(failed_urls)}\")\n",
        "        if failed_urls:\n",
        "            print(\"Failed URLs:\", failed_urls)\n",
        "\n",
        "        splits = self.text_splitter.split_documents(all_docs)\n",
        "        print(f\"- Total chunks after splitting: {len(splits)}\")\n",
        "        return splits, failed_urls # Return splits and failed_urls\n",
        "\n",
        "    def create_vector_store(self, splits):\n",
        "        \"\"\"Create and persist Chroma vector store\"\"\"\n",
        "        vectorstore = PineconeVectorStore.from_documents(\n",
        "        documents=splits,\n",
        "        embedding=self.embeddings,\n",
        "        index_name=index_name\n",
        ")\n",
        "        print(f\"Pinecone vector store '{index_name}' created successfully!\")\n",
        "        return vectorstore\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Your list of documents\n",
        "    documents = [\n",
        "        \"https://portlandpress.com/biochemj/article/477/23/4559/227194/Deep-learning-and-generative-methods-in\",\n",
        "        \"https://pubs.acs.org/doi/10.1021/acs.jcim.9b00236\",\n",
        "        \"https://www.osti.gov/servlets/purl/1427646\",\n",
        "        \"https://depth-first.com/articles/2019/02/04/chemception-deep-learning-from-2d-chemical-structure-images/\",\n",
        "        \"https://arxiv.org/ftp/arxiv/papers/1802/1802.04903.pdf\",\n",
        "        \"http://cucis.eecs.northwestern.edu/publications/pdf/PJA18.pdf\",\n",
        "        \"https://www.nature.com/articles/s41467-022-28494-3\",\n",
        "        \"https://link.springer.com/article/10.1007/s00521-021-05961-4\",\n",
        "        \"https://www.mdpi.com/journal/molecules/special_issues/deep_learning_structure\",\n",
        "        \"https://www.sciencedirect.com/science/article/abs/pii/B9780443186387000050\",\n",
        "        \"https://www.mdpi.com/1420-3049/25/12/2764\",\n",
        "        \"https://jcheminf.biomedcentral.com/articles/10.1186/s13321-020-00435-6\",\n",
        "        \"https://www.nature.com/articles/s41598-025-95720-5\",\n",
        "        \"https://pmc.ncbi.nlm.nih.gov/articles/PMC11571686/\",\n",
        "        \"https://link.springer.com/article/10.1557/s43578-022-00628-9\"\n",
        "    ]\n",
        "\n",
        "    # Initialize and process\n",
        "    processor = DocumentProcessor()\n",
        "    splits, failed_urls = processor.process_documents(documents) # Unpack the tuple here\n",
        "    vectorstore = processor.create_vector_store(splits) # Pass only splits\n",
        "\n",
        "    # Get retriever\n",
        "    retriever = vectorstore.as_retriever()\n",
        "    print(\"Vector store and retriever created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rkXtIp2HfEDg",
      "metadata": {
        "id": "rkXtIp2HfEDg"
      },
      "outputs": [],
      "source": [
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "docs=retriever.get_relevant_documents(\"what is SMILE?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sCXvz2J9fYN4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCXvz2J9fYN4",
        "outputId": "4d9109b0-f7cd-44d2-99de-10698308588d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "AIMessage(content=\"SMILES, or Simplified Molecular Input Line Entry System, is a prevalent method for representing molecules in deep learning. It uses ASCII character strings to represent a molecule's chemical structure, encoding the connection table and stereochemistry as a line of text. Each element in the periodic table is assigned a corresponding token using its atomic symbol, with bond types inferred or explicitly indicated using non-alphanumeric tokens and brackets for branches or cycles.\\n\\nSMILES can be considered a chemical language with chemical tokens as words and molecules as sentences, but it can have syntactic and grammar errors, especially with branches and cycles. While SMILES is a non-unique molecular representation, it can be transformed into a unique one through canonicalization algorithms. DeepSMILES and SELFIES are SMILES-like notations developed to address some of the limitations of SMILES, such as grammatical errors and valency constraints.\\n\\nSMILES notations are used as sequence inputs for neural networks in predicting chemical properties. SMILES can be generated from raw images using deep learning models, which learn chemical abbreviations and generate chemically valid SMILES without explicit subcomponent recognition. SMILES reconstruction accuracy can be improved by increasing the size of the training data and using techniques like Butina clustering to address potential overfitting.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': []}, id='run--3876f0a4-e400-4550-bde3-a5883a91fa80-0', usage_metadata={'input_tokens': 4447, 'output_tokens': 251, 'total_tokens': 4698, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"context\":docs,\"question\":\"What is SMILE tell me about it in detail\"})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0VgrLH3fgnXy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VgrLH3fgnXy",
        "outputId": "63093c19-fa17-46e8-edcb-09459c39342c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "768"
            ]
          },
          "execution_count": 136,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
